From 7f62cae72a49ab95af602ef5103ba0aeee68b604 Mon Sep 17 00:00:00 2001
From: Xiaotian Wu <wuxiaotian@loongson.cn>
Date: Tue, 20 Dec 2022 18:54:24 +0800
Subject: [PATCH 2/2] add loong64 support

---
 test/CodeGen/LoongArch/abi-lp64d.c            |  474 +++
 .../LoongArch/inlineasm-float-double-in-gpr.c |   49 +
 test/CodeGen/builtins-loongarch-base.c        |  417 ++
 test/CodeGen/builtins-loongarch-lasx-error.c  |  266 ++
 test/CodeGen/builtins-loongarch-lasx.c        | 3761 +++++++++++++++++
 test/CodeGen/builtins-loongarch-lsx-error.c   |  250 ++
 test/CodeGen/builtins-loongarch-lsx.c         | 3630 ++++++++++++++++
 test/CodeGen/loongarch-inline-asm-modifiers.c |   50 +
 test/CodeGen/loongarch-inline-asm.c           |   31 +
 .../LoongArch/abi-lp64d-struct-inherit.cpp    |   95 +
 test/Driver/loongarch-abi-fpu.c               |   26 +
 test/Driver/loongarch-alignment-feature.c     |    8 +
 test/Driver/loongarch-double-single-soft.c    |   12 +
 test/Driver/loongarch-mabi.c                  |   22 +
 test/Driver/loongarch-mfpu.c                  |   21 +
 15 files changed, 9112 insertions(+)
 create mode 100644 test/CodeGen/LoongArch/abi-lp64d.c
 create mode 100644 test/CodeGen/LoongArch/inlineasm-float-double-in-gpr.c
 create mode 100644 test/CodeGen/builtins-loongarch-base.c
 create mode 100644 test/CodeGen/builtins-loongarch-lasx-error.c
 create mode 100644 test/CodeGen/builtins-loongarch-lasx.c
 create mode 100644 test/CodeGen/builtins-loongarch-lsx-error.c
 create mode 100644 test/CodeGen/builtins-loongarch-lsx.c
 create mode 100644 test/CodeGen/loongarch-inline-asm-modifiers.c
 create mode 100644 test/CodeGen/loongarch-inline-asm.c
 create mode 100644 test/CodeGenCXX/LoongArch/abi-lp64d-struct-inherit.cpp
 create mode 100644 test/Driver/loongarch-abi-fpu.c
 create mode 100644 test/Driver/loongarch-alignment-feature.c
 create mode 100644 test/Driver/loongarch-double-single-soft.c
 create mode 100644 test/Driver/loongarch-mabi.c
 create mode 100644 test/Driver/loongarch-mfpu.c

diff --git a/test/CodeGen/LoongArch/abi-lp64d.c b/test/CodeGen/LoongArch/abi-lp64d.c
new file mode 100644
index 00000000..80435701
--- /dev/null
+++ b/test/CodeGen/LoongArch/abi-lp64d.c
@@ -0,0 +1,474 @@
+// RUN: %clang_cc1 -triple loongarch64 -target-feature +f -target-feature +d -target-abi lp64d \
+// RUN: -emit-llvm %s -o - | FileCheck %s
+
+/// This test checks the calling convention of the lp64d ABI.
+
+#include <stddef.h>
+#include <stdint.h>
+
+/// Part 0: C Data Types and Alignment.
+
+/// `char` datatype is signed by default.
+/// In most cases, the unsigned integer data types are zero-extended when stored
+/// in general-purpose register, and the signed integer data types are
+/// sign-extended. However, in the LP64D ABI, unsigned 32-bit types, such as
+/// unsigned int, are stored in general-purpose registers as proper sign
+/// extensions of their 32-bit values.
+
+// CHECK-LABEL: define{{.*}} zeroext i1 @check_bool()
+_Bool check_bool() { return 0; }
+
+// CHECK-LABEL: define{{.*}} signext i8 @check_char()
+char check_char() { return 0; }
+
+// CHECK-LABEL: define{{.*}} signext i16 @check_short()
+short check_short() { return 0; }
+
+// CHECK-LABEL: define{{.*}} signext i32 @check_int()
+int check_int() { return 0; }
+
+// CHECK-LABEL: define{{.*}} i64 @check_long()
+long check_long() { return 0; }
+
+// CHECK-LABEL: define{{.*}} i64 @check_longlong()
+long long check_longlong() { return 0; }
+
+// CHECK-LABEL: define{{.*}} zeroext i8 @check_uchar()
+unsigned char check_uchar() { return 0; }
+
+// CHECK-LABEL: define{{.*}} zeroext i16 @check_ushort()
+unsigned short check_ushort() { return 0; }
+
+// CHECK-LABEL: define{{.*}} signext i32 @check_uint()
+unsigned int check_uint() { return 0; }
+
+// CHECK-LABEL: define{{.*}} i64 @check_ulong()
+unsigned long check_ulong() { return 0; }
+
+// CHECK-LABEL: define{{.*}} i64 @check_ulonglong()
+unsigned long long check_ulonglong() { return 0; }
+
+// CHECK-LABEL: define{{.*}} float @check_float()
+float check_float() { return 0; }
+
+// CHECK-LABEL: define{{.*}} double @check_double()
+double check_double() { return 0; }
+
+// CHECK-LABEL: define{{.*}} fp128 @check_longdouble()
+long double check_longdouble() { return 0; }
+
+/// Part 1: Scalar arguments and return value.
+
+/// The lp64d abi says:
+/// 1. 1 < WOA <= GRLEN
+/// a. Argument is passed in a single argument register, or on the stack by
+/// value if none is available.
+/// i. If the argument is floating-point type, the argument is passed in FAR. if
+/// no FAR is available, it’s passed in GAR. If no GAR is available, it’s
+/// passed on the stack. When passed in registers or on the stack,
+/// floating-point types narrower than GRLEN bits are widened to GRLEN bits,
+/// with the upper bits undefined.
+/// ii. If the argument is integer or pointer type, the argument is passed in
+/// GAR. If no GAR is available, it’s passed on the stack. When passed in
+/// registers or on the stack, the unsigned integer scalars narrower than GRLEN
+/// bits are zero-extended to GRLEN bits, and the signed integer scalars are
+/// sign-extended.
+/// 2. GRLEN < WOA ≤ 2 × GRLEN
+/// a. The argument is passed in a pair of GAR, with the low-order GRLEN bits in
+/// the lower-numbered register and the high-order GRLEN bits in the
+/// higher-numbered register. If exactly one register is available, the
+/// low-order GRLEN bits are passed in the register and the high-order GRLEN
+/// bits are passed on the stack. If no GAR is available, it’s passed on the
+/// stack.
+
+/// Note that most of these conventions are handled at the llvm side, so here we
+/// only check the correctness of argument (or return value)'s sign/zero
+/// extension attribute.
+
+// CHECK-LABEL: define{{.*}} signext i32 @f_scalar(i1{{.*}} zeroext %a, i8{{.*}} signext %b, i8{{.*}} zeroext %c, i16{{.*}} signext %d, i16{{.*}} zeroext %e, i32{{.*}} signext %f, i32{{.*}} signext %g, i64{{.*}} %h, i1{{.*}} zeroext %i, i8{{.*}} signext %j, i8{{.*}} zeroext %k, i16{{.*}} signext %l, i16{{.*}} zeroext %m, i32{{.*}} signext %n, i32{{.*}} signext %o, i64{{.*}} %p)
+int f_scalar(_Bool a, int8_t b, uint8_t c, int16_t d, uint16_t e, int32_t f,
+             uint32_t g, int64_t h, /* begin of stack passing -> */ _Bool i,
+             int8_t j, uint8_t k, int16_t l, uint16_t m, int32_t n,
+             uint32_t o, int64_t p) {
+  return 0;
+}
+
+/// Part 2: Structure arguments and return value.
+
+/// The lp64d abi says:
+/// Empty structures are ignored by C compilers which support them as a
+/// non-standard extension(same as union arguments and return values). Bits
+/// unused due to padding, and bits past the end of a structure whose size in
+/// bits is not divisible by GRLEN, are undefined. And the layout of the
+/// structure on the stack is consistent with that in memory.
+
+/// Check empty structs are ignored.
+
+struct empty_s {};
+
+// CHECK-LABEL: define{{.*}} void @f_empty_s()
+struct empty_s f_empty_s(struct empty_s x) {
+  return x;
+}
+
+/// 1. 0 < WOA ≤ GRLEN
+/// a. The structure has only fixed-point members. If there is an available GAR,
+/// the structure is passed through the GAR by value passing; If no GAR is
+/// available, it’s passed on the stack.
+
+struct i16x4_s {
+  int16_t a, b, c, d;
+};
+
+// CHECK-LABEL: define{{.*}} i64 @f_i16x4_s(i64 %x.coerce)
+struct i16x4_s f_i16x4_s(struct i16x4_s x) {
+  return x;
+}
+
+/// b. The structure has only floating-point members:
+/// i. One floating-point member. The argument is passed in a FAR; If no FAR is
+/// available, the value is passed in a GAR; if no GAR is available, the value
+/// is passed on the stack.
+
+struct f32x1_s {
+  float a;
+};
+
+struct f64x1_s {
+  double a;
+};
+
+// CHECK-LABEL: define{{.*}} float @f_f32x1_s(float %0)
+struct f32x1_s f_f32x1_s(struct f32x1_s x) {
+  return x;
+}
+
+// CHECK-LABEL: define{{.*}} double @f_f64x1_s(double %0)
+struct f64x1_s f_f64x1_s(struct f64x1_s x) {
+  return x;
+}
+
+/// ii. Two floating-point members. The argument is passed in a pair of
+/// available FAR, with the low-order float member bits in the lower-numbered
+/// FAR and the high-order float member bits in the higher-numbered FAR. If the
+/// number of available FAR is less than 2, it’s passed in a GAR, and passed on
+/// the stack if no GAR is available.
+
+struct f32x2_s {
+  float a, b;
+};
+
+// CHECK-LABEL: define{{.*}} { float, float } @f_f32x2_s(float %0, float %1)
+struct f32x2_s f_f32x2_s(struct f32x2_s x) {
+  return x;
+}
+
+/// c. The structure has both fixed-point and floating-point members, i.e. the
+/// structure has one float member and...
+/// i. Multiple fixed-point members. If there are available GAR, the structure
+/// is passed in a GAR, and passed on the stack if no GAR is available.
+
+struct f32x1_i16x2_s {
+  float a;
+  int16_t b, c;
+};
+
+// CHECK-LABEL: define{{.*}} i64 @f_f32x1_i16x2_s(i64 %x.coerce)
+struct f32x1_i16x2_s f_f32x1_i16x2_s(struct f32x1_i16x2_s x) {
+  return x;
+}
+
+/// ii. Only one fixed-point member. If one FAR and one GAR are available, the
+/// floating-point member of the structure is passed in the FAR, and the integer
+/// member of the structure is passed in the GAR; If no floating-point register
+/// but one GAR is available, it’s passed in GAR; If no GAR is available, it’s
+/// passed on the stack.
+
+struct f32x1_i32x1_s {
+  float a;
+  int32_t b;
+};
+
+// CHECK-LABEL: define{{.*}} { float, i32 } @f_f32x1_i32x1_s(float %0, i32 %1)
+struct f32x1_i32x1_s f_f32x1_i32x1_s(struct f32x1_i32x1_s x) {
+  return x;
+}
+
+/// 2. GRLEN < WOA ≤ 2 × GRLEN
+/// a. Only fixed-point members.
+/// i. The argument is passed in a pair of available GAR, with the low-order
+/// bits in the lower-numbered GAR and the high-order bits in the
+/// higher-numbered GAR. If only one GAR is available, the low-order bits are in
+/// the GAR and the high-order bits are on the stack, and passed on the stack if
+/// no GAR is available.
+
+struct i64x2_s {
+  int64_t a, b;
+};
+
+// CHECK-LABEL: define{{.*}} [2 x i64] @f_i64x2_s([2 x i64] %x.coerce)
+struct i64x2_s f_i64x2_s(struct i64x2_s x) {
+  return x;
+}
+
+/// b. Only floating-point members.
+/// i. The structure has one long double member or one double member and two
+/// adjacent float members or 3-4 float members. The argument is passed in a
+/// pair of available GAR, with the low-order bits in the lower-numbered GAR and
+/// the high-order bits in the higher-numbered GAR. If only one GAR is
+/// available, the low-order bits are in the GAR and the high-order bits are on
+/// the stack, and passed on the stack if no GAR is available.
+
+struct f128x1_s {
+  long double a;
+};
+
+// CHECK-LABEL: define{{.*}} i128 @f_f128x1_s(i128 %x.coerce)
+struct f128x1_s f_f128x1_s(struct f128x1_s x) {
+  return x;
+}
+
+struct f64x1_f32x2_s {
+  double a;
+  float b, c;
+};
+
+// CHECK-LABEL: define{{.*}} [2 x i64] @f_f64x1_f32x2_s([2 x i64] %x.coerce)
+struct f64x1_f32x2_s f_f64x1_f32x2_s(struct f64x1_f32x2_s x) {
+  return x;
+}
+
+struct f32x3_s {
+  float a, b, c;
+};
+
+// CHECK-LABEL: define{{.*}} [2 x i64] @f_f32x3_s([2 x i64] %x.coerce)
+struct f32x3_s f_f32x3_s(struct f32x3_s x) {
+  return x;
+}
+
+struct f32x4_s {
+  float a, b, c, d;
+};
+
+// CHECK-LABEL: define{{.*}} [2 x i64] @f_f32x4_s([2 x i64] %x.coerce)
+struct f32x4_s f_f32x4_s(struct f32x4_s x) {
+  return x;
+}
+
+/// ii. The structure with two double members is passed in a pair of available
+/// FARs. If no a pair of available FARs, it’s passed in GARs. A structure with
+/// one double member and one float member is same.
+
+struct f64x2_s {
+  double a, b;
+};
+
+// CHECK-LABEL: define{{.*}} { double, double } @f_f64x2_s(double %0, double %1)
+struct f64x2_s f_f64x2_s(struct f64x2_s x) {
+  return x;
+}
+
+/// c. Both fixed-point and floating-point members.
+/// i. The structure has one double member and only one fixed-point member.
+/// A. If one FAR and one GAR are available, the floating-point member of the
+/// structure is passed in the FAR, and the integer member of the structure is
+/// passed in the GAR; If no floating-point registers but two GARs are
+/// available, it’s passed in the two GARs; If only one GAR is available, the
+/// low-order bits are in the GAR and the high-order bits are on the stack; And
+/// it’s passed on the stack if no GAR is available.
+
+struct f64x1_i64x1_s {
+  double a;
+  int64_t b;
+};
+
+// CHECK-LABEL: define{{.*}} { double, i64 } @f_f64x1_i64x1_s(double %0, i64 %1)
+struct f64x1_i64x1_s f_f64x1_i64x1_s(struct f64x1_i64x1_s x) {
+  return x;
+}
+
+/// ii. Others
+/// A. The argument is passed in a pair of available GAR, with the low-order
+/// bits in the lower-numbered GAR and the high-order bits in the
+/// higher-numbered GAR. If only one GAR is available, the low-order bits are in
+/// the GAR and the high-order bits are on the stack, and passed on the stack if
+/// no GAR is available.
+
+struct f64x1_i32x2_s {
+  double a;
+  int32_t b, c;
+};
+
+// CHECK-LABEL: define{{.*}} [2 x i64] @f_f64x1_i32x2_s([2 x i64] %x.coerce)
+struct f64x1_i32x2_s f_f64x1_i32x2_s(struct f64x1_i32x2_s x) {
+  return x;
+}
+
+struct f32x2_i32x2_s {
+  float a, b;
+  int32_t c, d;
+};
+
+// CHECK-LABEL: define{{.*}} [2 x i64] @f_f32x2_i32x2_s([2 x i64] %x.coerce)
+struct f32x2_i32x2_s f_f32x2_i32x2_s(struct f32x2_i32x2_s x) {
+  return x;
+}
+
+/// 3. WOA > 2 × GRLEN
+/// a. It’s passed by reference and are replaced in the argument list with the
+/// address. If there is an available GAR, the reference is passed in the GAR,
+/// and passed on the stack if no GAR is available.
+
+struct i64x4_s {
+  int64_t a, b, c, d;
+};
+
+// CHECK-LABEL: define{{.*}} void @f_i64x4_s(%struct.i64x4_s*{{.*}} sret(%struct.i64x4_s){{.*}} %agg.result, %struct.i64x4_s*{{.*}} %x)
+struct i64x4_s f_i64x4_s(struct i64x4_s x) {
+  return x;
+}
+
+struct f64x4_s {
+  double a, b, c, d;
+};
+
+// CHECK-LABEL: define{{.*}} void @f_f64x4_s(%struct.f64x4_s*{{.*}} sret(%struct.f64x4_s){{.*}} %agg.result, %struct.f64x4_s*{{.*}} %x)
+struct f64x4_s f_f64x4_s(struct f64x4_s x) {
+  return x;
+}
+
+/// Part 3: Union arguments and return value.
+
+/// Check empty unions are ignored.
+
+union empty_u {};
+
+// CHECK-LABEL: define{{.*}} void @f_empty_u()
+union empty_u f_empty_u(union empty_u x) {
+  return x;
+}
+
+/// Union is passed in GAR or stack.
+/// 1. 0 < WOA ≤ GRLEN
+/// a. The argument is passed in a GAR, or on the stack by value if no GAR is
+/// available.
+
+union i32_f32_u {
+  int32_t a;
+  float b;
+};
+
+// CHECK-LABEL: define{{.*}} i64 @f_i32_f32_u(i64 %x.coerce)
+union i32_f32_u f_i32_f32_u(union i32_f32_u x) {
+  return x;
+}
+
+union i64_f64_u {
+  int64_t a;
+  double b;
+};
+
+// CHECK-LABEL: define{{.*}} i64 @f_i64_f64_u(i64 %x.coerce)
+union i64_f64_u f_i64_f64_u(union i64_f64_u x) {
+  return x;
+}
+
+/// 2. GRLEN < WOA ≤ 2 × GRLEN
+/// a. The argument is passed in a pair of available GAR, with the low-order
+/// bits in the lower-numbered GAR and the high-order bits in the
+/// higher-numbered GAR. If only one GAR is available, the low-order bits are in
+/// the GAR and the high-order bits are on the stack. The arguments are passed
+/// on the stack when no GAR is available.
+
+union i128_f128_u {
+  __int128_t a;
+  long double b;
+};
+
+// CHECK-LABEL: define{{.*}} i128 @f_i128_f128_u(i128 %x.coerce)
+union i128_f128_u f_i128_f128_u(union i128_f128_u x) {
+  return x;
+}
+
+/// 3. WOA > 2 × GRLEN
+/// a. It’s passed by reference and are replaced in the argument list with the
+/// address. If there is an available GAR, the reference is passed in the GAR,
+/// and passed on the stack if no GAR is available.
+
+union i64_arr3_u {
+  int64_t a[3];
+};
+
+// CHECK-LABEL: define{{.*}} void @f_i64_arr3_u(%union.i64_arr3_u*{{.*}} sret(%union.i64_arr3_u){{.*}} %agg.result, %union.i64_arr3_u*{{.*}} %x)
+union i64_arr3_u f_i64_arr3_u(union i64_arr3_u x) {
+  return x;
+}
+
+/// Part 4: Complex number arguments and return value.
+
+/// A complex floating-point number, or a structure containing just one complex
+/// floating-point number, is passed as though it were a structure containing
+/// two floating-point reals.
+
+// CHECK-LABEL: define{{.*}} { float, float } @f_floatcomplex(float{{.*}} %x.coerce0, float{{.*}} %x.coerce1)
+float __complex__ f_floatcomplex(float __complex__ x) { return x; }
+
+// CHECK-LABEL: define{{.*}} { double, double } @f_doublecomplex(double{{.*}} %x.coerce0, double{{.*}} %x.coerce1)
+double __complex__ f_doublecomplex(double __complex__ x) { return x; }
+
+struct floatcomplex_s {
+  float __complex__ c;
+};
+// CHECK-LABEL: define{{.*}} { float, float } @f_floatcomplex_s(float %0, float %1)
+struct floatcomplex_s f_floatcomplex_s(struct floatcomplex_s x) {
+  return x;
+}
+
+struct doublecomplex_s {
+  double __complex__ c;
+};
+// CHECK-LABEL: define{{.*}} { double, double } @f_doublecomplex_s(double %0, double %1)
+struct doublecomplex_s f_doublecomplex_s(struct doublecomplex_s x) {
+  return x;
+}
+
+/// Part 5: Variadic arguments.
+
+/// Variadic arguments are passed in GARs in the same manner as named arguments.
+
+int f_va_callee(int, ...);
+
+// CHECK-LABEL: define{{.*}} void @f_va_caller()
+// CHECK: call signext i32 (i32, ...) @f_va_callee(i32{{.*}} signext 1, i32{{.*}} signext 2, i64{{.*}} 3, double{{.*}} 4.000000e+00, double{{.*}} 5.000000e+00, i64 {{.*}}, i64 {{.*}}, i64 {{.*}})
+void f_va_caller(void) {
+  f_va_callee(1, 2, 3LL, 4.0f, 5.0, (struct i16x4_s){6, 7, 8, 9},
+              (struct i64x2_s){10, 11});
+}
+
+// CHECK-LABE: define signext i32 @f_va_int(i8* %fmt, ...)
+// CHECK: entry:
+// CHECK:   %fmt.addr = alloca i8*, align 8
+// CHECK:   %va = alloca i8*, align 8
+// CHECK:   %v = alloca i32, align 4
+// CHECK:   store i8* %fmt, i8** %fmt.addr, align 8
+// CHECK:   %va1 = bitcast i8** %va to i8*
+// CHECK:   call void @llvm.va_start(i8* %va1)
+// CHECK:   %argp.cur = load i8*, i8** %va, align 8
+// CHECK:   %argp.next = getelementptr inbounds i8, i8* %argp.cur, i64 8
+// CHECK:   store i8* %argp.next, i8** %va, align 8
+// CHECK:   %0 = bitcast i8* %argp.cur to i32*
+// CHECK:   %1 = load i32, i32* %0, align 8
+// CHECK:   store i32 %1, i32* %v, align 4
+// CHECK:   %va2 = bitcast i8** %va to i8*
+// CHECK:   call void @llvm.va_end(i8* %va2)
+// CHECK:   %2 = load i32, i32* %v, align 4
+// CHECK:   ret i32 %2
+// CHECK: }
+int f_va_int(char *fmt, ...) {
+  __builtin_va_list va;
+  __builtin_va_start(va, fmt);
+  int v = __builtin_va_arg(va, int);
+  __builtin_va_end(va);
+  return v;
+}
diff --git a/test/CodeGen/LoongArch/inlineasm-float-double-in-gpr.c b/test/CodeGen/LoongArch/inlineasm-float-double-in-gpr.c
new file mode 100644
index 00000000..bc9c616b
--- /dev/null
+++ b/test/CodeGen/LoongArch/inlineasm-float-double-in-gpr.c
@@ -0,0 +1,49 @@
+// RUN: %clang_cc1 -triple loongarch64 -O2 -emit-llvm %s -o - \
+// RUN:   | FileCheck %s
+
+float f;
+double d;
+
+// CHECK-LABEL: @reg_float(
+// CHECK: [[FLT_ARG:%.*]] = load float, float* @f
+// CHECK: call void asm sideeffect "", "r"(float [[FLT_ARG]])
+// CHECK: ret void
+void reg_float() {
+  float a = f;
+  asm volatile(""
+               :
+               : "r"(a));
+}
+
+// CHECK-LABEL: @r4_float(
+// CHECK: [[FLT_ARG:%.*]] = load float, float* @f
+// CHECK: call void asm sideeffect "", "{$r4}"(float [[FLT_ARG]])
+// CHECK: ret void
+void r4_float() {
+  register float a asm("$r4") = f;
+  asm volatile(""
+               :
+               : "r"(a));
+}
+
+// CHECK-LABEL: @reg_double(
+// CHECK: [[DBL_ARG:%.*]] = load double, double* @d
+// CHECK: call void asm sideeffect "", "r"(double [[DBL_ARG]])
+// CHECK: ret void
+void reg_double() {
+  double a = d;
+  asm volatile(""
+               :
+               : "r"(a));
+}
+
+// CHECK-LABEL: @r4_double(
+// CHECK: [[DBL_ARG:%.*]] = load double, double* @d
+// CHECK: call void asm sideeffect "", "{$r4}"(double [[DBL_ARG]])
+// CHECK: ret void
+void r4_double() {
+  register double a asm("$r4") = d;
+  asm volatile(""
+               :
+               : "r"(a));
+}
diff --git a/test/CodeGen/builtins-loongarch-base.c b/test/CodeGen/builtins-loongarch-base.c
new file mode 100644
index 00000000..d7221359
--- /dev/null
+++ b/test/CodeGen/builtins-loongarch-base.c
@@ -0,0 +1,417 @@
+// REQUIRES: loongarch-registered-target
+// RUN: %clang_cc1 -triple loongarch64-linux-gnu -emit-llvm %s -o - | FileCheck %s
+
+#include <larchintrin.h>
+
+typedef char i8;
+typedef unsigned char u8;
+typedef short i16;
+typedef unsigned short u16;
+typedef int i32;
+typedef unsigned int u32;
+
+#if __LONG_MAX__ == __LONG_LONG_MAX__
+typedef long int i64;
+typedef unsigned long int u64;
+#else
+typedef long long i64;
+typedef unsigned long long u64;
+#endif
+
+__drdtime_t drdtime;
+__rdtime_t rdtime;
+
+void cpucfg(){
+
+  u32 u32_r, u32_a;
+  // __cpucfg
+  // rd, rj
+  // unsigned int, unsigned int
+  u32_r= __builtin_loongarch_cpucfg(u32_a); // CHECK: call i32 @llvm.loongarch.cpucfg
+
+}
+
+void csrrd(){
+
+  u32 u32_r;
+  // __csrrd
+  // rd, csr_num
+  // unsigned int, uimm14_32
+  u32_r=__builtin_loongarch_csrrd(1); // CHECK: call i32 @llvm.loongarch.csrrd
+
+}
+
+void dcsrrd(){
+
+  u64 u64_r;
+  // __dcsrrd
+  // rd, csr_num
+  // unsigned long int, uimm14
+  u64_r=__builtin_loongarch_dcsrrd(1); // CHECK: call i64 @llvm.loongarch.dcsrrd
+
+}
+
+void csrwr(){
+
+  u32 u32_r, u32_a;
+  // __csrwr
+  // rd, csr_num
+  // unsigned int, uimm14_32
+  u32_r=__builtin_loongarch_csrwr(u32_a, 1); // CHECK: call i32 @llvm.loongarch.csrwr
+
+}
+
+void dcsrwr(){
+
+  u64 u64_r, u64_a;
+  // __dcsrwr
+  // rd, csr_num
+  // unsigned long int, uimm14
+  u64_r=__builtin_loongarch_dcsrwr(u64_a, 1); // CHECK: call i64 @llvm.loongarch.dcsrwr
+
+}
+
+void csrxchg(){
+
+  u32 u32_r, u32_a, u32_b;
+  // __csrxchg
+  // rd, rj, csr_num
+  // unsigned int, unsigned int, uimm14_32
+  u32_r=__builtin_loongarch_csrxchg(u32_a, u32_b, 1); // CHECK: call i32 @llvm.loongarch.csrxchg
+
+}
+
+void dcsrxchg(){
+
+  u64 u64_r, u64_a, u64_b;
+  // __dcsrxchg
+  // rd, rj, csr_num
+  // unsigned long int, unsigned long int, uimm14
+  u64_r=__builtin_loongarch_dcsrxchg(u64_a, u64_b, 1); // CHECK: call i64 @llvm.loongarch.dcsrxchg
+
+}
+
+void iocsrrd_b(){
+
+  u32 u32_a;
+  u8 u8_r;
+  // __iocsrrd_b
+  // rd, rj
+  // unsigned char, unsigned int
+  u8_r=__builtin_loongarch_iocsrrd_b(u32_a); // CHECK: call i32 @llvm.loongarch.iocsrrd.b
+
+}
+
+void iocsrrd_h(){
+
+  u32 u32_a;
+  u16 u16_r;
+  // __iocsrrd_h
+  // rd, rj
+  // unsigned short, unsigned int
+  u16_r=__builtin_loongarch_iocsrrd_h(u32_a); // CHECK: call i32 @llvm.loongarch.iocsrrd.h
+
+}
+
+void iocsrrd_w(){
+
+  u32 u32_r, u32_a;
+  // __iocsrrd_w
+  // rd, rj
+  // unsigned int, unsigned int
+  u32_r=__builtin_loongarch_iocsrrd_w(u32_a); // CHECK: call i32 @llvm.loongarch.iocsrrd.w
+
+}
+
+void iocsrrd_d(){
+
+  u32 u32_a;
+  u64 u64_r;
+  // __iocsrrd_d
+  // rd, rj
+  // unsigned long int, unsigned int
+  u64_r=__builtin_loongarch_iocsrrd_d(u32_a); // CHECK: call i64 @llvm.loongarch.iocsrrd.d
+
+}
+
+void iocsrwr_b(){
+
+  u32 u32_a;
+  u8 u8_a;
+  // __iocsrwr_b
+  // rd, rj
+  // unsigned char, unsigned int
+  __builtin_loongarch_iocsrwr_b(u8_a, u32_a); // CHECK: void @llvm.loongarch.iocsrwr.b
+
+}
+
+void iocsrwr_h(){
+
+  u32 u32_a;
+  u16 u16_a;
+  // __iocsrwr_h
+  // rd, rj
+  // unsigned short, unsigned int
+  __builtin_loongarch_iocsrwr_h(u16_a, u32_a); // CHECK: void @llvm.loongarch.iocsrwr.h
+
+}
+
+void iocsrwr_w(){
+
+  u32 u32_a, u32_b;
+  // __iocsrwr_w
+  // rd, rj
+  // unsigned int, unsigned int
+  __builtin_loongarch_iocsrwr_w(u32_a, u32_b); // CHECK: void @llvm.loongarch.iocsrwr.w
+
+}
+
+void iocsrwr_d(){
+
+  u32 u32_a;
+  u64 u64_a;
+  // __iocsrwr_d
+  // rd, rj
+  // unsigned long int, unsigned int
+  __builtin_loongarch_iocsrwr_d(u64_a, u32_a); // CHECK: void @llvm.loongarch.iocsrwr.d
+
+}
+
+void cacop(){
+
+  i32 i32_a;
+  // __cacop
+  // op, rj, si12
+  // uimm5, unsigned int, simm12
+  __builtin_loongarch_cacop(1, i32_a, 2); // CHECK: void @llvm.loongarch.cacop
+
+}
+
+void dcacop(){
+
+  i64 i64_a;
+  // __dcacop
+  // op, rj, si12
+  // uimm5, unsigned long int, simm12
+  __builtin_loongarch_dcacop(1, i64_a, 2); // CHECK: void @llvm.loongarch.dcacop
+
+}
+
+void rdtime_d(){
+
+  drdtime= __builtin_loongarch_rdtime_d(); // CHECK: call { i64, i64 } asm sideeffect "rdtime.d\09$0,$1\0A\09", "=&r,=&r"()
+
+}
+
+void rdtimeh_w(){
+
+  rdtime= __builtin_loongarch_rdtimeh_w(); // CHECK: call { i32, i32 } asm sideeffect "rdtimeh.w\09$0,$1\0A\09", "=&r,=&r"()
+
+}
+
+void rdtimel_w(){
+
+  rdtime= __builtin_loongarch_rdtimel_w(); // CHECK: call { i32, i32 } asm sideeffect "rdtimel.w\09$0,$1\0A\09", "=&r,=&r"()
+
+}
+
+void crc_w_b_w(){
+
+  i32 i32_r, i32_a;
+  i8 i8_a;
+  // __crc_w_b_w
+  // rd, rj, rk
+  // int, char, int
+  i32_r=__builtin_loongarch_crc_w_b_w(i8_a, i32_a); // CHECK: call i32 @llvm.loongarch.crc.w.b.w
+
+}
+
+void crc_w_h_w(){
+
+  i32 i32_r, i32_a;
+  i16 i16_a;
+  // __crc_w_h_w
+  // rd, rj, rk
+  // int, short, int
+  i32_r=__builtin_loongarch_crc_w_h_w(i16_a, i32_a); // CHECK: call i32 @llvm.loongarch.crc.w.h.w
+
+}
+
+void crc_w_w_w(){
+
+  i32 i32_r, i32_a, i32_b;
+  // __crc_w_w_w
+  // rd, rj, rk
+  // int, int, int
+  i32_r=__builtin_loongarch_crc_w_w_w(i32_a, i32_b); // CHECK: call i32 @llvm.loongarch.crc.w.w.w
+
+}
+
+void crc_w_d_w(){
+
+  i32 i32_r, i32_a;
+  i64 i64_a;
+  // __crc_w_d_w
+  // rd, rj, rk
+  // int, long int, int
+  i32_r=__builtin_loongarch_crc_w_d_w(i64_a, i32_a); // CHECK: call i32 @llvm.loongarch.crc.w.d.w
+
+}
+
+void crcc_w_b_w(){
+
+  i32 i32_r, i32_a;
+  i8 i8_a;
+  // __crcc_w_b_w
+  // rd, rj, rk
+  // int, char, int
+  i32_r=__builtin_loongarch_crcc_w_b_w(i8_a, i32_a); // CHECK: call i32 @llvm.loongarch.crcc.w.b.w
+
+}
+
+void crcc_w_h_w(){
+
+  i32 i32_r, i32_a;
+  i16 i16_a;
+  // __crcc_w_h_w
+  // rd, rj, rk
+  // int, short, int
+  i32_r=__builtin_loongarch_crcc_w_h_w(i16_a, i32_a); // CHECK: call i32 @llvm.loongarch.crcc.w.h.w
+
+}
+
+void crcc_w_w_w(){
+
+  i32 i32_r, i32_a, i32_b;
+  // __crcc_w_w_w
+  // rd, rj, rk
+  // int, int, int
+  i32_r=__builtin_loongarch_crcc_w_w_w(i32_a, i32_b); // CHECK: call i32 @llvm.loongarch.crcc.w.w.w
+
+}
+
+void crcc_w_d_w(){
+
+  i32 i32_r, i32_a;
+  i64 i64_a;
+  // __crcc_w_d_w
+  // rd, rj, rk
+  // int, long int, int
+  i32_r=__builtin_loongarch_crcc_w_d_w(i64_a, i32_a); // CHECK: call i32 @llvm.loongarch.crcc.w.d.w
+
+}
+
+void tlbclr(){
+
+  // __tlbclr
+  __builtin_loongarch_tlbclr(); // CHECK: call void @llvm.loongarch.tlbclr
+
+}
+
+void tlbflush(){
+
+  // __tlbflush
+  __builtin_loongarch_tlbflush(); // CHECK: call void @llvm.loongarch.tlbflush
+
+}
+
+void tlbfill(){
+
+  // __tlbfill
+  __builtin_loongarch_tlbfill(); // CHECK: call void @llvm.loongarch.tlbfill 
+
+}
+
+void tlbrd(){
+
+  // __tlbrd
+  __builtin_loongarch_tlbrd(); // CHECK: call void @llvm.loongarch.tlbrd
+
+}
+
+void tlbwr(){
+
+  // __tlbwr
+  __builtin_loongarch_tlbwr(); // CHECK: call void @llvm.loongarch.tlbwr
+
+}
+
+void tlbsrch(){
+
+  // __tlbsrch
+  __builtin_loongarch_tlbsrch(); // CHECK: call void @llvm.loongarch.tlbsrch
+
+}
+
+void syscall(){
+
+  // __syscall
+  // Code
+  // uimm15
+  __builtin_loongarch_syscall(1); // CHECK: call void @llvm.loongarch.syscall
+
+}
+
+void break_builtin(){
+
+  // __break
+  // Code
+  // uimm15
+  __builtin_loongarch_break(1); // CHECK: call void @llvm.loongarch.break
+
+}
+
+void asrtle_d(){
+
+  i64 i64_a, i64_b;
+  // __asrtle_d
+  // rj, rk
+  // long int, long int
+  __builtin_loongarch_asrtle_d(i64_a, i64_b); // CHECK: call void @llvm.loongarch.asrtle.d
+
+}
+
+void asrtgt_d(){
+
+  i64 i64_a, i64_b;
+  // __asrtgt_d
+  // rj, rk
+  // long int, long int
+  __builtin_loongarch_asrtgt_d(i64_a, i64_b); // CHECK: call void @llvm.loongarch.asrtgt.d
+
+}
+
+void dbar(){
+
+  // __dbar
+  // hint
+  // uimm15
+  __builtin_loongarch_dbar(0); // CHECK: call void @llvm.loongarch.dbar
+
+}
+
+void ibar(){
+
+  // __ibar
+  // hint
+  // uimm15
+  __builtin_loongarch_ibar(0); // CHECK: call void @llvm.loongarch.ibar
+
+}
+
+void movfcsr2gr(){
+
+  u32 u32_r;
+  // __movfcsr2gr
+  u32_r=__movfcsr2gr(0); // CHECK: call i32 asm sideeffect "movfcsr2gr $0, $$fcsr0", "=&r"()
+
+}
+
+
+void movgr2fcsr() {
+
+  u32 u32_a;
+  // __movgr2fcsr
+  __movgr2fcsr(0, u32_a); // CHECK: call void asm sideeffect "movgr2fcsr $$fcsr0, $0", "r"(i32 %0)
+
+}
diff --git a/test/CodeGen/builtins-loongarch-lasx-error.c b/test/CodeGen/builtins-loongarch-lasx-error.c
new file mode 100644
index 00000000..99f2687e
--- /dev/null
+++ b/test/CodeGen/builtins-loongarch-lasx-error.c
@@ -0,0 +1,266 @@
+// REQUIRES: loongarch-registered-target
+// RUN: %clang_cc1 -triple loongarch64-unknown-linux-gnu -fsyntax-only %s \
+// RUN:            -target-feature +lasx \
+// RUN:            -verify -o - 2>&1
+
+#include <lasxintrin.h>
+
+void test() {
+  v32i8 v32i8_a = (v32i8){0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
+                          16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31};
+  v32i8 v32i8_b = (v32i8){1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
+                          17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32};
+  v32i8 v32i8_c = (v32i8){2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,
+                          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33};
+  v32i8 v32i8_r;
+
+  v16i16 v16i16_a = (v16i16){0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
+  v16i16 v16i16_b = (v16i16){1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
+  v16i16 v16i16_c = (v16i16){2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17};
+  v16i16 v16i16_r;
+
+  v8i32 v8i32_a = (v8i32){0, 1, 2, 3, 4, 5, 6, 7};
+  v8i32 v8i32_b = (v8i32){1, 2, 3, 4, 5, 6, 7, 8};
+  v8i32 v8i32_c = (v8i32){2, 3, 4, 5, 6, 7, 8, 9};
+  v8i32 v8i32_r;
+
+  v4i64 v4i64_a = (v4i64){0, 1, 2, 3};
+  v4i64 v4i64_b = (v4i64){1, 2, 3, 4};
+  v4i64 v4i64_c = (v4i64){2, 3, 4, 5};
+  v4i64 v4i64_r;
+
+  v32u8 v32u8_a = (v32u8){0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
+                          16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31};
+  v32u8 v32u8_b = (v32u8){1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
+                          17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32};
+  v32u8 v32u8_c = (v32u8){2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,
+                          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33};
+  v32u8 v32u8_r;
+
+  v16u16 v16u16_a = (v16u16){0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
+  v16u16 v16u16_b = (v16u16){1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
+  v16u16 v16u16_c = (v16u16){2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17};
+  v16u16 v16u16_r;
+
+  v8u32 v8u32_a = (v8u32){0, 1, 2, 3, 4, 5, 6, 7};
+  v8u32 v8u32_b = (v8u32){1, 2, 3, 4, 5, 6, 7, 8};
+  v8u32 v8u32_c = (v8u32){2, 3, 4, 5, 6, 7, 8, 9};
+  v8u32 v8u32_r;
+
+  v4u64 v4u64_a = (v4u64){0, 1, 2, 3};
+  v4u64 v4u64_b = (v4u64){1, 2, 3, 4};
+  v4u64 v4u64_c = (v4u64){2, 3, 4, 5};
+  v4u64 v4u64_r;
+
+  v8f32 v8f32_a = (v8f32){0.5, 1, 2, 3, 4, 5, 6, 7};
+  v8f32 v8f32_b = (v8f32){1.5, 2, 3, 4, 5, 6, 7, 8};
+  v8f32 v8f32_c = (v8f32){2.5, 3, 4, 5, 6, 7, 8, 9};
+  v8f32 v8f32_r;
+  v4f64 v4f64_a = (v4f64){0.5, 1, 2, 3};
+  v4f64 v4f64_b = (v4f64){1.5, 2, 3, 4};
+  v4f64 v4f64_c = (v4f64){2.5, 3, 4, 5};
+  v4f64 v4f64_r;
+
+  int i32_r;
+  int i32_a = 1;
+  int i32_b = 2;
+  unsigned int u32_r;
+  unsigned int u32_a = 1;
+  unsigned int u32_b = 2;
+  long long i64_r;
+  long long i64_a = 1;
+  long long i64_b = 2;
+  long long i64_c = 3;
+  unsigned long long u64_r;
+  unsigned long long u64_a = 1;
+  unsigned long long u64_b = 2;
+  unsigned long long u64_c = 3;
+
+  v32i8_r = __lasx_xvslli_b(v32i8_a, 8);                    // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v16i16_r = __lasx_xvslli_h(v16i16_a, 16);                 // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i32_r = __lasx_xvslli_w(v8i32_a, 32);                   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i64_r = __lasx_xvslli_d(v4i64_a, 64);                   // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v32i8_r = __lasx_xvsrai_b(v32i8_a, 8);                    // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v16i16_r = __lasx_xvsrai_h(v16i16_a, 16);                 // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i32_r = __lasx_xvsrai_w(v8i32_a, 32);                   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i64_r = __lasx_xvsrai_d(v4i64_a, 64);                   // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v32i8_r = __lasx_xvsrari_b(v32i8_a, 8);                   // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v16i16_r = __lasx_xvsrari_h(v16i16_a, 16);                // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i32_r = __lasx_xvsrari_w(v8i32_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i64_r = __lasx_xvsrari_d(v4i64_a, 64);                  // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v32i8_r = __lasx_xvsrli_b(v32i8_a, 8);                    // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v16i16_r = __lasx_xvsrli_h(v16i16_a, 16);                 // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i32_r = __lasx_xvsrli_w(v8i32_a, 32);                   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i64_r = __lasx_xvsrli_d(v4i64_a, 64);                   // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v32i8_r = __lasx_xvsrlri_b(v32i8_a, 8);                   // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v16i16_r = __lasx_xvsrlri_h(v16i16_a, 16);                // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i32_r = __lasx_xvsrlri_w(v8i32_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i64_r = __lasx_xvsrlri_d(v4i64_a, 64);                  // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v32u8_r = __lasx_xvbitclri_b(v32u8_a, 8);                 // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v16u16_r = __lasx_xvbitclri_h(v16u16_a, 16);              // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8u32_r = __lasx_xvbitclri_w(v8u32_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4u64_r = __lasx_xvbitclri_d(v4u64_a, 64);                // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v32u8_r = __lasx_xvbitseti_b(v32u8_a, 8);                 // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v16u16_r = __lasx_xvbitseti_h(v16u16_a, 16);              // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8u32_r = __lasx_xvbitseti_w(v8u32_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4u64_r = __lasx_xvbitseti_d(v4u64_a, 64);                // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v32u8_r = __lasx_xvbitrevi_b(v32u8_a, 8);                 // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v16u16_r = __lasx_xvbitrevi_h(v16u16_a, 16);              // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8u32_r = __lasx_xvbitrevi_w(v8u32_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4u64_r = __lasx_xvbitrevi_d(v4u64_a, 64);                // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v32i8_r = __lasx_xvaddi_bu(v32i8_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i16_r = __lasx_xvaddi_hu(v16i16_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i32_r = __lasx_xvaddi_wu(v8i32_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i64_r = __lasx_xvaddi_du(v4i64_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v32i8_r = __lasx_xvsubi_bu(v32i8_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i16_r = __lasx_xvsubi_hu(v16i16_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i32_r = __lasx_xvsubi_wu(v8i32_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i64_r = __lasx_xvsubi_du(v4i64_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v32i8_r = __lasx_xvmaxi_b(v32i8_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v16i16_r = __lasx_xvmaxi_h(v16i16_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v8i32_r = __lasx_xvmaxi_w(v8i32_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v4i64_r = __lasx_xvmaxi_d(v4i64_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v32u8_r = __lasx_xvmaxi_bu(v32u8_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16u16_r = __lasx_xvmaxi_hu(v16u16_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8u32_r = __lasx_xvmaxi_wu(v8u32_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4u64_r = __lasx_xvmaxi_du(v4u64_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v32i8_r = __lasx_xvmini_b(v32i8_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v16i16_r = __lasx_xvmini_h(v16i16_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v8i32_r = __lasx_xvmini_w(v8i32_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v4i64_r = __lasx_xvmini_d(v4i64_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v32u8_r = __lasx_xvmini_bu(v32u8_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16u16_r = __lasx_xvmini_hu(v16u16_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8u32_r = __lasx_xvmini_wu(v8u32_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4u64_r = __lasx_xvmini_du(v4u64_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v32i8_r = __lasx_xvseqi_b(v32i8_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v16i16_r = __lasx_xvseqi_h(v16i16_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v8i32_r = __lasx_xvseqi_w(v8i32_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v4i64_r = __lasx_xvseqi_d(v4i64_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v32i8_r = __lasx_xvslti_b(v32i8_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v16i16_r = __lasx_xvslti_h(v16i16_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v8i32_r = __lasx_xvslti_w(v8i32_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v4i64_r = __lasx_xvslti_d(v4i64_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v32i8_r = __lasx_xvslti_bu(v32u8_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i16_r = __lasx_xvslti_hu(v16u16_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i32_r = __lasx_xvslti_wu(v8u32_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i64_r = __lasx_xvslti_du(v4u64_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v32i8_r = __lasx_xvslei_b(v32i8_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v16i16_r = __lasx_xvslei_h(v16i16_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v8i32_r = __lasx_xvslei_w(v8i32_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v4i64_r = __lasx_xvslei_d(v4i64_a, -17);                  // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v32i8_r = __lasx_xvslei_bu(v32u8_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i16_r = __lasx_xvslei_hu(v16u16_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i32_r = __lasx_xvslei_wu(v8u32_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i64_r = __lasx_xvslei_du(v4u64_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v32i8_r = __lasx_xvsat_b(v32i8_a, 8);                     // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v16i16_r = __lasx_xvsat_h(v16i16_a, 16);                  // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i32_r = __lasx_xvsat_w(v8i32_a, 32);                    // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i64_r = __lasx_xvsat_d(v4i64_a, 64);                    // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v32u8_r = __lasx_xvsat_bu(v32u8_a, 8);                    // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v16u16_r = __lasx_xvsat_hu(v16u16_a, 16);                 // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8u32_r = __lasx_xvsat_wu(v8u32_a, 32);                   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4u64_r = __lasx_xvsat_du(v4u64_a, 64);                   // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v32i8_r = __lasx_xvrepl128vei_b(v32i8_a, 16);             // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v16i16_r = __lasx_xvrepl128vei_h(v16i16_a, 8);            // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8i32_r = __lasx_xvrepl128vei_w(v8i32_a, 4);              // expected-error {{argument value 4 is outside the valid range [0, 3]}}
+  v4i64_r = __lasx_xvrepl128vei_d(v4i64_a, 2);              // expected-error {{argument value 2 is outside the valid range [0, 1]}}
+  v32u8_r = __lasx_xvandi_b(v32u8_a, 256);                  // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v32u8_r = __lasx_xvori_b(v32u8_a, 256);                   // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v32u8_r = __lasx_xvnori_b(v32u8_a, 256);                  // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v32u8_r = __lasx_xvxori_b(v32u8_a, 256);                  // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v32u8_r = __lasx_xvbitseli_b(v32u8_a, v32u8_b, 256);      // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v32i8_r = __lasx_xvshuf4i_b(v32i8_a, 256);                // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v16i16_r = __lasx_xvshuf4i_h(v16i16_a, 256);              // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v8i32_r = __lasx_xvshuf4i_w(v8i32_a, 256);                // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v8i32_r = __lasx_xvpermi_w(v8i32_a, v8i32_b, 256);        // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v16i16_r = __lasx_xvsllwil_h_b(v32i8_a, 8);               // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8i32_r = __lasx_xvsllwil_w_h(v16i16_a, 16);              // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v4i64_r = __lasx_xvsllwil_d_w(v8i32_a, 32);               // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16u16_r = __lasx_xvsllwil_hu_bu(v32u8_a, 8);             // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8u32_r = __lasx_xvsllwil_wu_hu(v16u16_a, 16);            // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v4u64_r = __lasx_xvsllwil_du_wu(v8u32_a, 32);             // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v32i8_r = __lasx_xvfrstpi_b(v32i8_a, v32i8_b, 32);        // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i16_r = __lasx_xvfrstpi_h(v16i16_a, v16i16_b, 32);     // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i64_r = __lasx_xvshuf4i_d(v4i64_a, v4i64_b, 256);       // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v32i8_r = __lasx_xvbsrl_v(v32i8_a, 32);                   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v32i8_r = __lasx_xvbsll_v(v32i8_a, 32);                   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v32i8_r = __lasx_xvextrins_b(v32i8_a, v32i8_b, 256);      // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v16i16_r = __lasx_xvextrins_h(v16i16_a, v16i16_b, 256);   // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v8i32_r = __lasx_xvextrins_w(v8i32_a, v8i32_b, 256);      // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v4i64_r = __lasx_xvextrins_d(v4i64_a, v4i64_b, 256);      // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v32i8_r = __lasx_xvld(&v32i8_a, -2049);                   // expected-error {{argument value -2049 is outside the valid range [-2048, 2047]}}
+  __lasx_xvst(v32i8_a, &v32i8_b, -2049);                    // expected-error {{argument value -2049 is outside the valid range [-2048, 2047]}}
+  __lasx_xvstelm_b(v32i8_a, &v32i8_b, 0, 32);               // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  __lasx_xvstelm_h(v16i16_a, &v16i16_b, 0, 16);             // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  __lasx_xvstelm_w(v8i32_a, &v8i32_b, 0, 8);                // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  __lasx_xvstelm_d(v4i64_a, &v4i64_b, 0, 4);                // expected-error {{argument value 4 is outside the valid range [0, 3]}}
+  v8i32_r = __lasx_xvinsve0_w(v8i32_a, v8i32_b, 8);         // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v4i64_r = __lasx_xvinsve0_d(v4i64_a, v4i64_b, 4);         // expected-error {{argument value 4 is outside the valid range [0, 3]}}
+  v8i32_r = __lasx_xvpickve_w(v8i32_b, 8);                  // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v4i64_r = __lasx_xvpickve_d(v4i64_b, 4);                  // expected-error {{argument value 4 is outside the valid range [0, 3]}}
+  v4i64_r = __lasx_xvldi(-4097);                            // expected-error {{argument value -4097 is outside the valid range [-4096, 4095]}}
+  v8i32_r = __lasx_xvinsgr2vr_w(v8i32_a, i32_b, 8);         // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v4i64_r = __lasx_xvinsgr2vr_d(v4i64_a, i64_b, 4);         // expected-error {{argument value 4 is outside the valid range [0, 3]}}
+  v32i8_r = __lasx_xvpermi_q(v32i8_a, v32i8_b, 256);        // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v4i64_r = __lasx_xvpermi_d(v4i64_a, 256);                 // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v32i8_r = __lasx_xvldrepl_b(&v32i8_a, -2049);             // expected-error {{argument value -2049 is outside the valid range [-2048, 2047]}}
+  v16i16_r = __lasx_xvldrepl_h(&v16i16_a, -1025);           // expected-error {{argument value -1025 is outside the valid range [-1024, 1023]}}
+  v8i32_r = __lasx_xvldrepl_w(&v8i32_a, -513);              // expected-error {{argument value -513 is outside the valid range [-512, 511]}}
+  v4i64_r = __lasx_xvldrepl_d(&v4i64_a, -257);              // expected-error {{argument value -257 is outside the valid range [-256, 255]}}
+  i32_r = __lasx_xvpickve2gr_w(v8i32_a, 8);                 // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  u32_r = __lasx_xvpickve2gr_wu(v8i32_a, 8);                // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  i64_r = __lasx_xvpickve2gr_d(v4i64_a, 4);                 // expected-error {{argument value 4 is outside the valid range [0, 3]}}
+  u64_r = __lasx_xvpickve2gr_du(v4i64_a, 4);                // expected-error {{argument value 4 is outside the valid range [0, 3]}}
+  v32i8_r = __lasx_xvrotri_b(v32i8_a, 8);                   // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v16i16_r = __lasx_xvrotri_h(v16i16_a, 16);                // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i32_r = __lasx_xvrotri_w(v8i32_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i64_r = __lasx_xvrotri_d(v4i64_a, 64);                  // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v32i8_r = __lasx_xvsrlni_b_h(v32i8_a, v32i8_b, 16);       // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v16i16_r = __lasx_xvsrlni_h_w(v16i16_a, v16i16_b, 32);    // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i32_r = __lasx_xvsrlni_w_d(v8i32_a, v8i32_b, 64);       // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v4i64_r = __lasx_xvsrlni_d_q(v4i64_a, v4i64_b, 128);      // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v32i8_r = __lasx_xvsrlrni_b_h(v32i8_a, v32i8_b, 16);      // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v16i16_r = __lasx_xvsrlrni_h_w(v16i16_a, v16i16_b, 32);   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i32_r = __lasx_xvsrlrni_w_d(v8i32_a, v8i32_b, 64);      // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v4i64_r = __lasx_xvsrlrni_d_q(v4i64_a, v4i64_b, 128);     // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v32i8_r = __lasx_xvssrlni_b_h(v32i8_a, v32i8_b, 16);      // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v16i16_r = __lasx_xvssrlni_h_w(v16i16_a, v16i16_b, 32);   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i32_r = __lasx_xvssrlni_w_d(v8i32_a, v8i32_b, 64);      // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v4i64_r = __lasx_xvssrlni_d_q(v4i64_a, v4i64_b, 128);     // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v32u8_r = __lasx_xvssrlni_bu_h(v32u8_a, v32i8_b, 16);     // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v16u16_r = __lasx_xvssrlni_hu_w(v16u16_a, v16i16_b, 32);  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8u32_r = __lasx_xvssrlni_wu_d(v8u32_a, v8i32_b, 64);     // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v4u64_r = __lasx_xvssrlni_du_q(v4u64_a, v4i64_b, 128);    // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v32i8_r = __lasx_xvssrlrni_b_h(v32i8_a, v32i8_b, 16);     // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v16i16_r = __lasx_xvssrlrni_h_w(v16i16_a, v16i16_b, 32);  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i32_r = __lasx_xvssrlrni_w_d(v8i32_a, v8i32_b, 64);     // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v4i64_r = __lasx_xvssrlrni_d_q(v4i64_a, v4i64_b, 128);    // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v32u8_r = __lasx_xvssrlrni_bu_h(v32u8_a, v32i8_b, 16);    // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v16u16_r = __lasx_xvssrlrni_hu_w(v16u16_a, v16i16_b, 32); // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8u32_r = __lasx_xvssrlrni_wu_d(v8u32_a, v8i32_b, 64);    // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v4u64_r = __lasx_xvssrlrni_du_q(v4u64_a, v4i64_b, 128);   // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v32i8_r = __lasx_xvsrani_b_h(v32i8_a, v32i8_b, 16);       // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v16i16_r = __lasx_xvsrani_h_w(v16i16_a, v16i16_b, 32);    // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i32_r = __lasx_xvsrani_w_d(v8i32_a, v8i32_b, 64);       // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v4i64_r = __lasx_xvsrani_d_q(v4i64_a, v4i64_b, 128);      // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v32i8_r = __lasx_xvsrarni_b_h(v32i8_a, v32i8_b, 16);      // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v16i16_r = __lasx_xvsrarni_h_w(v16i16_a, v16i16_b, 32);   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i32_r = __lasx_xvsrarni_w_d(v8i32_a, v8i32_b, 64);      // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v4i64_r = __lasx_xvsrarni_d_q(v4i64_a, v4i64_b, 128);     // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v32i8_r = __lasx_xvssrani_b_h(v32i8_a, v32i8_b, 16);      // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v16i16_r = __lasx_xvssrani_h_w(v16i16_a, v16i16_b, 32);   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i32_r = __lasx_xvssrani_w_d(v8i32_a, v8i32_b, 64);      // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v4i64_r = __lasx_xvssrani_d_q(v4i64_a, v4i64_b, 128);     // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v32u8_r = __lasx_xvssrani_bu_h(v32u8_a, v32i8_b, 16);     // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v16u16_r = __lasx_xvssrani_hu_w(v16u16_a, v16i16_b, 32);  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8u32_r = __lasx_xvssrani_wu_d(v8u32_a, v8i32_b, 64);     // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v4u64_r = __lasx_xvssrani_du_q(v4u64_a, v4i64_b, 128);    // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v32i8_r = __lasx_xvssrarni_b_h(v32i8_a, v32i8_b, 16);     // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v16i16_r = __lasx_xvssrarni_h_w(v16i16_a, v16i16_b, 32);  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i32_r = __lasx_xvssrarni_w_d(v8i32_a, v8i32_b, 64);     // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v4i64_r = __lasx_xvssrarni_d_q(v4i64_a, v4i64_b, 128);    // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v32u8_r = __lasx_xvssrarni_bu_h(v32u8_a, v32i8_b, 16);    // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v16u16_r = __lasx_xvssrarni_hu_w(v16u16_a, v16i16_b, 32); // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8u32_r = __lasx_xvssrarni_wu_d(v8u32_a, v8i32_b, 64);    // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v4u64_r = __lasx_xvssrarni_du_q(v4u64_a, v4i64_b, 128);   // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+}
diff --git a/test/CodeGen/builtins-loongarch-lasx.c b/test/CodeGen/builtins-loongarch-lasx.c
new file mode 100644
index 00000000..b9ec3a3c
--- /dev/null
+++ b/test/CodeGen/builtins-loongarch-lasx.c
@@ -0,0 +1,3761 @@
+// REQUIRES: loongarch-registered-target
+// RUN: %clang_cc1 -triple loongarch64-unknown-linux-gnu -emit-llvm %s \
+// RUN:            -target-feature +lasx \
+// RUN:            -target-feature +d \
+// RUN:            -o - | FileCheck %s
+
+#include <lasxintrin.h>
+
+#define ui1_b 1
+#define ui2 1
+#define ui2_b ui2
+#define ui3 4
+#define ui3_b ui3
+#define ui4 7
+#define ui4_b ui4
+#define ui5 25
+#define ui5_b ui5
+#define ui6 44
+#define ui6_b ui6
+#define ui7 100
+#define ui7_b ui7
+#define ui8 127 //200
+#define ui8_b ui8
+#define si5_b -4
+#define si8 -100
+#define si9 0
+#define si10 0
+#define si11 0
+#define si12 0
+#define i10 500
+#define i13 4000
+#define mode 0
+#define idx1 1
+#define idx2 2
+#define idx3 4
+#define idx4 8
+
+void test(void) {
+  v32i8 v32i8_a = (v32i8){0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
+                          16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31};
+  v32i8 v32i8_b = (v32i8){1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
+                          17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32};
+  v32i8 v32i8_c = (v32i8){2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,
+                          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33};
+  v32i8 v32i8_r;
+
+  v16i16 v16i16_a = (v16i16){0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
+  v16i16 v16i16_b = (v16i16){1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
+  v16i16 v16i16_c = (v16i16){2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17};
+  v16i16 v16i16_r;
+
+  v8i32 v8i32_a = (v8i32){0, 1, 2, 3, 4, 5, 6, 7};
+  v8i32 v8i32_b = (v8i32){1, 2, 3, 4, 5, 6, 7, 8};
+  v8i32 v8i32_c = (v8i32){2, 3, 4, 5, 6, 7, 8, 9};
+  v8i32 v8i32_r;
+
+  v4i64 v4i64_a = (v4i64){0, 1, 2, 3};
+  v4i64 v4i64_b = (v4i64){1, 2, 3, 4};
+  v4i64 v4i64_c = (v4i64){2, 3, 4, 5};
+  v4i64 v4i64_r;
+
+  v32u8 v32u8_a = (v32u8){0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
+                          16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31};
+  v32u8 v32u8_b = (v32u8){1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
+                          17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32};
+  v32u8 v32u8_c = (v32u8){2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,
+                          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33};
+  v32u8 v32u8_r;
+
+  v16u16 v16u16_a = (v16u16){0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
+  v16u16 v16u16_b = (v16u16){1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
+  v16u16 v16u16_c = (v16u16){2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17};
+  v16u16 v16u16_r;
+
+  v8u32 v8u32_a = (v8u32){0, 1, 2, 3, 4, 5, 6, 7};
+  v8u32 v8u32_b = (v8u32){1, 2, 3, 4, 5, 6, 7, 8};
+  v8u32 v8u32_c = (v8u32){2, 3, 4, 5, 6, 7, 8, 9};
+  v8u32 v8u32_r;
+
+  v4u64 v4u64_a = (v4u64){0, 1, 2, 3};
+  v4u64 v4u64_b = (v4u64){1, 2, 3, 4};
+  v4u64 v4u64_c = (v4u64){2, 3, 4, 5};
+  v4u64 v4u64_r;
+
+  v8f32 v8f32_a = (v8f32){0.5, 1, 2, 3, 4, 5, 6, 7};
+  v8f32 v8f32_b = (v8f32){1.5, 2, 3, 4, 5, 6, 7, 8};
+  v8f32 v8f32_c = (v8f32){2.5, 3, 4, 5, 6, 7, 8, 9};
+  v8f32 v8f32_r;
+  v4f64 v4f64_a = (v4f64){0.5, 1, 2, 3};
+  v4f64 v4f64_b = (v4f64){1.5, 2, 3, 4};
+  v4f64 v4f64_c = (v4f64){2.5, 3, 4, 5};
+  v4f64 v4f64_r;
+
+  int i32_r;
+  int i32_a = 1;
+  int i32_b = 2;
+  unsigned int u32_r;
+  unsigned int u32_a = 1;
+  unsigned int u32_b = 2;
+  long long i64_r;
+  long long i64_a = 1;
+  long long i64_b = 2;
+  long long i64_c = 3;
+  long int i64_d = 0;
+  unsigned long long u64_r;
+  unsigned long long u64_a = 1;
+  unsigned long long u64_b = 2;
+  unsigned long long u64_c = 3;
+
+  // __lasx_xvsll_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvsll_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsll.b(
+
+  // __lasx_xvsll_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvsll_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsll.h(
+
+  // __lasx_xvsll_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvsll_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsll.w(
+
+  // __lasx_xvsll_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvsll_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsll.d(
+
+  // __lasx_xvslli_b
+  // xd, xj, ui3
+  // V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvslli_b(v32i8_a, ui3_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvslli.b(
+
+  // __lasx_xvslli_h
+  // xd, xj, ui4
+  // V16HI, V16HI, UQI
+  v16i16_r = __lasx_xvslli_h(v16i16_a, ui4_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvslli.h(
+
+  // __lasx_xvslli_w
+  // xd, xj, ui5
+  // V8SI, V8SI, UQI
+  v8i32_r = __lasx_xvslli_w(v8i32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvslli.w(
+
+  // __lasx_xvslli_d
+  // xd, xj, ui6
+  // V4DI, V4DI, UQI
+  v4i64_r = __lasx_xvslli_d(v4i64_a, ui6_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvslli.d(
+
+  // __lasx_xvsra_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvsra_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsra.b(
+
+  // __lasx_xvsra_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvsra_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsra.h(
+
+  // __lasx_xvsra_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvsra_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsra.w(
+
+  // __lasx_xvsra_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvsra_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsra.d(
+
+  // __lasx_xvsrai_b
+  // xd, xj, ui3
+  // V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvsrai_b(v32i8_a, ui3_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrai.b(
+
+  // __lasx_xvsrai_h
+  // xd, xj, ui4
+  // V16HI, V16HI, UQI
+  v16i16_r = __lasx_xvsrai_h(v16i16_a, ui4_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrai.h(
+
+  // __lasx_xvsrai_w
+  // xd, xj, ui5
+  // V8SI, V8SI, UQI
+  v8i32_r = __lasx_xvsrai_w(v8i32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrai.w(
+
+  // __lasx_xvsrai_d
+  // xd, xj, ui6
+  // V4DI, V4DI, UQI
+  v4i64_r = __lasx_xvsrai_d(v4i64_a, ui6_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsrai.d(
+
+  // __lasx_xvsrar_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvsrar_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrar.b(
+
+  // __lasx_xvsrar_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvsrar_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrar.h(
+
+  // __lasx_xvsrar_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvsrar_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrar.w(
+
+  // __lasx_xvsrar_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvsrar_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsrar.d(
+
+  // __lasx_xvsrari_b
+  // xd, xj, ui3
+  // V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvsrari_b(v32i8_a, ui3_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrari.b(
+
+  // __lasx_xvsrari_h
+  // xd, xj, ui4
+  // V16HI, V16HI, UQI
+  v16i16_r = __lasx_xvsrari_h(v16i16_a, ui4_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrari.h(
+
+  // __lasx_xvsrari_w
+  // xd, xj, ui5
+  // V8SI, V8SI, UQI
+  v8i32_r = __lasx_xvsrari_w(v8i32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrari.w(
+
+  // __lasx_xvsrari_d
+  // xd, xj, ui6
+  // V4DI, V4DI, UQI
+  v4i64_r = __lasx_xvsrari_d(v4i64_a, ui6_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsrari.d(
+
+  // __lasx_xvsrl_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvsrl_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrl.b(
+
+  // __lasx_xvsrl_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvsrl_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrl.h(
+
+  // __lasx_xvsrl_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvsrl_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrl.w(
+
+  // __lasx_xvsrl_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvsrl_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsrl.d(
+
+  // __lasx_xvsrli_b
+  // xd, xj, ui3
+  // V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvsrli_b(v32i8_a, ui3_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrli.b(
+
+  // __lasx_xvsrli_h
+  // xd, xj, ui4
+  // V16HI, V16HI, UQI
+  v16i16_r = __lasx_xvsrli_h(v16i16_a, ui4_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrli.h(
+
+  // __lasx_xvsrli_w
+  // xd, xj, ui5
+  // V8SI, V8SI, UQI
+  v8i32_r = __lasx_xvsrli_w(v8i32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrli.w(
+
+  // __lasx_xvsrli_d
+  // xd, xj, ui6
+  // V4DI, V4DI, UQI
+  v4i64_r = __lasx_xvsrli_d(v4i64_a, ui6_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsrli.d(
+
+  // __lasx_xvsrlr_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvsrlr_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrlr.b(
+
+  // __lasx_xvsrlr_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvsrlr_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrlr.h(
+
+  // __lasx_xvsrlr_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvsrlr_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrlr.w(
+
+  // __lasx_xvsrlr_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvsrlr_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsrlr.d(
+
+  // __lasx_xvsrlri_b
+  // xd, xj, ui3
+  // V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvsrlri_b(v32i8_a, ui3_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrlri.b(
+
+  // __lasx_xvsrlri_h
+  // xd, xj, ui4
+  // V16HI, V16HI, UQI
+  v16i16_r = __lasx_xvsrlri_h(v16i16_a, ui4_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrlri.h(
+
+  // __lasx_xvsrlri_w
+  // xd, xj, ui5
+  // V8SI, V8SI, UQI
+  v8i32_r = __lasx_xvsrlri_w(v8i32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrlri.w(
+
+  // __lasx_xvsrlri_d
+  // xd, xj, ui6
+  // V4DI, V4DI, UQI
+  v4i64_r = __lasx_xvsrlri_d(v4i64_a, ui6_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsrlri.d(
+
+  // __lasx_xvbitclr_b
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvbitclr_b(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvbitclr.b(
+
+  // __lasx_xvbitclr_h
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV16HI
+  v16u16_r = __lasx_xvbitclr_h(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvbitclr.h(
+
+  // __lasx_xvbitclr_w
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV8SI
+  v8u32_r = __lasx_xvbitclr_w(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvbitclr.w(
+
+  // __lasx_xvbitclr_d
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvbitclr_d(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvbitclr.d(
+
+  // __lasx_xvbitclri_b
+  // xd, xj, ui3
+  // UV32QI, UV32QI, UQI
+  v32u8_r = __lasx_xvbitclri_b(v32u8_a, ui3_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvbitclri.b(
+
+  // __lasx_xvbitclri_h
+  // xd, xj, ui4
+  // UV16HI, UV16HI, UQI
+  v16u16_r = __lasx_xvbitclri_h(v16u16_a, ui4_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvbitclri.h(
+
+  // __lasx_xvbitclri_w
+  // xd, xj, ui5
+  // UV8SI, UV8SI, UQI
+  v8u32_r = __lasx_xvbitclri_w(v8u32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvbitclri.w(
+
+  // __lasx_xvbitclri_d
+  // xd, xj, ui6
+  // UV4DI, UV4DI, UQI
+  v4u64_r = __lasx_xvbitclri_d(v4u64_a, ui6_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvbitclri.d(
+
+  // __lasx_xvbitset_b
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvbitset_b(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvbitset.b(
+
+  // __lasx_xvbitset_h
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV16HI
+  v16u16_r = __lasx_xvbitset_h(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvbitset.h(
+
+  // __lasx_xvbitset_w
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV8SI
+  v8u32_r = __lasx_xvbitset_w(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvbitset.w(
+
+  // __lasx_xvbitset_d
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvbitset_d(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvbitset.d(
+
+  // __lasx_xvbitseti_b
+  // xd, xj, ui3
+  // UV32QI, UV32QI, UQI
+  v32u8_r = __lasx_xvbitseti_b(v32u8_a, ui3_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvbitseti.b(
+
+  // __lasx_xvbitseti_h
+  // xd, xj, ui4
+  // UV16HI, UV16HI, UQI
+  v16u16_r = __lasx_xvbitseti_h(v16u16_a, ui4_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvbitseti.h(
+
+  // __lasx_xvbitseti_w
+  // xd, xj, ui5
+  // UV8SI, UV8SI, UQI
+  v8u32_r = __lasx_xvbitseti_w(v8u32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvbitseti.w(
+
+  // __lasx_xvbitseti_d
+  // xd, xj, ui6
+  // UV4DI, UV4DI, UQI
+  v4u64_r = __lasx_xvbitseti_d(v4u64_a, ui6_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvbitseti.d(
+
+  // __lasx_xvbitrev_b
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvbitrev_b(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvbitrev.b(
+
+  // __lasx_xvbitrev_h
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV16HI
+  v16u16_r = __lasx_xvbitrev_h(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvbitrev.h(
+
+  // __lasx_xvbitrev_w
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV8SI
+  v8u32_r = __lasx_xvbitrev_w(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvbitrev.w(
+
+  // __lasx_xvbitrev_d
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvbitrev_d(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvbitrev.d(
+
+  // __lasx_xvbitrevi_b
+  // xd, xj, ui3
+  // UV32QI, UV32QI, UQI
+  v32u8_r = __lasx_xvbitrevi_b(v32u8_a, ui3_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvbitrevi.b(
+
+  // __lasx_xvbitrevi_h
+  // xd, xj, ui4
+  // UV16HI, UV16HI, UQI
+  v16u16_r = __lasx_xvbitrevi_h(v16u16_a, ui4_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvbitrevi.h(
+
+  // __lasx_xvbitrevi_w
+  // xd, xj, ui5
+  // UV8SI, UV8SI, UQI
+  v8u32_r = __lasx_xvbitrevi_w(v8u32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvbitrevi.w(
+
+  // __lasx_xvbitrevi_d
+  // xd, xj, ui6
+  // UV4DI, UV4DI, UQI
+  v4u64_r = __lasx_xvbitrevi_d(v4u64_a, ui6_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvbitrevi.d(
+
+  // __lasx_xvadd_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvadd_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvadd.b(
+
+  // __lasx_xvadd_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvadd_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvadd.h(
+
+  // __lasx_xvadd_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvadd_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvadd.w(
+
+  // __lasx_xvadd_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvadd_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvadd.d(
+
+  // __lasx_xvaddi_bu
+  // xd, xj, ui5
+  // V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvaddi_bu(v32i8_a, ui5_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvaddi.bu(
+
+  // __lasx_xvaddi_hu
+  // xd, xj, ui5
+  // V16HI, V16HI, UQI
+  v16i16_r = __lasx_xvaddi_hu(v16i16_a, ui5_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvaddi.hu(
+
+  // __lasx_xvaddi_wu
+  // xd, xj, ui5
+  // V8SI, V8SI, UQI
+  v8i32_r = __lasx_xvaddi_wu(v8i32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvaddi.wu(
+
+  // __lasx_xvaddi_du
+  // xd, xj, ui5
+  // V4DI, V4DI, UQI
+  v4i64_r = __lasx_xvaddi_du(v4i64_a, ui5_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvaddi.du(
+
+  // __lasx_xvsub_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvsub_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsub.b(
+
+  // __lasx_xvsub_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvsub_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsub.h(
+
+  // __lasx_xvsub_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvsub_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsub.w(
+
+  // __lasx_xvsub_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvsub_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsub.d(
+
+  // __lasx_xvsubi_bu
+  // xd, xj, ui5
+  // V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvsubi_bu(v32i8_a, ui5_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsubi.bu(
+
+  // __lasx_xvsubi_hu
+  // xd, xj, ui5
+  // V16HI, V16HI, UQI
+  v16i16_r = __lasx_xvsubi_hu(v16i16_a, ui5_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsubi.hu(
+
+  // __lasx_xvsubi_wu
+  // xd, xj, ui5
+  // V8SI, V8SI, UQI
+  v8i32_r = __lasx_xvsubi_wu(v8i32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsubi.wu(
+
+  // __lasx_xvsubi_du
+  // xd, xj, ui5
+  // V4DI, V4DI, UQI
+  v4i64_r = __lasx_xvsubi_du(v4i64_a, ui5_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsubi.du(
+
+  // __lasx_xvmax_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvmax_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmax.b(
+
+  // __lasx_xvmax_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvmax_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmax.h(
+
+  // __lasx_xvmax_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvmax_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmax.w(
+
+  // __lasx_xvmax_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvmax_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmax.d(
+
+  // __lasx_xvmaxi_b
+  // xd, xj, si5
+  // V32QI, V32QI, QI
+  v32i8_r = __lasx_xvmaxi_b(v32i8_a, si5_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmaxi.b(
+
+  // __lasx_xvmaxi_h
+  // xd, xj, si5
+  // V16HI, V16HI, QI
+  v16i16_r = __lasx_xvmaxi_h(v16i16_a, si5_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmaxi.h(
+
+  // __lasx_xvmaxi_w
+  // xd, xj, si5
+  // V8SI, V8SI, QI
+  v8i32_r = __lasx_xvmaxi_w(v8i32_a, si5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmaxi.w(
+
+  // __lasx_xvmaxi_d
+  // xd, xj, si5
+  // V4DI, V4DI, QI
+  v4i64_r = __lasx_xvmaxi_d(v4i64_a, si5_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaxi.d(
+
+  // __lasx_xvmax_bu
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvmax_bu(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmax.bu(
+
+  // __lasx_xvmax_hu
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV16HI
+  v16u16_r = __lasx_xvmax_hu(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmax.hu(
+
+  // __lasx_xvmax_wu
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV8SI
+  v8u32_r = __lasx_xvmax_wu(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmax.wu(
+
+  // __lasx_xvmax_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvmax_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmax.du(
+
+  // __lasx_xvmaxi_bu
+  // xd, xj, ui5
+  // UV32QI, UV32QI, UQI
+  v32u8_r = __lasx_xvmaxi_bu(v32u8_a, ui5_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmaxi.bu(
+
+  // __lasx_xvmaxi_hu
+  // xd, xj, ui5
+  // UV16HI, UV16HI, UQI
+  v16u16_r = __lasx_xvmaxi_hu(v16u16_a, ui5_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmaxi.hu(
+
+  // __lasx_xvmaxi_wu
+  // xd, xj, ui5
+  // UV8SI, UV8SI, UQI
+  v8u32_r = __lasx_xvmaxi_wu(v8u32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmaxi.wu(
+
+  // __lasx_xvmaxi_du
+  // xd, xj, ui5
+  // UV4DI, UV4DI, UQI
+  v4u64_r = __lasx_xvmaxi_du(v4u64_a, ui5_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaxi.du(
+
+  // __lasx_xvmin_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvmin_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmin.b(
+
+  // __lasx_xvmin_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvmin_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmin.h(
+
+  // __lasx_xvmin_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvmin_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmin.w(
+
+  // __lasx_xvmin_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvmin_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmin.d(
+
+  // __lasx_xvmini_b
+  // xd, xj, si5
+  // V32QI, V32QI, QI
+  v32i8_r = __lasx_xvmini_b(v32i8_a, si5_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmini.b(
+
+  // __lasx_xvmini_h
+  // xd, xj, si5
+  // V16HI, V16HI, QI
+  v16i16_r = __lasx_xvmini_h(v16i16_a, si5_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmini.h(
+
+  // __lasx_xvmini_w
+  // xd, xj, si5
+  // V8SI, V8SI, QI
+  v8i32_r = __lasx_xvmini_w(v8i32_a, si5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmini.w(
+
+  // __lasx_xvmini_d
+  // xd, xj, si5
+  // V4DI, V4DI, QI
+  v4i64_r = __lasx_xvmini_d(v4i64_a, si5_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmini.d(
+
+  // __lasx_xvmin_bu
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvmin_bu(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmin.bu(
+
+  // __lasx_xvmin_hu
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV16HI
+  v16u16_r = __lasx_xvmin_hu(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmin.hu(
+
+  // __lasx_xvmin_wu
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV8SI
+  v8u32_r = __lasx_xvmin_wu(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmin.wu(
+
+  // __lasx_xvmin_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvmin_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmin.du(
+
+  // __lasx_xvmini_bu
+  // xd, xj, ui5
+  // UV32QI, UV32QI, UQI
+  v32u8_r = __lasx_xvmini_bu(v32u8_a, ui5_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmini.bu(
+
+  // __lasx_xvmini_hu
+  // xd, xj, ui5
+  // UV16HI, UV16HI, UQI
+  v16u16_r = __lasx_xvmini_hu(v16u16_a, ui5_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmini.hu(
+
+  // __lasx_xvmini_wu
+  // xd, xj, ui5
+  // UV8SI, UV8SI, UQI
+  v8u32_r = __lasx_xvmini_wu(v8u32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmini.wu(
+
+  // __lasx_xvmini_du
+  // xd, xj, ui5
+  // UV4DI, UV4DI, UQI
+  v4u64_r = __lasx_xvmini_du(v4u64_a, ui5_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmini.du(
+
+  // __lasx_xvseq_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvseq_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvseq.b(
+
+  // __lasx_xvseq_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvseq_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvseq.h(
+
+  // __lasx_xvseq_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvseq_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvseq.w(
+
+  // __lasx_xvseq_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvseq_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvseq.d(
+
+  // __lasx_xvseqi_b
+  // xd, xj, si5
+  // V32QI, V32QI, QI
+  v32i8_r = __lasx_xvseqi_b(v32i8_a, si5_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvseqi.b(
+
+  // __lasx_xvseqi_h
+  // xd, xj, si5
+  // V16HI, V16HI, QI
+  v16i16_r = __lasx_xvseqi_h(v16i16_a, si5_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvseqi.h(
+
+  // __lasx_xvseqi_w
+  // xd, xj, si5
+  // V8SI, V8SI, QI
+  v8i32_r = __lasx_xvseqi_w(v8i32_a, si5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvseqi.w(
+
+  // __lasx_xvseqi_d
+  // xd, xj, si5
+  // V4DI, V4DI, QI
+  v4i64_r = __lasx_xvseqi_d(v4i64_a, si5_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvseqi.d(
+
+  // __lasx_xvslt_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvslt_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvslt.b(
+
+  // __lasx_xvslt_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvslt_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvslt.h(
+
+  // __lasx_xvslt_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvslt_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvslt.w(
+
+  // __lasx_xvslt_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvslt_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvslt.d(
+
+  // __lasx_xvslti_b
+  // xd, xj, si5
+  // V32QI, V32QI, QI
+  v32i8_r = __lasx_xvslti_b(v32i8_a, si5_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvslti.b(
+
+  // __lasx_xvslti_h
+  // xd, xj, si5
+  // V16HI, V16HI, QI
+  v16i16_r = __lasx_xvslti_h(v16i16_a, si5_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvslti.h(
+
+  // __lasx_xvslti_w
+  // xd, xj, si5
+  // V8SI, V8SI, QI
+  v8i32_r = __lasx_xvslti_w(v8i32_a, si5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvslti.w(
+
+  // __lasx_xvslti_d
+  // xd, xj, si5
+  // V4DI, V4DI, QI
+  v4i64_r = __lasx_xvslti_d(v4i64_a, si5_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvslti.d(
+
+  // __lasx_xvslt_bu
+  // xd, xj, xk
+  // V32QI, UV32QI, UV32QI
+  v32i8_r = __lasx_xvslt_bu(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvslt.bu(
+
+  // __lasx_xvslt_hu
+  // xd, xj, xk
+  // V16HI, UV16HI, UV16HI
+  v16i16_r = __lasx_xvslt_hu(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvslt.hu(
+
+  // __lasx_xvslt_wu
+  // xd, xj, xk
+  // V8SI, UV8SI, UV8SI
+  v8i32_r = __lasx_xvslt_wu(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvslt.wu(
+
+  // __lasx_xvslt_du
+  // xd, xj, xk
+  // V4DI, UV4DI, UV4DI
+  v4i64_r = __lasx_xvslt_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvslt.du(
+
+  // __lasx_xvslti_bu
+  // xd, xj, ui5
+  // V32QI, UV32QI, UQI
+  v32i8_r = __lasx_xvslti_bu(v32u8_a, ui5_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvslti.bu(
+
+  // __lasx_xvslti_hu
+  // xd, xj, ui5
+  // V16HI, UV16HI, UQI
+  v16i16_r = __lasx_xvslti_hu(v16u16_a, ui5_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvslti.hu(
+
+  // __lasx_xvslti_wu
+  // xd, xj, ui5
+  // V8SI, UV8SI, UQI
+  v8i32_r = __lasx_xvslti_wu(v8u32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvslti.wu(
+
+  // __lasx_xvslti_du
+  // xd, xj, ui5
+  // V4DI, UV4DI, UQI
+  v4i64_r = __lasx_xvslti_du(v4u64_a, ui5_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvslti.du(
+
+  // __lasx_xvsle_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvsle_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsle.b(
+
+  // __lasx_xvsle_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvsle_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsle.h(
+
+  // __lasx_xvsle_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvsle_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsle.w(
+
+  // __lasx_xvsle_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvsle_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsle.d(
+
+  // __lasx_xvslei_b
+  // xd, xj, si5
+  // V32QI, V32QI, QI
+  v32i8_r = __lasx_xvslei_b(v32i8_a, si5_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvslei.b(
+
+  // __lasx_xvslei_h
+  // xd, xj, si5
+  // V16HI, V16HI, QI
+  v16i16_r = __lasx_xvslei_h(v16i16_a, si5_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvslei.h(
+
+  // __lasx_xvslei_w
+  // xd, xj, si5
+  // V8SI, V8SI, QI
+  v8i32_r = __lasx_xvslei_w(v8i32_a, si5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvslei.w(
+
+  // __lasx_xvslei_d
+  // xd, xj, si5
+  // V4DI, V4DI, QI
+  v4i64_r = __lasx_xvslei_d(v4i64_a, si5_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvslei.d(
+
+  // __lasx_xvsle_bu
+  // xd, xj, xk
+  // V32QI, UV32QI, UV32QI
+  v32i8_r = __lasx_xvsle_bu(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsle.bu(
+
+  // __lasx_xvsle_hu
+  // xd, xj, xk
+  // V16HI, UV16HI, UV16HI
+  v16i16_r = __lasx_xvsle_hu(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsle.hu(
+
+  // __lasx_xvsle_wu
+  // xd, xj, xk
+  // V8SI, UV8SI, UV8SI
+  v8i32_r = __lasx_xvsle_wu(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsle.wu(
+
+  // __lasx_xvsle_du
+  // xd, xj, xk
+  // V4DI, UV4DI, UV4DI
+  v4i64_r = __lasx_xvsle_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsle.du(
+
+  // __lasx_xvslei_bu
+  // xd, xj, ui5
+  // V32QI, UV32QI, UQI
+  v32i8_r = __lasx_xvslei_bu(v32u8_a, ui5_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvslei.bu(
+
+  // __lasx_xvslei_hu
+  // xd, xj, ui5
+  // V16HI, UV16HI, UQI
+  v16i16_r = __lasx_xvslei_hu(v16u16_a, ui5_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvslei.hu(
+
+  // __lasx_xvslei_wu
+  // xd, xj, ui5
+  // V8SI, UV8SI, UQI
+  v8i32_r = __lasx_xvslei_wu(v8u32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvslei.wu(
+
+  // __lasx_xvslei_du
+  // xd, xj, ui5
+  // V4DI, UV4DI, UQI
+  v4i64_r = __lasx_xvslei_du(v4u64_a, ui5_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvslei.du(
+
+  // __lasx_xvsat_b
+  // xd, xj, ui3
+  // V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvsat_b(v32i8_a, ui3_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsat.b(
+
+  // __lasx_xvsat_h
+  // xd, xj, ui4
+  // V16HI, V16HI, UQI
+  v16i16_r = __lasx_xvsat_h(v16i16_a, ui4_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsat.h(
+
+  // __lasx_xvsat_w
+  // xd, xj, ui5
+  // V8SI, V8SI, UQI
+  v8i32_r = __lasx_xvsat_w(v8i32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsat.w(
+
+  // __lasx_xvsat_d
+  // xd, xj, ui6
+  // V4DI, V4DI, UQI
+  v4i64_r = __lasx_xvsat_d(v4i64_a, ui6_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsat.d(
+
+  // __lasx_xvsat_bu
+  // xd, xj, ui3
+  // UV32QI, UV32QI, UQI
+  v32u8_r = __lasx_xvsat_bu(v32u8_a, ui3_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsat.bu(
+
+  // __lasx_xvsat_hu
+  // xd, xj, ui4
+  // UV16HI, UV16HI, UQI
+  v16u16_r = __lasx_xvsat_hu(v16u16_a, ui4_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsat.hu(
+
+  // __lasx_xvsat_wu
+  // xd, xj, ui5
+  // UV8SI, UV8SI, UQI
+  v8u32_r = __lasx_xvsat_wu(v8u32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsat.wu(
+
+  // __lasx_xvsat_du
+  // xd, xj, ui6
+  // UV4DI, UV4DI, UQI
+  v4u64_r = __lasx_xvsat_du(v4u64_a, ui6_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsat.du(
+
+  // __lasx_xvadda_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvadda_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvadda.b(
+
+  // __lasx_xvadda_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvadda_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvadda.h(
+
+  // __lasx_xvadda_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvadda_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvadda.w(
+
+  // __lasx_xvadda_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvadda_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvadda.d(
+
+  // __lasx_xvsadd_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvsadd_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsadd.b(
+
+  // __lasx_xvsadd_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvsadd_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsadd.h(
+
+  // __lasx_xvsadd_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvsadd_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsadd.w(
+
+  // __lasx_xvsadd_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvsadd_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsadd.d(
+
+  // __lasx_xvsadd_bu
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvsadd_bu(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsadd.bu(
+
+  // __lasx_xvsadd_hu
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV16HI
+  v16u16_r = __lasx_xvsadd_hu(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsadd.hu(
+
+  // __lasx_xvsadd_wu
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV8SI
+  v8u32_r = __lasx_xvsadd_wu(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsadd.wu(
+
+  // __lasx_xvsadd_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvsadd_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsadd.du(
+
+  // __lasx_xvavg_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvavg_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvavg.b(
+
+  // __lasx_xvavg_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvavg_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvavg.h(
+
+  // __lasx_xvavg_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvavg_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvavg.w(
+
+  // __lasx_xvavg_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvavg_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvavg.d(
+
+  // __lasx_xvavg_bu
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvavg_bu(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvavg.bu(
+
+  // __lasx_xvavg_hu
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV16HI
+  v16u16_r = __lasx_xvavg_hu(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvavg.hu(
+
+  // __lasx_xvavg_wu
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV8SI
+  v8u32_r = __lasx_xvavg_wu(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvavg.wu(
+
+  // __lasx_xvavg_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvavg_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvavg.du(
+
+  // __lasx_xvavgr_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvavgr_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvavgr.b(
+
+  // __lasx_xvavgr_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvavgr_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvavgr.h(
+
+  // __lasx_xvavgr_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvavgr_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvavgr.w(
+
+  // __lasx_xvavgr_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvavgr_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvavgr.d(
+
+  // __lasx_xvavgr_bu
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvavgr_bu(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvavgr.bu(
+
+  // __lasx_xvavgr_hu
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV16HI
+  v16u16_r = __lasx_xvavgr_hu(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvavgr.hu(
+
+  // __lasx_xvavgr_wu
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV8SI
+  v8u32_r = __lasx_xvavgr_wu(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvavgr.wu(
+
+  // __lasx_xvavgr_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvavgr_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvavgr.du(
+
+  // __lasx_xvssub_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvssub_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssub.b(
+
+  // __lasx_xvssub_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvssub_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssub.h(
+
+  // __lasx_xvssub_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvssub_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssub.w(
+
+  // __lasx_xvssub_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvssub_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvssub.d(
+
+  // __lasx_xvssub_bu
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvssub_bu(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssub.bu(
+
+  // __lasx_xvssub_hu
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV16HI
+  v16u16_r = __lasx_xvssub_hu(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssub.hu(
+
+  // __lasx_xvssub_wu
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV8SI
+  v8u32_r = __lasx_xvssub_wu(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssub.wu(
+
+  // __lasx_xvssub_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvssub_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvssub.du(
+
+  // __lasx_xvabsd_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvabsd_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvabsd.b(
+
+  // __lasx_xvabsd_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvabsd_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvabsd.h(
+
+  // __lasx_xvabsd_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvabsd_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvabsd.w(
+
+  // __lasx_xvabsd_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvabsd_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvabsd.d(
+
+  // __lasx_xvabsd_bu
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvabsd_bu(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvabsd.bu(
+
+  // __lasx_xvabsd_hu
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV16HI
+  v16u16_r = __lasx_xvabsd_hu(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvabsd.hu(
+
+  // __lasx_xvabsd_wu
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV8SI
+  v8u32_r = __lasx_xvabsd_wu(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvabsd.wu(
+
+  // __lasx_xvabsd_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvabsd_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvabsd.du(
+
+  // __lasx_xvmul_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvmul_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmul.b(
+
+  // __lasx_xvmul_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvmul_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmul.h(
+
+  // __lasx_xvmul_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvmul_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmul.w(
+
+  // __lasx_xvmul_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvmul_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmul.d(
+
+  // __lasx_xvmadd_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvmadd_b(v32i8_a, v32i8_b, v32i8_c); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmadd.b(
+
+  // __lasx_xvmadd_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvmadd_h(v16i16_a, v16i16_b, v16i16_c); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmadd.h(
+
+  // __lasx_xvmadd_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvmadd_w(v8i32_a, v8i32_b, v8i32_c); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmadd.w(
+
+  // __lasx_xvmadd_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvmadd_d(v4i64_a, v4i64_b, v4i64_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmadd.d(
+
+  // __lasx_xvmsub_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvmsub_b(v32i8_a, v32i8_b, v32i8_c); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmsub.b(
+
+  // __lasx_xvmsub_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvmsub_h(v16i16_a, v16i16_b, v16i16_c); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmsub.h(
+
+  // __lasx_xvmsub_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvmsub_w(v8i32_a, v8i32_b, v8i32_c); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmsub.w(
+
+  // __lasx_xvmsub_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvmsub_d(v4i64_a, v4i64_b, v4i64_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmsub.d(
+
+  // __lasx_xvdiv_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvdiv_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvdiv.b(
+
+  // __lasx_xvdiv_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvdiv_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvdiv.h(
+
+  // __lasx_xvdiv_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvdiv_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvdiv.w(
+
+  // __lasx_xvdiv_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvdiv_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvdiv.d(
+
+  // __lasx_xvdiv_bu
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvdiv_bu(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvdiv.bu(
+
+  // __lasx_xvdiv_hu
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV16HI
+  v16u16_r = __lasx_xvdiv_hu(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvdiv.hu(
+
+  // __lasx_xvdiv_wu
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV8SI
+  v8u32_r = __lasx_xvdiv_wu(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvdiv.wu(
+
+  // __lasx_xvdiv_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvdiv_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvdiv.du(
+
+  // __lasx_xvhaddw_h_b
+  // xd, xj, xk
+  // V16HI, V32QI, V32QI
+  v16i16_r = __lasx_xvhaddw_h_b(v32i8_a, v32i8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvhaddw.h.b(
+
+  // __lasx_xvhaddw_w_h
+  // xd, xj, xk
+  // V8SI, V16HI, V16HI
+  v8i32_r = __lasx_xvhaddw_w_h(v16i16_a, v16i16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvhaddw.w.h(
+
+  // __lasx_xvhaddw_d_w
+  // xd, xj, xk
+  // V4DI, V8SI, V8SI
+  v4i64_r = __lasx_xvhaddw_d_w(v8i32_a, v8i32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvhaddw.d.w(
+
+  // __lasx_xvhaddw_hu_bu
+  // xd, xj, xk
+  // UV16HI, UV32QI, UV32QI
+  v16u16_r = __lasx_xvhaddw_hu_bu(v32u8_a, v32u8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvhaddw.hu.bu(
+
+  // __lasx_xvhaddw_wu_hu
+  // xd, xj, xk
+  // UV8SI, UV16HI, UV16HI
+  v8u32_r = __lasx_xvhaddw_wu_hu(v16u16_a, v16u16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvhaddw.wu.hu(
+
+  // __lasx_xvhaddw_du_wu
+  // xd, xj, xk
+  // UV4DI, UV8SI, UV8SI
+  v4u64_r = __lasx_xvhaddw_du_wu(v8u32_a, v8u32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvhaddw.du.wu(
+
+  // __lasx_xvhsubw_h_b
+  // xd, xj, xk
+  // V16HI, V32QI, V32QI
+  v16i16_r = __lasx_xvhsubw_h_b(v32i8_a, v32i8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvhsubw.h.b(
+
+  // __lasx_xvhsubw_w_h
+  // xd, xj, xk
+  // V8SI, V16HI, V16HI
+  v8i32_r = __lasx_xvhsubw_w_h(v16i16_a, v16i16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvhsubw.w.h(
+
+  // __lasx_xvhsubw_d_w
+  // xd, xj, xk
+  // V4DI, V8SI, V8SI
+  v4i64_r = __lasx_xvhsubw_d_w(v8i32_a, v8i32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvhsubw.d.w(
+
+  // __lasx_xvhsubw_hu_bu
+  // xd, xj, xk
+  // V16HI, UV32QI, UV32QI
+  v16i16_r = __lasx_xvhsubw_hu_bu(v32u8_a, v32u8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvhsubw.hu.bu(
+
+  // __lasx_xvhsubw_wu_hu
+  // xd, xj, xk
+  // V8SI, UV16HI, UV16HI
+  v8i32_r = __lasx_xvhsubw_wu_hu(v16u16_a, v16u16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvhsubw.wu.hu(
+
+  // __lasx_xvhsubw_du_wu
+  // xd, xj, xk
+  // V4DI, UV8SI, UV8SI
+  v4i64_r = __lasx_xvhsubw_du_wu(v8u32_a, v8u32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvhsubw.du.wu(
+
+  // __lasx_xvmod_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvmod_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmod.b(
+
+  // __lasx_xvmod_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvmod_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmod.h(
+
+  // __lasx_xvmod_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvmod_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmod.w(
+
+  // __lasx_xvmod_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvmod_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmod.d(
+
+  // __lasx_xvmod_bu
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvmod_bu(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmod.bu(
+
+  // __lasx_xvmod_hu
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV16HI
+  v16u16_r = __lasx_xvmod_hu(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmod.hu(
+
+  // __lasx_xvmod_wu
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV8SI
+  v8u32_r = __lasx_xvmod_wu(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmod.wu(
+
+  // __lasx_xvmod_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvmod_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmod.du(
+
+  // __lasx_xvrepl128vei_b
+  // xd, xj, ui4
+  // V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvrepl128vei_b(v32i8_a, ui4_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvrepl128vei.b(
+
+  // __lasx_xvrepl128vei_h
+  // xd, xj, ui3
+  // V16HI, V16HI, UQI
+  v16i16_r = __lasx_xvrepl128vei_h(v16i16_a, ui3_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvrepl128vei.h(
+
+  // __lasx_xvrepl128vei_w
+  // xd, xj, ui2
+  // V8SI, V8SI, UQI
+  v8i32_r = __lasx_xvrepl128vei_w(v8i32_a, ui2_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvrepl128vei.w(
+
+  // __lasx_xvrepl128vei_d
+  // xd, xj, ui1
+  // V4DI, V4DI, UQI
+  v4i64_r = __lasx_xvrepl128vei_d(v4i64_a, ui1_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvrepl128vei.d(
+
+  // __lasx_xvpickev_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvpickev_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvpickev.b(
+
+  // __lasx_xvpickev_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvpickev_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvpickev.h(
+
+  // __lasx_xvpickev_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvpickev_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvpickev.w(
+
+  // __lasx_xvpickev_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvpickev_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvpickev.d(
+
+  // __lasx_xvpickod_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvpickod_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvpickod.b(
+
+  // __lasx_xvpickod_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvpickod_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvpickod.h(
+
+  // __lasx_xvpickod_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvpickod_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvpickod.w(
+
+  // __lasx_xvpickod_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvpickod_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvpickod.d(
+
+  // __lasx_xvilvh_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvilvh_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvilvh.b(
+
+  // __lasx_xvilvh_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvilvh_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvilvh.h(
+
+  // __lasx_xvilvh_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvilvh_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvilvh.w(
+
+  // __lasx_xvilvh_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvilvh_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvilvh.d(
+
+  // __lasx_xvilvl_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvilvl_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvilvl.b(
+
+  // __lasx_xvilvl_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvilvl_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvilvl.h(
+
+  // __lasx_xvilvl_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvilvl_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvilvl.w(
+
+  // __lasx_xvilvl_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvilvl_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvilvl.d(
+
+  // __lasx_xvpackev_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvpackev_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvpackev.b(
+
+  // __lasx_xvpackev_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvpackev_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvpackev.h(
+
+  // __lasx_xvpackev_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvpackev_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvpackev.w(
+
+  // __lasx_xvpackev_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvpackev_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvpackev.d(
+
+  // __lasx_xvpackod_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvpackod_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvpackod.b(
+
+  // __lasx_xvpackod_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvpackod_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvpackod.h(
+
+  // __lasx_xvpackod_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvpackod_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvpackod.w(
+
+  // __lasx_xvpackod_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvpackod_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvpackod.d(
+
+  // __lasx_xvshuf_b
+  // xd, xj, xk, xa
+  // V32QI, V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvshuf_b(v32i8_a, v32i8_b, v32i8_c); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvshuf.b(
+
+  // __lasx_xvshuf_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvshuf_h(v16i16_a, v16i16_b, v16i16_c); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvshuf.h(
+
+  // __lasx_xvshuf_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvshuf_w(v8i32_a, v8i32_b, v8i32_c); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvshuf.w(
+
+  // __lasx_xvshuf_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvshuf_d(v4i64_a, v4i64_b, v4i64_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvshuf.d(
+
+  // __lasx_xvand_v
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvand_v(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvand.v(
+
+  // __lasx_xvandi_b
+  // xd, xj, ui8
+  // UV32QI, UV32QI, UQI
+  v32u8_r = __lasx_xvandi_b(v32u8_a, ui8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvandi.b(
+
+  // __lasx_xvor_v
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvor_v(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvor.v(
+
+  // __lasx_xvori_b
+  // xd, xj, ui8
+  // UV32QI, UV32QI, UQI
+  v32u8_r = __lasx_xvori_b(v32u8_a, ui8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvori.b(
+
+  // __lasx_xvnor_v
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvnor_v(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvnor.v(
+
+  // __lasx_xvnori_b
+  // xd, xj, ui8
+  // UV32QI, UV32QI, UQI
+  v32u8_r = __lasx_xvnori_b(v32u8_a, ui8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvnori.b(
+
+  // __lasx_xvxor_v
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvxor_v(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvxor.v(
+
+  // __lasx_xvxori_b
+  // xd, xj, ui8
+  // UV32QI, UV32QI, UQI
+  v32u8_r = __lasx_xvxori_b(v32u8_a, ui8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvxori.b(
+
+  // __lasx_xvbitsel_v
+  // xd, xj, xk, xa
+  // UV32QI, UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvbitsel_v(v32u8_a, v32u8_b, v32u8_c); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvbitsel.v(
+
+  // __lasx_xvbitseli_b
+  // xd, xj, ui8
+  // UV32QI, UV32QI, UV32QI, UQI
+  v32u8_r = __lasx_xvbitseli_b(v32u8_a, v32u8_b, ui8); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvbitseli.b(
+
+  // __lasx_xvshuf4i_b
+  // xd, xj, ui8
+  // V32QI, V32QI, USI
+  v32i8_r = __lasx_xvshuf4i_b(v32i8_a, ui8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvshuf4i.b(
+
+  // __lasx_xvshuf4i_h
+  // xd, xj, ui8
+  // V16HI, V16HI, USI
+  v16i16_r = __lasx_xvshuf4i_h(v16i16_a, ui8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvshuf4i.h(
+
+  // __lasx_xvshuf4i_w
+  // xd, xj, ui8
+  // V8SI, V8SI, USI
+  v8i32_r = __lasx_xvshuf4i_w(v8i32_a, ui8_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvshuf4i.w(
+
+  // __lasx_xvreplgr2vr_b
+  // xd, rj
+  // V32QI, SI
+  v32i8_r = __lasx_xvreplgr2vr_b(i32_a); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvreplgr2vr.b(
+
+  // __lasx_xvreplgr2vr_h
+  // xd, rj
+  // V16HI, SI
+  v16i16_r = __lasx_xvreplgr2vr_h(i32_a); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvreplgr2vr.h(
+
+  // __lasx_xvreplgr2vr_w
+  // xd, rj
+  // V8SI, SI
+  v8i32_r = __lasx_xvreplgr2vr_w(i32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvreplgr2vr.w(
+
+  // __lasx_xvreplgr2vr_d
+  // xd, rj
+  // V4DI, DI
+  v4i64_r = __lasx_xvreplgr2vr_d(i64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvreplgr2vr.d(
+
+  // __lasx_xvpcnt_b
+  // xd, xj
+  // V32QI, V32QI
+  v32i8_r = __lasx_xvpcnt_b(v32i8_a); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvpcnt.b(
+
+  // __lasx_xvpcnt_h
+  // xd, xj
+  // V16HI, V16HI
+  v16i16_r = __lasx_xvpcnt_h(v16i16_a); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvpcnt.h(
+
+  // __lasx_xvpcnt_w
+  // xd, xj
+  // V8SI, V8SI
+  v8i32_r = __lasx_xvpcnt_w(v8i32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvpcnt.w(
+
+  // __lasx_xvpcnt_d
+  // xd, xj
+  // V4DI, V4DI
+  v4i64_r = __lasx_xvpcnt_d(v4i64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvpcnt.d(
+
+  // __lasx_xvclo_b
+  // xd, xj
+  // V32QI, V32QI
+  v32i8_r = __lasx_xvclo_b(v32i8_a); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvclo.b(
+
+  // __lasx_xvclo_h
+  // xd, xj
+  // V16HI, V16HI
+  v16i16_r = __lasx_xvclo_h(v16i16_a); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvclo.h(
+
+  // __lasx_xvclo_w
+  // xd, xj
+  // V8SI, V8SI
+  v8i32_r = __lasx_xvclo_w(v8i32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvclo.w(
+
+  // __lasx_xvclo_d
+  // xd, xj
+  // V4DI, V4DI
+  v4i64_r = __lasx_xvclo_d(v4i64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvclo.d(
+
+  // __lasx_xvclz_b
+  // xd, xj
+  // V32QI, V32QI
+  v32i8_r = __lasx_xvclz_b(v32i8_a); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvclz.b(
+
+  // __lasx_xvclz_h
+  // xd, xj
+  // V16HI, V16HI
+  v16i16_r = __lasx_xvclz_h(v16i16_a); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvclz.h(
+
+  // __lasx_xvclz_w
+  // xd, xj
+  // V8SI, V8SI
+  v8i32_r = __lasx_xvclz_w(v8i32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvclz.w(
+
+  // __lasx_xvclz_d
+  // xd, xj
+  // V4DI, V4DI
+  v4i64_r = __lasx_xvclz_d(v4i64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvclz.d(
+
+  // __lasx_xvfcmp_caf_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_caf_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.caf.s(
+
+  // __lasx_xvfcmp_caf_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_caf_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.caf.d(
+
+  // __lasx_xvfcmp_cor_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_cor_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.cor.s(
+
+  // __lasx_xvfcmp_cor_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_cor_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.cor.d(
+
+  // __lasx_xvfcmp_cun_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_cun_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.cun.s(
+
+  // __lasx_xvfcmp_cun_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_cun_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.cun.d(
+
+  // __lasx_xvfcmp_cune_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_cune_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.cune.s(
+
+  // __lasx_xvfcmp_cune_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_cune_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.cune.d(
+
+  // __lasx_xvfcmp_cueq_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_cueq_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.cueq.s(
+
+  // __lasx_xvfcmp_cueq_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_cueq_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.cueq.d(
+
+  // __lasx_xvfcmp_ceq_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_ceq_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.ceq.s(
+
+  // __lasx_xvfcmp_ceq_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_ceq_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.ceq.d(
+
+  // __lasx_xvfcmp_cne_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_cne_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.cne.s(
+
+  // __lasx_xvfcmp_cne_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_cne_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.cne.d(
+
+  // __lasx_xvfcmp_clt_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_clt_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.clt.s(
+
+  // __lasx_xvfcmp_clt_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_clt_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.clt.d(
+
+  // __lasx_xvfcmp_cult_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_cult_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.cult.s(
+
+  // __lasx_xvfcmp_cult_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_cult_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.cult.d(
+
+  // __lasx_xvfcmp_cle_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_cle_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.cle.s(
+
+  // __lasx_xvfcmp_cle_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_cle_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.cle.d(
+
+  // __lasx_xvfcmp_cule_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_cule_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.cule.s(
+
+  // __lasx_xvfcmp_cule_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_cule_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.cule.d(
+
+  // __lasx_xvfcmp_saf_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_saf_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.saf.s(
+
+  // __lasx_xvfcmp_saf_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_saf_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.saf.d(
+
+  // __lasx_xvfcmp_sor_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_sor_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.sor.s(
+
+  // __lasx_xvfcmp_sor_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_sor_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.sor.d(
+
+  // __lasx_xvfcmp_sun_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_sun_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.sun.s(
+
+  // __lasx_xvfcmp_sun_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_sun_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.sun.d(
+
+  // __lasx_xvfcmp_sune_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_sune_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.sune.s(
+
+  // __lasx_xvfcmp_sune_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_sune_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.sune.d(
+
+  // __lasx_xvfcmp_sueq_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_sueq_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.sueq.s(
+
+  // __lasx_xvfcmp_sueq_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_sueq_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.sueq.d(
+
+  // __lasx_xvfcmp_seq_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_seq_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.seq.s(
+
+  // __lasx_xvfcmp_seq_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_seq_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.seq.d(
+
+  // __lasx_xvfcmp_sne_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_sne_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.sne.s(
+
+  // __lasx_xvfcmp_sne_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_sne_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.sne.d(
+
+  // __lasx_xvfcmp_slt_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_slt_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.slt.s(
+
+  // __lasx_xvfcmp_slt_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_slt_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.slt.d(
+
+  // __lasx_xvfcmp_sult_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_sult_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.sult.s(
+
+  // __lasx_xvfcmp_sult_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_sult_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.sult.d(
+
+  // __lasx_xvfcmp_sle_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_sle_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.sle.s(
+
+  // __lasx_xvfcmp_sle_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_sle_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.sle.d(
+
+  // __lasx_xvfcmp_sule_s
+  // xd, xj, xk
+  // V8SI, V8SF, V8SF
+  v8i32_r = __lasx_xvfcmp_sule_s(v8f32_a, v8f32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfcmp.sule.s(
+
+  // __lasx_xvfcmp_sule_d
+  // xd, xj, xk
+  // V4DI, V4DF, V4DF
+  v4i64_r = __lasx_xvfcmp_sule_d(v4f64_a, v4f64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfcmp.sule.d(
+
+  // __lasx_xvfadd_s
+  // xd, xj, xk
+  // V8SF, V8SF, V8SF
+  v8f32_r = __lasx_xvfadd_s(v8f32_a, v8f32_b); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfadd.s(
+
+  // __lasx_xvfadd_d
+  // xd, xj, xk
+  // V4DF, V4DF, V4DF
+  v4f64_r = __lasx_xvfadd_d(v4f64_a, v4f64_b); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfadd.d(
+
+  // __lasx_xvfsub_s
+  // xd, xj, xk
+  // V8SF, V8SF, V8SF
+  v8f32_r = __lasx_xvfsub_s(v8f32_a, v8f32_b); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfsub.s(
+
+  // __lasx_xvfsub_d
+  // xd, xj, xk
+  // V4DF, V4DF, V4DF
+  v4f64_r = __lasx_xvfsub_d(v4f64_a, v4f64_b); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfsub.d(
+
+  // __lasx_xvfmul_s
+  // xd, xj, xk
+  // V8SF, V8SF, V8SF
+  v8f32_r = __lasx_xvfmul_s(v8f32_a, v8f32_b); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfmul.s(
+
+  // __lasx_xvfmul_d
+  // xd, xj, xk
+  // V4DF, V4DF, V4DF
+  v4f64_r = __lasx_xvfmul_d(v4f64_a, v4f64_b); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfmul.d(
+
+  // __lasx_xvfdiv_s
+  // xd, xj, xk
+  // V8SF, V8SF, V8SF
+  v8f32_r = __lasx_xvfdiv_s(v8f32_a, v8f32_b); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfdiv.s(
+
+  // __lasx_xvfdiv_d
+  // xd, xj, xk
+  // V4DF, V4DF, V4DF
+  v4f64_r = __lasx_xvfdiv_d(v4f64_a, v4f64_b); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfdiv.d(
+
+  // __lasx_xvfcvt_h_s
+  // xd, xj, xk
+  // V16HI, V8SF, V8SF
+  v16i16_r = __lasx_xvfcvt_h_s(v8f32_a, v8f32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvfcvt.h.s(
+
+  // __lasx_xvfcvt_s_d
+  // xd, xj, xk
+  // V8SF, V4DF, V4DF
+  v8f32_r = __lasx_xvfcvt_s_d(v4f64_a, v4f64_b); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfcvt.s.d(
+
+  // __lasx_xvfmin_s
+  // xd, xj, xk
+  // V8SF, V8SF, V8SF
+  v8f32_r = __lasx_xvfmin_s(v8f32_a, v8f32_b); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfmin.s(
+
+  // __lasx_xvfmin_d
+  // xd, xj, xk
+  // V4DF, V4DF, V4DF
+  v4f64_r = __lasx_xvfmin_d(v4f64_a, v4f64_b); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfmin.d(
+
+  // __lasx_xvfmina_s
+  // xd, xj, xk
+  // V8SF, V8SF, V8SF
+  v8f32_r = __lasx_xvfmina_s(v8f32_a, v8f32_b); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfmina.s(
+
+  // __lasx_xvfmina_d
+  // xd, xj, xk
+  // V4DF, V4DF, V4DF
+  v4f64_r = __lasx_xvfmina_d(v4f64_a, v4f64_b); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfmina.d(
+
+  // __lasx_xvfmax_s
+  // xd, xj, xk
+  // V8SF, V8SF, V8SF
+  v8f32_r = __lasx_xvfmax_s(v8f32_a, v8f32_b); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfmax.s(
+
+  // __lasx_xvfmax_d
+  // xd, xj, xk
+  // V4DF, V4DF, V4DF
+  v4f64_r = __lasx_xvfmax_d(v4f64_a, v4f64_b); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfmax.d(
+
+  // __lasx_xvfmaxa_s
+  // xd, xj, xk
+  // V8SF, V8SF, V8SF
+  v8f32_r = __lasx_xvfmaxa_s(v8f32_a, v8f32_b); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfmaxa.s(
+
+  // __lasx_xvfmaxa_d
+  // xd, xj, xk
+  // V4DF, V4DF, V4DF
+  v4f64_r = __lasx_xvfmaxa_d(v4f64_a, v4f64_b); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfmaxa.d(
+
+  // __lasx_xvfclass_s
+  // xd, xj
+  // V8SI, V8SF
+  v8i32_r = __lasx_xvfclass_s(v8f32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfclass.s(
+
+  // __lasx_xvfclass_d
+  // xd, xj
+  // V4DI, V4DF
+  v4i64_r = __lasx_xvfclass_d(v4f64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfclass.d(
+
+  // __lasx_xvfsqrt_s
+  // xd, xj
+  // V8SF, V8SF
+  v8f32_r = __lasx_xvfsqrt_s(v8f32_a); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfsqrt.s(
+
+  // __lasx_xvfsqrt_d
+  // xd, xj
+  // V4DF, V4DF
+  v4f64_r = __lasx_xvfsqrt_d(v4f64_a); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfsqrt.d(
+
+  // __lasx_xvfrecip_s
+  // xd, xj
+  // V8SF, V8SF
+  v8f32_r = __lasx_xvfrecip_s(v8f32_a); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfrecip.s(
+
+  // __lasx_xvfrecip_d
+  // xd, xj
+  // V4DF, V4DF
+  v4f64_r = __lasx_xvfrecip_d(v4f64_a); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfrecip.d(
+
+  // __lasx_xvfrint_s
+  // xd, xj
+  // V8SF, V8SF
+  v8f32_r = __lasx_xvfrint_s(v8f32_a); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfrint.s(
+
+  // __lasx_xvfrint_d
+  // xd, xj
+  // V4DF, V4DF
+  v4f64_r = __lasx_xvfrint_d(v4f64_a); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfrint.d(
+
+  // __lasx_xvfrsqrt_s
+  // xd, xj
+  // V8SF, V8SF
+  v8f32_r = __lasx_xvfrsqrt_s(v8f32_a); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfrsqrt.s(
+
+  // __lasx_xvfrsqrt_d
+  // xd, xj
+  // V4DF, V4DF
+  v4f64_r = __lasx_xvfrsqrt_d(v4f64_a); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfrsqrt.d(
+
+  // __lasx_xvflogb_s
+  // xd, xj
+  // V8SF, V8SF
+  v8f32_r = __lasx_xvflogb_s(v8f32_a); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvflogb.s(
+
+  // __lasx_xvflogb_d
+  // xd, xj
+  // V4DF, V4DF
+  v4f64_r = __lasx_xvflogb_d(v4f64_a); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvflogb.d(
+
+  // __lasx_xvfcvth_s_h
+  // xd, xj
+  // V8SF, V16HI
+  v8f32_r = __lasx_xvfcvth_s_h(v16i16_a); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfcvth.s.h(
+
+  // __lasx_xvfcvth_d_s
+  // xd, xj
+  // V4DF, V8SF
+  v4f64_r = __lasx_xvfcvth_d_s(v8f32_a); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfcvth.d.s(
+
+  // __lasx_xvfcvtl_s_h
+  // xd, xj
+  // V8SF, V16HI
+  v8f32_r = __lasx_xvfcvtl_s_h(v16i16_a); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfcvtl.s.h(
+
+  // __lasx_xvfcvtl_d_s
+  // xd, xj
+  // V4DF, V8SF
+  v4f64_r = __lasx_xvfcvtl_d_s(v8f32_a); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfcvtl.d.s(
+
+  // __lasx_xvftint_w_s
+  // xd, xj
+  // V8SI, V8SF
+  v8i32_r = __lasx_xvftint_w_s(v8f32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvftint.w.s(
+
+  // __lasx_xvftint_l_d
+  // xd, xj
+  // V4DI, V4DF
+  v4i64_r = __lasx_xvftint_l_d(v4f64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftint.l.d(
+
+  // __lasx_xvftint_wu_s
+  // xd, xj
+  // UV8SI, V8SF
+  v8u32_r = __lasx_xvftint_wu_s(v8f32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvftint.wu.s(
+
+  // __lasx_xvftint_lu_d
+  // xd, xj
+  // UV4DI, V4DF
+  v4u64_r = __lasx_xvftint_lu_d(v4f64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftint.lu.d(
+
+  // __lasx_xvftintrz_w_s
+  // xd, xj
+  // V8SI, V8SF
+  v8i32_r = __lasx_xvftintrz_w_s(v8f32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvftintrz.w.s(
+
+  // __lasx_xvftintrz_l_d
+  // xd, xj
+  // V4DI, V4DF
+  v4i64_r = __lasx_xvftintrz_l_d(v4f64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintrz.l.d(
+
+  // __lasx_xvftintrz_wu_s
+  // xd, xj
+  // UV8SI, V8SF
+  v8u32_r = __lasx_xvftintrz_wu_s(v8f32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvftintrz.wu.s(
+
+  // __lasx_xvftintrz_lu_d
+  // xd, xj
+  // UV4DI, V4DF
+  v4u64_r = __lasx_xvftintrz_lu_d(v4f64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintrz.lu.d(
+
+  // __lasx_xvffint_s_w
+  // xd, xj
+  // V8SF, V8SI
+  v8f32_r = __lasx_xvffint_s_w(v8i32_a); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvffint.s.w(
+
+  // __lasx_xvffint_d_l
+  // xd, xj
+  // V4DF, V4DI
+  v4f64_r = __lasx_xvffint_d_l(v4i64_a); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvffint.d.l(
+
+  // __lasx_xvffint_s_wu
+  // xd, xj
+  // V8SF, UV8SI
+  v8f32_r = __lasx_xvffint_s_wu(v8u32_a); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvffint.s.wu(
+
+  // __lasx_xvffint_d_lu
+  // xd, xj
+  // V4DF, UV4DI
+  v4f64_r = __lasx_xvffint_d_lu(v4u64_a); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvffint.d.lu(
+
+  // __lasx_xvreplve_b
+  // xd, xj, rk
+  // V32QI, V32QI, SI
+  v32i8_r = __lasx_xvreplve_b(v32i8_a, i32_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvreplve.b(
+
+  // __lasx_xvreplve_h
+  // xd, xj, rk
+  // V16HI, V16HI, SI
+  v16i16_r = __lasx_xvreplve_h(v16i16_a, i32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvreplve.h(
+
+  // __lasx_xvreplve_w
+  // xd, xj, rk
+  // V8SI, V8SI, SI
+  v8i32_r = __lasx_xvreplve_w(v8i32_a, i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvreplve.w(
+
+  // __lasx_xvreplve_d
+  // xd, xj, rk
+  // V4DI, V4DI, SI
+  v4i64_r = __lasx_xvreplve_d(v4i64_a, i32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvreplve.d(
+
+  // __lasx_xvpermi_w
+  // xd, xj, ui8
+  // V8SI, V8SI, V8SI, USI
+  v8i32_r = __lasx_xvpermi_w(v8i32_a, v8i32_b, ui8); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvpermi.w(
+
+  // __lasx_xvandn_v
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvandn_v(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvandn.v(
+
+  // __lasx_xvneg_b
+  // xd, xj
+  // V32QI, V32QI
+  v32i8_r = __lasx_xvneg_b(v32i8_a); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvneg.b(
+
+  // __lasx_xvneg_h
+  // xd, xj
+  // V16HI, V16HI
+  v16i16_r = __lasx_xvneg_h(v16i16_a); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvneg.h(
+
+  // __lasx_xvneg_w
+  // xd, xj
+  // V8SI, V8SI
+  v8i32_r = __lasx_xvneg_w(v8i32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvneg.w(
+
+  // __lasx_xvneg_d
+  // xd, xj
+  // V4DI, V4DI
+  v4i64_r = __lasx_xvneg_d(v4i64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvneg.d(
+
+  // __lasx_xvmuh_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvmuh_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmuh.b(
+
+  // __lasx_xvmuh_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvmuh_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmuh.h(
+
+  // __lasx_xvmuh_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvmuh_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmuh.w(
+
+  // __lasx_xvmuh_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvmuh_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmuh.d(
+
+  // __lasx_xvmuh_bu
+  // xd, xj, xk
+  // UV32QI, UV32QI, UV32QI
+  v32u8_r = __lasx_xvmuh_bu(v32u8_a, v32u8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmuh.bu(
+
+  // __lasx_xvmuh_hu
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV16HI
+  v16u16_r = __lasx_xvmuh_hu(v16u16_a, v16u16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmuh.hu(
+
+  // __lasx_xvmuh_wu
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV8SI
+  v8u32_r = __lasx_xvmuh_wu(v8u32_a, v8u32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmuh.wu(
+
+  // __lasx_xvmuh_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvmuh_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmuh.du(
+
+  // __lasx_xvsllwil_h_b
+  // xd, xj, ui3
+  // V16HI, V32QI, UQI
+  v16i16_r = __lasx_xvsllwil_h_b(v32i8_a, ui3_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsllwil.h.b(
+
+  // __lasx_xvsllwil_w_h
+  // xd, xj, ui4
+  // V8SI, V16HI, UQI
+  v8i32_r = __lasx_xvsllwil_w_h(v16i16_a, ui4_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsllwil.w.h(
+
+  // __lasx_xvsllwil_d_w
+  // xd, xj, ui5
+  // V4DI, V8SI, UQI
+  v4i64_r = __lasx_xvsllwil_d_w(v8i32_a, ui5_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsllwil.d.w(
+
+  // __lasx_xvsllwil_hu_bu
+  // xd, xj, ui3
+  // UV16HI, UV32QI, UQI
+  v16u16_r = __lasx_xvsllwil_hu_bu(v32u8_a, ui3_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsllwil.hu.bu(
+
+  // __lasx_xvsllwil_wu_hu
+  // xd, xj, ui4
+  // UV8SI, UV16HI, UQI
+  v8u32_r = __lasx_xvsllwil_wu_hu(v16u16_a, ui4_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsllwil.wu.hu(
+
+  // __lasx_xvsllwil_du_wu
+  // xd, xj, ui5
+  // UV4DI, UV8SI, UQI
+  v4u64_r = __lasx_xvsllwil_du_wu(v8u32_a, ui5_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsllwil.du.wu(
+
+  // __lasx_xvsran_b_h
+  // xd, xj, xk
+  // V32QI, V16HI, V16HI
+  v32i8_r = __lasx_xvsran_b_h(v16i16_a, v16i16_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsran.b.h(
+
+  // __lasx_xvsran_h_w
+  // xd, xj, xk
+  // V16HI, V8SI, V8SI
+  v16i16_r = __lasx_xvsran_h_w(v8i32_a, v8i32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsran.h.w(
+
+  // __lasx_xvsran_w_d
+  // xd, xj, xk
+  // V8SI, V4DI, V4DI
+  v8i32_r = __lasx_xvsran_w_d(v4i64_a, v4i64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsran.w.d(
+
+  // __lasx_xvssran_b_h
+  // xd, xj, xk
+  // V32QI, V16HI, V16HI
+  v32i8_r = __lasx_xvssran_b_h(v16i16_a, v16i16_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssran.b.h(
+
+  // __lasx_xvssran_h_w
+  // xd, xj, xk
+  // V16HI, V8SI, V8SI
+  v16i16_r = __lasx_xvssran_h_w(v8i32_a, v8i32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssran.h.w(
+
+  // __lasx_xvssran_w_d
+  // xd, xj, xk
+  // V8SI, V4DI, V4DI
+  v8i32_r = __lasx_xvssran_w_d(v4i64_a, v4i64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssran.w.d(
+
+  // __lasx_xvssran_bu_h
+  // xd, xj, xk
+  // UV32QI, UV16HI, UV16HI
+  v32u8_r = __lasx_xvssran_bu_h(v16u16_a, v16u16_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssran.bu.h(
+
+  // __lasx_xvssran_hu_w
+  // xd, xj, xk
+  // UV16HI, UV8SI, UV8SI
+  v16u16_r = __lasx_xvssran_hu_w(v8u32_a, v8u32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssran.hu.w(
+
+  // __lasx_xvssran_wu_d
+  // xd, xj, xk
+  // UV8SI, UV4DI, UV4DI
+  v8u32_r = __lasx_xvssran_wu_d(v4u64_a, v4u64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssran.wu.d(
+
+  // __lasx_xvsrarn_b_h
+  // xd, xj, xk
+  // V32QI, V16HI, V16HI
+  v32i8_r = __lasx_xvsrarn_b_h(v16i16_a, v16i16_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrarn.b.h(
+
+  // __lasx_xvsrarn_h_w
+  // xd, xj, xk
+  // V16HI, V8SI, V8SI
+  v16i16_r = __lasx_xvsrarn_h_w(v8i32_a, v8i32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrarn.h.w(
+
+  // __lasx_xvsrarn_w_d
+  // xd, xj, xk
+  // V8SI, V4DI, V4DI
+  v8i32_r = __lasx_xvsrarn_w_d(v4i64_a, v4i64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrarn.w.d(
+
+  // __lasx_xvssrarn_b_h
+  // xd, xj, xk
+  // V32QI, V16HI, V16HI
+  v32i8_r = __lasx_xvssrarn_b_h(v16i16_a, v16i16_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrarn.b.h(
+
+  // __lasx_xvssrarn_h_w
+  // xd, xj, xk
+  // V16HI, V8SI, V8SI
+  v16i16_r = __lasx_xvssrarn_h_w(v8i32_a, v8i32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrarn.h.w(
+
+  // __lasx_xvssrarn_w_d
+  // xd, xj, xk
+  // V8SI, V4DI, V4DI
+  v8i32_r = __lasx_xvssrarn_w_d(v4i64_a, v4i64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrarn.w.d(
+
+  // __lasx_xvssrarn_bu_h
+  // xd, xj, xk
+  // UV32QI, UV16HI, UV16HI
+  v32u8_r = __lasx_xvssrarn_bu_h(v16u16_a, v16u16_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrarn.bu.h(
+
+  // __lasx_xvssrarn_hu_w
+  // xd, xj, xk
+  // UV16HI, UV8SI, UV8SI
+  v16u16_r = __lasx_xvssrarn_hu_w(v8u32_a, v8u32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrarn.hu.w(
+
+  // __lasx_xvssrarn_wu_d
+  // xd, xj, xk
+  // UV8SI, UV4DI, UV4DI
+  v8u32_r = __lasx_xvssrarn_wu_d(v4u64_a, v4u64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrarn.wu.d(
+
+  // __lasx_xvsrln_b_h
+  // xd, xj, xk
+  // V32QI, V16HI, V16HI
+  v32i8_r = __lasx_xvsrln_b_h(v16i16_a, v16i16_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrln.b.h(
+
+  // __lasx_xvsrln_h_w
+  // xd, xj, xk
+  // V16HI, V8SI, V8SI
+  v16i16_r = __lasx_xvsrln_h_w(v8i32_a, v8i32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrln.h.w(
+
+  // __lasx_xvsrln_w_d
+  // xd, xj, xk
+  // V8SI, V4DI, V4DI
+  v8i32_r = __lasx_xvsrln_w_d(v4i64_a, v4i64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrln.w.d(
+
+  // __lasx_xvssrln_bu_h
+  // xd, xj, xk
+  // UV32QI, UV16HI, UV16HI
+  v32u8_r = __lasx_xvssrln_bu_h(v16u16_a, v16u16_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrln.bu.h(
+
+  // __lasx_xvssrln_hu_w
+  // xd, xj, xk
+  // UV16HI, UV8SI, UV8SI
+  v16u16_r = __lasx_xvssrln_hu_w(v8u32_a, v8u32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrln.hu.w(
+
+  // __lasx_xvssrln_wu_d
+  // xd, xj, xk
+  // UV8SI, UV4DI, UV4DI
+  v8u32_r = __lasx_xvssrln_wu_d(v4u64_a, v4u64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrln.wu.d(
+
+  // __lasx_xvsrlrn_b_h
+  // xd, xj, xk
+  // V32QI, V16HI, V16HI
+  v32i8_r = __lasx_xvsrlrn_b_h(v16i16_a, v16i16_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrlrn.b.h(
+
+  // __lasx_xvsrlrn_h_w
+  // xd, xj, xk
+  // V16HI, V8SI, V8SI
+  v16i16_r = __lasx_xvsrlrn_h_w(v8i32_a, v8i32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrlrn.h.w(
+
+  // __lasx_xvsrlrn_w_d
+  // xd, xj, xk
+  // V8SI, V4DI, V4DI
+  v8i32_r = __lasx_xvsrlrn_w_d(v4i64_a, v4i64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrlrn.w.d(
+
+  // __lasx_xvssrlrn_bu_h
+  // xd, xj, xk
+  // UV32QI, UV16HI, UV16HI
+  v32u8_r = __lasx_xvssrlrn_bu_h(v16u16_a, v16u16_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrlrn.bu.h(
+
+  // __lasx_xvssrlrn_hu_w
+  // xd, xj, xk
+  // UV16HI, UV8SI, UV8SI
+  v16u16_r = __lasx_xvssrlrn_hu_w(v8u32_a, v8u32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrlrn.hu.w(
+
+  // __lasx_xvssrlrn_wu_d
+  // xd, xj, xk
+  // UV8SI, UV4DI, UV4DI
+  v8u32_r = __lasx_xvssrlrn_wu_d(v4u64_a, v4u64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrlrn.wu.d(
+
+  // __lasx_xvfrstpi_b
+  // xd, xj, ui5
+  // V32QI, V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvfrstpi_b(v32i8_a, v32i8_b, ui5); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvfrstpi.b(
+
+  // __lasx_xvfrstpi_h
+  // xd, xj, ui5
+  // V16HI, V16HI, V16HI, UQI
+  v16i16_r = __lasx_xvfrstpi_h(v16i16_a, v16i16_b, ui5); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvfrstpi.h(
+
+  // __lasx_xvfrstp_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvfrstp_b(v32i8_a, v32i8_b, v32i8_c); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvfrstp.b(
+
+  // __lasx_xvfrstp_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvfrstp_h(v16i16_a, v16i16_b, v16i16_c); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvfrstp.h(
+
+  // __lasx_xvshuf4i_d
+  // xd, xj, ui8
+  // V4DI, V4DI, V4DI, USI
+  v4i64_r = __lasx_xvshuf4i_d(v4i64_a, v4i64_b, ui8); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvshuf4i.d(
+
+  // __lasx_xvbsrl_v
+  // xd, xj, ui5
+  // V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvbsrl_v(v32i8_a, ui5_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvbsrl.v(
+
+  // __lasx_xvbsll_v
+  // xd, xj, ui5
+  // V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvbsll_v(v32i8_a, ui5_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvbsll.v(
+
+  // __lasx_xvextrins_b
+  // xd, xj, ui8
+  // V32QI, V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvextrins_b(v32i8_a, v32i8_b, ui8); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvextrins.b(
+
+  // __lasx_xvextrins_h
+  // xd, xj, ui8
+  // V16HI, V16HI, V16HI, UQI
+  v16i16_r = __lasx_xvextrins_h(v16i16_a, v16i16_b, ui8); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvextrins.h(
+
+  // __lasx_xvextrins_w
+  // xd, xj, ui8
+  // V8SI, V8SI, V8SI, UQI
+  v8i32_r = __lasx_xvextrins_w(v8i32_a, v8i32_b, ui8); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvextrins.w(
+
+  // __lasx_xvextrins_d
+  // xd, xj, ui8
+  // V4DI, V4DI, V4DI, UQI
+  v4i64_r = __lasx_xvextrins_d(v4i64_a, v4i64_b, ui8); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvextrins.d(
+
+  // __lasx_xvmskltz_b
+  // xd, xj
+  // V32QI, V32QI
+  v32i8_r = __lasx_xvmskltz_b(v32i8_a); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmskltz.b(
+
+  // __lasx_xvmskltz_h
+  // xd, xj
+  // V16HI, V16HI
+  v16i16_r = __lasx_xvmskltz_h(v16i16_a); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmskltz.h(
+
+  // __lasx_xvmskltz_w
+  // xd, xj
+  // V8SI, V8SI
+  v8i32_r = __lasx_xvmskltz_w(v8i32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmskltz.w(
+
+  // __lasx_xvmskltz_d
+  // xd, xj
+  // V4DI, V4DI
+  v4i64_r = __lasx_xvmskltz_d(v4i64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmskltz.d(
+
+  // __lasx_xvsigncov_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvsigncov_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsigncov.b(
+
+  // __lasx_xvsigncov_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvsigncov_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsigncov.h(
+
+  // __lasx_xvsigncov_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvsigncov_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsigncov.w(
+
+  // __lasx_xvsigncov_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvsigncov_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsigncov.d(
+
+  // __lasx_xvfmadd_s
+  // xd, xj, xk, xa
+  // V8SF, V8SF, V8SF, V8SF
+  v8f32_r = __lasx_xvfmadd_s(v8f32_a, v8f32_b, v8f32_c); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfmadd.s(
+
+  // __lasx_xvfmadd_d
+  // xd, xj, xk, xa
+  // V4DF, V4DF, V4DF, V4DF
+  v4f64_r = __lasx_xvfmadd_d(v4f64_a, v4f64_b, v4f64_c); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfmadd.d(
+
+  // __lasx_xvfmsub_s
+  // xd, xj, xk, xa
+  // V8SF, V8SF, V8SF, V8SF
+  v8f32_r = __lasx_xvfmsub_s(v8f32_a, v8f32_b, v8f32_c); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfmsub.s(
+
+  // __lasx_xvfmsub_d
+  // xd, xj, xk, xa
+  // V4DF, V4DF, V4DF, V4DF
+  v4f64_r = __lasx_xvfmsub_d(v4f64_a, v4f64_b, v4f64_c); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfmsub.d(
+
+  // __lasx_xvfnmadd_s
+  // xd, xj, xk, xa
+  // V8SF, V8SF, V8SF, V8SF
+  v8f32_r = __lasx_xvfnmadd_s(v8f32_a, v8f32_b, v8f32_c); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfnmadd.s(
+
+  // __lasx_xvfnmadd_d
+  // xd, xj, xk, xa
+  // V4DF, V4DF, V4DF, V4DF
+  v4f64_r = __lasx_xvfnmadd_d(v4f64_a, v4f64_b, v4f64_c); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfnmadd.d(
+
+  // __lasx_xvfnmsub_s
+  // xd, xj, xk, xa
+  // V8SF, V8SF, V8SF, V8SF
+  v8f32_r = __lasx_xvfnmsub_s(v8f32_a, v8f32_b, v8f32_c); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvfnmsub.s(
+
+  // __lasx_xvfnmsub_d
+  // xd, xj, xk, xa
+  // V4DF, V4DF, V4DF, V4DF
+  v4f64_r = __lasx_xvfnmsub_d(v4f64_a, v4f64_b, v4f64_c); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvfnmsub.d(
+
+  // __lasx_xvftintrne_w_s
+  // xd, xj
+  // V8SI, V8SF
+  v8i32_r = __lasx_xvftintrne_w_s(v8f32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvftintrne.w.s(
+
+  // __lasx_xvftintrne_l_d
+  // xd, xj
+  // V4DI, V4DF
+  v4i64_r = __lasx_xvftintrne_l_d(v4f64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintrne.l.d(
+
+  // __lasx_xvftintrp_w_s
+  // xd, xj
+  // V8SI, V8SF
+  v8i32_r = __lasx_xvftintrp_w_s(v8f32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvftintrp.w.s(
+
+  // __lasx_xvftintrp_l_d
+  // xd, xj
+  // V4DI, V4DF
+  v4i64_r = __lasx_xvftintrp_l_d(v4f64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintrp.l.d(
+
+  // __lasx_xvftintrm_w_s
+  // xd, xj
+  // V8SI, V8SF
+  v8i32_r = __lasx_xvftintrm_w_s(v8f32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvftintrm.w.s(
+
+  // __lasx_xvftintrm_l_d
+  // xd, xj
+  // V4DI, V4DF
+  v4i64_r = __lasx_xvftintrm_l_d(v4f64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintrm.l.d(
+
+  // __lasx_xvftint_w_d
+  // xd, xj, xk
+  // V8SI, V4DF, V4DF
+  v8i32_r = __lasx_xvftint_w_d(v4f64_a, v4f64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvftint.w.d(
+
+  // __lasx_xvffint_s_l
+  // xd, xj, xk
+  // V8SF, V4DI, V4DI
+  v8f32_r = __lasx_xvffint_s_l(v4i64_a, v4i64_b); // CHECK: call <8 x float> @llvm.loongarch.lasx.xvffint.s.l(
+
+  // __lasx_xvftintrz_w_d
+  // xd, xj, xk
+  // V8SI, V4DF, V4DF
+  v8i32_r = __lasx_xvftintrz_w_d(v4f64_a, v4f64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvftintrz.w.d(
+
+  // __lasx_xvftintrp_w_d
+  // xd, xj, xk
+  // V8SI, V4DF, V4DF
+  v8i32_r = __lasx_xvftintrp_w_d(v4f64_a, v4f64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvftintrp.w.d(
+
+  // __lasx_xvftintrm_w_d
+  // xd, xj, xk
+  // V8SI, V4DF, V4DF
+  v8i32_r = __lasx_xvftintrm_w_d(v4f64_a, v4f64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvftintrm.w.d(
+
+  // __lasx_xvftintrne_w_d
+  // xd, xj, xk
+  // V8SI, V4DF, V4DF
+  v8i32_r = __lasx_xvftintrne_w_d(v4f64_a, v4f64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvftintrne.w.d(
+
+  // __lasx_xvftinth_l_s
+  // xd, xj
+  // V4DI, V8SF
+  v4i64_r = __lasx_xvftinth_l_s(v8f32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftinth.l.s(
+
+  // __lasx_xvftintl_l_s
+  // xd, xj
+  // V4DI, V8SF
+  v4i64_r = __lasx_xvftintl_l_s(v8f32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintl.l.s(
+
+  // __lasx_xvffinth_d_w
+  // xd, xj
+  // V4DF, V8SI
+  v4f64_r = __lasx_xvffinth_d_w(v8i32_a); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvffinth.d.w(
+
+  // __lasx_xvffintl_d_w
+  // xd, xj
+  // V4DF, V8SI
+  v4f64_r = __lasx_xvffintl_d_w(v8i32_a); // CHECK: call <4 x double> @llvm.loongarch.lasx.xvffintl.d.w(
+
+  // __lasx_xvftintrzh_l_s
+  // xd, xj
+  // V4DI, V8SF
+  v4i64_r = __lasx_xvftintrzh_l_s(v8f32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintrzh.l.s(
+
+  // __lasx_xvftintrzl_l_s
+  // xd, xj
+  // V4DI, V8SF
+  v4i64_r = __lasx_xvftintrzl_l_s(v8f32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintrzl.l.s(
+
+  // __lasx_xvftintrph_l_s
+  // xd, xj
+  // V4DI, V8SF
+  v4i64_r = __lasx_xvftintrph_l_s(v8f32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintrph.l.s(
+
+  // __lasx_xvftintrpl_l_s
+  // xd, xj
+  // V4DI, V8SF
+  v4i64_r = __lasx_xvftintrpl_l_s(v8f32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintrpl.l.s(
+
+  // __lasx_xvftintrmh_l_s
+  // xd, xj
+  // V4DI, V8SF
+  v4i64_r = __lasx_xvftintrmh_l_s(v8f32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintrmh.l.s(
+
+  // __lasx_xvftintrml_l_s
+  // xd, xj
+  // V4DI, V8SF
+  v4i64_r = __lasx_xvftintrml_l_s(v8f32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintrml.l.s(
+
+  // __lasx_xvftintrneh_l_s
+  // xd, xj
+  // V4DI, V8SF
+  v4i64_r = __lasx_xvftintrneh_l_s(v8f32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintrneh.l.s(
+
+  // __lasx_xvftintrnel_l_s
+  // xd, xj
+  // V4DI, V8SF
+  v4i64_r = __lasx_xvftintrnel_l_s(v8f32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvftintrnel.l.s(
+
+  // __lasx_xvfrintrne_s
+  // xd, xj
+  // V8SI, V8SF
+  v8i32_r = __lasx_xvfrintrne_s(v8f32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfrintrne.s(
+
+  // __lasx_xvfrintrne_d
+  // xd, xj
+  // V4DI, V4DF
+  v4i64_r = __lasx_xvfrintrne_d(v4f64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfrintrne.d(
+
+  // __lasx_xvfrintrz_s
+  // xd, xj
+  // V8SI, V8SF
+  v8i32_r = __lasx_xvfrintrz_s(v8f32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfrintrz.s(
+
+  // __lasx_xvfrintrz_d
+  // xd, xj
+  // V4DI, V4DF
+  v4i64_r = __lasx_xvfrintrz_d(v4f64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfrintrz.d(
+
+  // __lasx_xvfrintrp_s
+  // xd, xj
+  // V8SI, V8SF
+  v8i32_r = __lasx_xvfrintrp_s(v8f32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfrintrp.s(
+
+  // __lasx_xvfrintrp_d
+  // xd, xj
+  // V4DI, V4DF
+  v4i64_r = __lasx_xvfrintrp_d(v4f64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfrintrp.d(
+
+  // __lasx_xvfrintrm_s
+  // xd, xj
+  // V8SI, V8SF
+  v8i32_r = __lasx_xvfrintrm_s(v8f32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvfrintrm.s(
+
+  // __lasx_xvfrintrm_d
+  // xd, xj
+  // V4DI, V4DF
+  v4i64_r = __lasx_xvfrintrm_d(v4f64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvfrintrm.d(
+
+  // __lasx_xvld
+  // xd, rj, si12
+  // V32QI, CVPOINTER, SI
+  v32i8_r = __lasx_xvld(&v32i8_a, si12); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvld(
+
+  // __lasx_xvst
+  // xd, rj, si12
+  // VOID, V32QI, CVPOINTER, SI
+  __lasx_xvst(v32i8_a, &v32i8_b, si12); // CHECK: call void @llvm.loongarch.lasx.xvst(
+
+  // __lasx_xvstelm_b
+  // xd, rj, si8, idx
+  // VOID, V32QI, CVPOINTER, SI, UQI
+  __lasx_xvstelm_b(v32i8_a, &v32i8_b, 0, idx4); // CHECK: call void @llvm.loongarch.lasx.xvstelm.b(
+
+  // __lasx_xvstelm_h
+  // xd, rj, si8, idx
+  // VOID, V16HI, CVPOINTER, SI, UQI
+  __lasx_xvstelm_h(v16i16_a, &v16i16_b, 0, idx3); // CHECK: call void @llvm.loongarch.lasx.xvstelm.h(
+
+  // __lasx_xvstelm_w
+  // xd, rj, si8, idx
+  // VOID, V8SI, CVPOINTER, SI, UQI
+  __lasx_xvstelm_w(v8i32_a, &v8i32_b, 0, idx2); // CHECK: call void @llvm.loongarch.lasx.xvstelm.w(
+
+  // __lasx_xvstelm_d
+  // xd, rj, si8, idx
+  // VOID, V4DI, CVPOINTER, SI, UQI
+  __lasx_xvstelm_d(v4i64_a, &v4i64_b, 0, idx1); // CHECK: call void @llvm.loongarch.lasx.xvstelm.d(
+
+  // __lasx_xvinsve0_w
+  // xd, xj, ui3
+  // V8SI, V8SI, V8SI, UQI
+  v8i32_r = __lasx_xvinsve0_w(v8i32_a, v8i32_b, 2); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvinsve0.w(
+
+  // __lasx_xvinsve0_d
+  // xd, xj, ui2
+  // V4DI, V4DI, V4DI, UQI
+  v4i64_r = __lasx_xvinsve0_d(v4i64_a, v4i64_b, ui2); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvinsve0.d(
+
+  // __lasx_xvpickve_w
+  // xd, xj, ui3
+  // V8SI, V8SI, UQI
+  v8i32_r = __lasx_xvpickve_w(v8i32_b, 2); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvpickve.w(
+
+  // __lasx_xvpickve_d
+  // xd, xj, ui2
+  // V4DI, V4DI, UQI
+  v4i64_r = __lasx_xvpickve_d(v4i64_b, ui2); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvpickve.d(
+
+  // __lasx_xvssrlrn_b_h
+  // xd, xj, xk
+  // V32QI, V16HI, V16HI
+  v32i8_r = __lasx_xvssrlrn_b_h(v16i16_a, v16i16_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrlrn.b.h(
+
+  // __lasx_xvssrlrn_h_w
+  // xd, xj, xk
+  // V16HI, V8SI, V8SI
+  v16i16_r = __lasx_xvssrlrn_h_w(v8i32_a, v8i32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrlrn.h.w(
+
+  // __lasx_xvssrlrn_w_d
+  // xd, xj, xk
+  // V8SI, V4DI, V4DI
+  v8i32_r = __lasx_xvssrlrn_w_d(v4i64_a, v4i64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrlrn.w.d(
+
+  // __lasx_xvssrln_b_h
+  // xd, xj, xk
+  // V32QI, V16HI, V16HI
+  v32i8_r = __lasx_xvssrln_b_h(v16i16_a, v16i16_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrln.b.h(
+
+  // __lasx_xvssrln_h_w
+  // xd, xj, xk
+  // V16HI, V8SI, V8SI
+  v16i16_r = __lasx_xvssrln_h_w(v8i32_a, v8i32_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrln.h.w(
+
+  // __lasx_xvssrln_w_d
+  // xd, xj, xk
+  // V8SI, V4DI, V4DI
+  v8i32_r = __lasx_xvssrln_w_d(v4i64_a, v4i64_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrln.w.d(
+
+  // __lasx_xvorn_v
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvorn_v(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvorn.v(
+
+  // __lasx_xvldi
+  // xd, i13
+  // V4DI, HI
+  v4i64_r = __lasx_xvldi(i13); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvldi(
+
+  // __lasx_xvldx
+  // xd, rj, rk
+  // V32QI, CVPOINTER, DI
+  v32i8_r = __lasx_xvldx(&v32i8_a, i64_d); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvldx(
+
+  // __lasx_xvstx
+  // xd, rj, rk
+  // VOID, V32QI, CVPOINTER, DI
+  __lasx_xvstx(v32i8_a, &v32i8_b, i64_d); // CHECK: call void @llvm.loongarch.lasx.xvstx(
+
+  // __lasx_xvinsgr2vr_w
+  // xd, rj, ui3
+  // V8SI, V8SI, SI, UQI
+  v8i32_r = __lasx_xvinsgr2vr_w(v8i32_a, i32_b, ui3); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvinsgr2vr.w(
+
+  // __lasx_xvinsgr2vr_d
+  // xd, rj, ui2
+  // V4DI, V4DI, DI, UQI
+  v4i64_r = __lasx_xvinsgr2vr_d(v4i64_a, i64_b, ui2); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvinsgr2vr.d(
+
+  // __lasx_xvreplve0_b
+  // xd, xj
+  // V32QI, V32QI
+  v32i8_r = __lasx_xvreplve0_b(v32i8_a); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvreplve0.b(
+
+  // __lasx_xvreplve0_h
+  // xd, xj
+  // V16HI, V16HI
+  v16i16_r = __lasx_xvreplve0_h(v16i16_a); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvreplve0.h(
+
+  // __lasx_xvreplve0_w
+  // xd, xj
+  // V8SI, V8SI
+  v8i32_r = __lasx_xvreplve0_w(v8i32_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvreplve0.w(
+
+  // __lasx_xvreplve0_d
+  // xd, xj
+  // V4DI, V4DI
+  v4i64_r = __lasx_xvreplve0_d(v4i64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvreplve0.d(
+
+  // __lasx_xvreplve0_q
+  // xd, xj
+  // V32QI, V32QI
+  v32i8_r = __lasx_xvreplve0_q(v32i8_a); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvreplve0.q(
+
+  // __lasx_vext2xv_h_b
+  // xd, xj
+  // V16HI, V32QI
+  v16i16_r = __lasx_vext2xv_h_b(v32i8_a); // CHECK: call <16 x i16> @llvm.loongarch.lasx.vext2xv.h.b(
+
+  // __lasx_vext2xv_w_h
+  // xd, xj
+  // V8SI, V16HI
+  v8i32_r = __lasx_vext2xv_w_h(v16i16_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.vext2xv.w.h(
+
+  // __lasx_vext2xv_d_w
+  // xd, xj
+  // V4DI, V8SI
+  v4i64_r = __lasx_vext2xv_d_w(v8i32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.vext2xv.d.w(
+
+  // __lasx_vext2xv_w_b
+  // xd, xj
+  // V8SI, V32QI
+  v8i32_r = __lasx_vext2xv_w_b(v32i8_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.vext2xv.w.b(
+
+  //gcc build fail
+  // __lasx_vext2xv_d_h
+  // xd, xj
+  // V4DI, V16HI
+  v4i64_r = __lasx_vext2xv_d_h(v16i16_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.vext2xv.d.h(
+
+  // __lasx_vext2xv_d_b
+  // xd, xj
+  // V4DI, V32QI
+  v4i64_r = __lasx_vext2xv_d_b(v32i8_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.vext2xv.d.b(
+
+  // __lasx_vext2xv_hu_bu
+  // xd, xj
+  // V16HI, V32QI
+  v16i16_r = __lasx_vext2xv_hu_bu(v32i8_a); // CHECK: call <16 x i16> @llvm.loongarch.lasx.vext2xv.hu.bu(
+
+  // __lasx_vext2xv_wu_hu
+  // xd, xj
+  // V8SI, V16HI
+  v8i32_r = __lasx_vext2xv_wu_hu(v16i16_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.vext2xv.wu.hu(
+
+  // __lasx_vext2xv_du_wu
+  // xd, xj
+  // V4DI, V8SI
+  v4i64_r = __lasx_vext2xv_du_wu(v8i32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.vext2xv.du.wu(
+
+  // __lasx_vext2xv_wu_bu
+  // xd, xj
+  // V8SI, V32QI
+  v8i32_r = __lasx_vext2xv_wu_bu(v32i8_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.vext2xv.wu.bu(
+
+  //gcc build fail
+  // __lasx_vext2xv_du_hu
+  // xd, xj
+  // V4DI, V16HI
+  v4i64_r = __lasx_vext2xv_du_hu(v16i16_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.vext2xv.du.hu(
+
+  // __lasx_vext2xv_du_bu
+  // xd, xj
+  // V4DI, V32QI
+  v4i64_r = __lasx_vext2xv_du_bu(v32i8_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.vext2xv.du.bu(
+
+  // __lasx_xvpermi_q
+  // xd, xj, ui8
+  // V32QI, V32QI, V32QI, USI
+  v32i8_r = __lasx_xvpermi_q(v32i8_a, v32i8_b, ui8); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvpermi.q(
+
+  // __lasx_xvpermi_d
+  // xd, xj, ui8
+  // V4DI, V4DI, USI
+  v4i64_r = __lasx_xvpermi_d(v4i64_a, ui8); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvpermi.d(
+
+  // __lasx_xvperm_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvperm_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvperm.w(
+
+  // __lasx_xvldrepl_b
+  // xd, rj, si12
+  // V32QI, CVPOINTER, SI
+  v32i8_r = __lasx_xvldrepl_b(&v32i8_a, si12); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvldrepl.b(
+
+  // __lasx_xvldrepl_h
+  // xd, rj, si11
+  // V16HI, CVPOINTER, SI
+  v16i16_r = __lasx_xvldrepl_h(&v16i16_a, si11); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvldrepl.h(
+
+  // __lasx_xvldrepl_w
+  // xd, rj, si10
+  // V8SI, CVPOINTER, SI
+  v8i32_r = __lasx_xvldrepl_w(&v8i32_a, si10); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvldrepl.w(
+
+  // __lasx_xvldrepl_d
+  // xd, rj, si9
+  // V4DI, CVPOINTER, SI
+  v4i64_r = __lasx_xvldrepl_d(&v4i64_a, si9); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvldrepl.d(
+
+  // __lasx_xvpickve2gr_w
+  // rd, xj, ui3
+  // SI, V8SI, UQI
+  i32_r = __lasx_xvpickve2gr_w(v8i32_a, ui3_b); // CHECK: call i32 @llvm.loongarch.lasx.xvpickve2gr.w(
+
+  // __lasx_xvpickve2gr_wu
+  // rd, xj, ui3
+  // USI, V8SI, UQI
+  u32_r = __lasx_xvpickve2gr_wu(v8i32_a, ui3_b); // CHECK: call i32 @llvm.loongarch.lasx.xvpickve2gr.wu(
+
+  // __lasx_xvpickve2gr_d
+  // rd, xj, ui2
+  // DI, V4DI, UQI
+  i64_r = __lasx_xvpickve2gr_d(v4i64_a, ui2_b); // CHECK: call i64 @llvm.loongarch.lasx.xvpickve2gr.d(
+
+  // __lasx_xvpickve2gr_du
+  // rd, xj, ui2
+  // UDI, V4DI, UQI
+  u64_r = __lasx_xvpickve2gr_du(v4i64_a, ui2_b); // CHECK: call i64 @llvm.loongarch.lasx.xvpickve2gr.du(
+
+  // __lasx_xvaddwev_q_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvaddwev_q_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvaddwev.q.d(
+
+  // __lasx_xvaddwev_d_w
+  // xd, xj, xk
+  // V4DI, V8SI, V8SI
+  v4i64_r = __lasx_xvaddwev_d_w(v8i32_a, v8i32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvaddwev.d.w(
+
+  // __lasx_xvaddwev_w_h
+  // xd, xj, xk
+  // V8SI, V16HI, V16HI
+  v8i32_r = __lasx_xvaddwev_w_h(v16i16_a, v16i16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvaddwev.w.h(
+
+  // __lasx_xvaddwev_h_b
+  // xd, xj, xk
+  // V16HI, V32QI, V32QI
+  v16i16_r = __lasx_xvaddwev_h_b(v32i8_a, v32i8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvaddwev.h.b(
+
+  // __lasx_xvaddwev_q_du
+  // xd, xj, xk
+  // V4DI, UV4DI, UV4DI
+  v4i64_r = __lasx_xvaddwev_q_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvaddwev.q.du(
+
+  // __lasx_xvaddwev_d_wu
+  // xd, xj, xk
+  // V4DI, UV8SI, UV8SI
+  v4i64_r = __lasx_xvaddwev_d_wu(v8u32_a, v8u32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvaddwev.d.wu(
+
+  // __lasx_xvaddwev_w_hu
+  // xd, xj, xk
+  // V8SI, UV16HI, UV16HI
+  v8i32_r = __lasx_xvaddwev_w_hu(v16u16_a, v16u16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvaddwev.w.hu(
+
+  // __lasx_xvaddwev_h_bu
+  // xd, xj, xk
+  // V16HI, UV32QI, UV32QI
+  v16i16_r = __lasx_xvaddwev_h_bu(v32u8_a, v32u8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvaddwev.h.bu(
+
+  // __lasx_xvsubwev_q_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvsubwev_q_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsubwev.q.d(
+
+  // __lasx_xvsubwev_d_w
+  // xd, xj, xk
+  // V4DI, V8SI, V8SI
+  v4i64_r = __lasx_xvsubwev_d_w(v8i32_a, v8i32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsubwev.d.w(
+
+  // __lasx_xvsubwev_w_h
+  // xd, xj, xk
+  // V8SI, V16HI, V16HI
+  v8i32_r = __lasx_xvsubwev_w_h(v16i16_a, v16i16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsubwev.w.h(
+
+  // __lasx_xvsubwev_h_b
+  // xd, xj, xk
+  // V16HI, V32QI, V32QI
+  v16i16_r = __lasx_xvsubwev_h_b(v32i8_a, v32i8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsubwev.h.b(
+
+  // __lasx_xvsubwev_q_du
+  // xd, xj, xk
+  // V4DI, UV4DI, UV4DI
+  v4i64_r = __lasx_xvsubwev_q_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsubwev.q.du(
+
+  // __lasx_xvsubwev_d_wu
+  // xd, xj, xk
+  // V4DI, UV8SI, UV8SI
+  v4i64_r = __lasx_xvsubwev_d_wu(v8u32_a, v8u32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsubwev.d.wu(
+
+  // __lasx_xvsubwev_w_hu
+  // xd, xj, xk
+  // V8SI, UV16HI, UV16HI
+  v8i32_r = __lasx_xvsubwev_w_hu(v16u16_a, v16u16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsubwev.w.hu(
+
+  // __lasx_xvsubwev_h_bu
+  // xd, xj, xk
+  // V16HI, UV32QI, UV32QI
+  v16i16_r = __lasx_xvsubwev_h_bu(v32u8_a, v32u8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsubwev.h.bu(
+
+  // __lasx_xvmulwev_q_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvmulwev_q_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmulwev.q.d(
+
+  // __lasx_xvmulwev_d_w
+  // xd, xj, xk
+  // V4DI, V8SI, V8SI
+  v4i64_r = __lasx_xvmulwev_d_w(v8i32_a, v8i32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmulwev.d.w(
+
+  // __lasx_xvmulwev_w_h
+  // xd, xj, xk
+  // V8SI, V16HI, V16HI
+  v8i32_r = __lasx_xvmulwev_w_h(v16i16_a, v16i16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmulwev.w.h(
+
+  // __lasx_xvmulwev_h_b
+  // xd, xj, xk
+  // V16HI, V32QI, V32QI
+  v16i16_r = __lasx_xvmulwev_h_b(v32i8_a, v32i8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmulwev.h.b(
+
+  // __lasx_xvmulwev_q_du
+  // xd, xj, xk
+  // V4DI, UV4DI, UV4DI
+  v4i64_r = __lasx_xvmulwev_q_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmulwev.q.du(
+
+  // __lasx_xvmulwev_d_wu
+  // xd, xj, xk
+  // V4DI, UV8SI, UV8SI
+  v4i64_r = __lasx_xvmulwev_d_wu(v8u32_a, v8u32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmulwev.d.wu(
+
+  // __lasx_xvmulwev_w_hu
+  // xd, xj, xk
+  // V8SI, UV16HI, UV16HI
+  v8i32_r = __lasx_xvmulwev_w_hu(v16u16_a, v16u16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmulwev.w.hu(
+
+  // __lasx_xvmulwev_h_bu
+  // xd, xj, xk
+  // V16HI, UV32QI, UV32QI
+  v16i16_r = __lasx_xvmulwev_h_bu(v32u8_a, v32u8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmulwev.h.bu(
+
+  // __lasx_xvaddwod_q_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvaddwod_q_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvaddwod.q.d(
+
+  // __lasx_xvaddwod_d_w
+  // xd, xj, xk
+  // V4DI, V8SI, V8SI
+  v4i64_r = __lasx_xvaddwod_d_w(v8i32_a, v8i32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvaddwod.d.w(
+
+  // __lasx_xvaddwod_w_h
+  // xd, xj, xk
+  // V8SI, V16HI, V16HI
+  v8i32_r = __lasx_xvaddwod_w_h(v16i16_a, v16i16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvaddwod.w.h(
+
+  // __lasx_xvaddwod_h_b
+  // xd, xj, xk
+  // V16HI, V32QI, V32QI
+  v16i16_r = __lasx_xvaddwod_h_b(v32i8_a, v32i8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvaddwod.h.b(
+
+  // __lasx_xvaddwod_q_du
+  // xd, xj, xk
+  // V4DI, UV4DI, UV4DI
+  v4i64_r = __lasx_xvaddwod_q_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvaddwod.q.du(
+
+  // __lasx_xvaddwod_d_wu
+  // xd, xj, xk
+  // V4DI, UV8SI, UV8SI
+  v4i64_r = __lasx_xvaddwod_d_wu(v8u32_a, v8u32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvaddwod.d.wu(
+
+  // __lasx_xvaddwod_w_hu
+  // xd, xj, xk
+  // V8SI, UV16HI, UV16HI
+  v8i32_r = __lasx_xvaddwod_w_hu(v16u16_a, v16u16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvaddwod.w.hu(
+
+  // __lasx_xvaddwod_h_bu
+  // xd, xj, xk
+  // V16HI, UV32QI, UV32QI
+  v16i16_r = __lasx_xvaddwod_h_bu(v32u8_a, v32u8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvaddwod.h.bu(
+
+  // __lasx_xvsubwod_q_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvsubwod_q_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsubwod.q.d(
+
+  // __lasx_xvsubwod_d_w
+  // xd, xj, xk
+  // V4DI, V8SI, V8SI
+  v4i64_r = __lasx_xvsubwod_d_w(v8i32_a, v8i32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsubwod.d.w(
+
+  // __lasx_xvsubwod_w_h
+  // xd, xj, xk
+  // V8SI, V16HI, V16HI
+  v8i32_r = __lasx_xvsubwod_w_h(v16i16_a, v16i16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsubwod.w.h(
+
+  // __lasx_xvsubwod_h_b
+  // xd, xj, xk
+  // V16HI, V32QI, V32QI
+  v16i16_r = __lasx_xvsubwod_h_b(v32i8_a, v32i8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsubwod.h.b(
+
+  // __lasx_xvsubwod_q_du
+  // xd, xj, xk
+  // V4DI, UV4DI, UV4DI
+  v4i64_r = __lasx_xvsubwod_q_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsubwod.q.du(
+
+  // __lasx_xvsubwod_d_wu
+  // xd, xj, xk
+  // V4DI, UV8SI, UV8SI
+  v4i64_r = __lasx_xvsubwod_d_wu(v8u32_a, v8u32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsubwod.d.wu(
+
+  // __lasx_xvsubwod_w_hu
+  // xd, xj, xk
+  // V8SI, UV16HI, UV16HI
+  v8i32_r = __lasx_xvsubwod_w_hu(v16u16_a, v16u16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsubwod.w.hu(
+
+  // __lasx_xvsubwod_h_bu
+  // xd, xj, xk
+  // V16HI, UV32QI, UV32QI
+  v16i16_r = __lasx_xvsubwod_h_bu(v32u8_a, v32u8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsubwod.h.bu(
+
+  // __lasx_xvmulwod_q_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvmulwod_q_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmulwod.q.d(
+
+  // __lasx_xvmulwod_d_w
+  // xd, xj, xk
+  // V4DI, V8SI, V8SI
+  v4i64_r = __lasx_xvmulwod_d_w(v8i32_a, v8i32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmulwod.d.w(
+
+  // __lasx_xvmulwod_w_h
+  // xd, xj, xk
+  // V8SI, V16HI, V16HI
+  v8i32_r = __lasx_xvmulwod_w_h(v16i16_a, v16i16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmulwod.w.h(
+
+  // __lasx_xvmulwod_h_b
+  // xd, xj, xk
+  // V16HI, V32QI, V32QI
+  v16i16_r = __lasx_xvmulwod_h_b(v32i8_a, v32i8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmulwod.h.b(
+
+  // __lasx_xvmulwod_q_du
+  // xd, xj, xk
+  // V4DI, UV4DI, UV4DI
+  v4i64_r = __lasx_xvmulwod_q_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmulwod.q.du(
+
+  // __lasx_xvmulwod_d_wu
+  // xd, xj, xk
+  // V4DI, UV8SI, UV8SI
+  v4i64_r = __lasx_xvmulwod_d_wu(v8u32_a, v8u32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmulwod.d.wu(
+
+  // __lasx_xvmulwod_w_hu
+  // xd, xj, xk
+  // V8SI, UV16HI, UV16HI
+  v8i32_r = __lasx_xvmulwod_w_hu(v16u16_a, v16u16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmulwod.w.hu(
+
+  // __lasx_xvmulwod_h_bu
+  // xd, xj, xk
+  // V16HI, UV32QI, UV32QI
+  v16i16_r = __lasx_xvmulwod_h_bu(v32u8_a, v32u8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmulwod.h.bu(
+
+  // __lasx_xvaddwev_d_wu_w
+  // xd, xj, xk
+  // V4DI, UV8SI, V8SI
+  v4i64_r = __lasx_xvaddwev_d_wu_w(v8u32_a, v8i32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvaddwev.d.wu.w(
+
+  // __lasx_xvaddwev_w_hu_h
+  // xd, xj, xk
+  // V8SI, UV16HI, V16HI
+  v8i32_r = __lasx_xvaddwev_w_hu_h(v16u16_a, v16i16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvaddwev.w.hu.h(
+
+  // __lasx_xvaddwev_h_bu_b
+  // xd, xj, xk
+  // V16HI, UV32QI, V32QI
+  v16i16_r = __lasx_xvaddwev_h_bu_b(v32u8_a, v32i8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvaddwev.h.bu.b(
+
+  // __lasx_xvmulwev_d_wu_w
+  // xd, xj, xk
+  // V4DI, UV8SI, V8SI
+  v4i64_r = __lasx_xvmulwev_d_wu_w(v8u32_a, v8i32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmulwev.d.wu.w(
+
+  // __lasx_xvmulwev_w_hu_h
+  // xd, xj, xk
+  // V8SI, UV16HI, V16HI
+  v8i32_r = __lasx_xvmulwev_w_hu_h(v16u16_a, v16i16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmulwev.w.hu.h(
+
+  // __lasx_xvmulwev_h_bu_b
+  // xd, xj, xk
+  // V16HI, UV32QI, V32QI
+  v16i16_r = __lasx_xvmulwev_h_bu_b(v32u8_a, v32i8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmulwev.h.bu.b(
+
+  // __lasx_xvaddwod_d_wu_w
+  // xd, xj, xk
+  // V4DI, UV8SI, V8SI
+  v4i64_r = __lasx_xvaddwod_d_wu_w(v8u32_a, v8i32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvaddwod.d.wu.w(
+
+  // __lasx_xvaddwod_w_hu_h
+  // xd, xj, xk
+  // V8SI, UV16HI, V16HI
+  v8i32_r = __lasx_xvaddwod_w_hu_h(v16u16_a, v16i16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvaddwod.w.hu.h(
+
+  // __lasx_xvaddwod_h_bu_b
+  // xd, xj, xk
+  // V16HI, UV32QI, V32QI
+  v16i16_r = __lasx_xvaddwod_h_bu_b(v32u8_a, v32i8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvaddwod.h.bu.b(
+
+  // __lasx_xvmulwod_d_wu_w
+  // xd, xj, xk
+  // V4DI, UV8SI, V8SI
+  v4i64_r = __lasx_xvmulwod_d_wu_w(v8u32_a, v8i32_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmulwod.d.wu.w(
+
+  // __lasx_xvmulwod_w_hu_h
+  // xd, xj, xk
+  // V8SI, UV16HI, V16HI
+  v8i32_r = __lasx_xvmulwod_w_hu_h(v16u16_a, v16i16_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmulwod.w.hu.h(
+
+  // __lasx_xvmulwod_h_bu_b
+  // xd, xj, xk
+  // V16HI, UV32QI, V32QI
+  v16i16_r = __lasx_xvmulwod_h_bu_b(v32u8_a, v32i8_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmulwod.h.bu.b(
+
+  // __lasx_xvhaddw_q_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvhaddw_q_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvhaddw.q.d(
+
+  // __lasx_xvhaddw_qu_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvhaddw_qu_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvhaddw.qu.du(
+
+  // __lasx_xvhsubw_q_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvhsubw_q_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvhsubw.q.d(
+
+  // __lasx_xvhsubw_qu_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvhsubw_qu_du(v4u64_a, v4u64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvhsubw.qu.du(
+
+  // __lasx_xvmaddwev_q_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvmaddwev_q_d(v4i64_a, v4i64_b, v4i64_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaddwev.q.d(
+
+  // __lasx_xvmaddwev_d_w
+  // xd, xj, xk
+  // V4DI, V4DI, V8SI, V8SI
+  v4i64_r = __lasx_xvmaddwev_d_w(v4i64_a, v8i32_b, v8i32_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaddwev.d.w(
+
+  // __lasx_xvmaddwev_w_h
+  // xd, xj, xk
+  // V8SI, V8SI, V16HI, V16HI
+  v8i32_r = __lasx_xvmaddwev_w_h(v8i32_a, v16i16_b, v16i16_c); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmaddwev.w.h(
+
+  // __lasx_xvmaddwev_h_b
+  // xd, xj, xk
+  // V16HI, V16HI, V32QI, V32QI
+  v16i16_r = __lasx_xvmaddwev_h_b(v16i16_a, v32i8_b, v32i8_c); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmaddwev.h.b(
+
+  // __lasx_xvmaddwev_q_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvmaddwev_q_du(v4u64_a, v4u64_b, v4u64_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaddwev.q.du(
+
+  // __lasx_xvmaddwev_d_wu
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV8SI, UV8SI
+  v4u64_r = __lasx_xvmaddwev_d_wu(v4u64_a, v8u32_b, v8u32_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaddwev.d.wu(
+
+  // __lasx_xvmaddwev_w_hu
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV16HI, UV16HI
+  v8u32_r = __lasx_xvmaddwev_w_hu(v8u32_a, v16u16_b, v16u16_c); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmaddwev.w.hu(
+
+  // __lasx_xvmaddwev_h_bu
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV32QI, UV32QI
+  v16u16_r = __lasx_xvmaddwev_h_bu(v16u16_a, v32u8_b, v32u8_c); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmaddwev.h.bu(
+
+  // __lasx_xvmaddwod_q_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvmaddwod_q_d(v4i64_a, v4i64_b, v4i64_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaddwod.q.d(
+
+  // __lasx_xvmaddwod_d_w
+  // xd, xj, xk
+  // V4DI, V4DI, V8SI, V8SI
+  v4i64_r = __lasx_xvmaddwod_d_w(v4i64_a, v8i32_b, v8i32_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaddwod.d.w(
+
+  // __lasx_xvmaddwod_w_h
+  // xd, xj, xk
+  // V8SI, V8SI, V16HI, V16HI
+  v8i32_r = __lasx_xvmaddwod_w_h(v8i32_a, v16i16_b, v16i16_c); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmaddwod.w.h(
+
+  // __lasx_xvmaddwod_h_b
+  // xd, xj, xk
+  // V16HI, V16HI, V32QI, V32QI
+  v16i16_r = __lasx_xvmaddwod_h_b(v16i16_a, v32i8_b, v32i8_c); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmaddwod.h.b(
+
+  // __lasx_xvmaddwod_q_du
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV4DI, UV4DI
+  v4u64_r = __lasx_xvmaddwod_q_du(v4u64_a, v4u64_b, v4u64_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaddwod.q.du(
+
+  // __lasx_xvmaddwod_d_wu
+  // xd, xj, xk
+  // UV4DI, UV4DI, UV8SI, UV8SI
+  v4u64_r = __lasx_xvmaddwod_d_wu(v4u64_a, v8u32_b, v8u32_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaddwod.d.wu(
+
+  // __lasx_xvmaddwod_w_hu
+  // xd, xj, xk
+  // UV8SI, UV8SI, UV16HI, UV16HI
+  v8u32_r = __lasx_xvmaddwod_w_hu(v8u32_a, v16u16_b, v16u16_c); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmaddwod.w.hu(
+
+  // __lasx_xvmaddwod_h_bu
+  // xd, xj, xk
+  // UV16HI, UV16HI, UV32QI, UV32QI
+  v16u16_r = __lasx_xvmaddwod_h_bu(v16u16_a, v32u8_b, v32u8_c); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmaddwod.h.bu(
+
+  // __lasx_xvmaddwev_q_du_d
+  // xd, xj, xk
+  // V4DI, V4DI, UV4DI, V4DI
+  v4i64_r = __lasx_xvmaddwev_q_du_d(v4i64_a, v4u64_b, v4i64_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaddwev.q.du.d(
+
+  // __lasx_xvmaddwev_d_wu_w
+  // xd, xj, xk
+  // V4DI, V4DI, UV8SI, V8SI
+  v4i64_r = __lasx_xvmaddwev_d_wu_w(v4i64_a, v8u32_b, v8i32_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaddwev.d.wu.w(
+
+  // __lasx_xvmaddwev_w_hu_h
+  // xd, xj, xk
+  // V8SI, V8SI, UV16HI, V16HI
+  v8i32_r = __lasx_xvmaddwev_w_hu_h(v8i32_a, v16u16_b, v16i16_c); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmaddwev.w.hu.h(
+
+  // __lasx_xvmaddwev_h_bu_b
+  // xd, xj, xk
+  // V16HI, V16HI, UV32QI, V32QI
+  v16i16_r = __lasx_xvmaddwev_h_bu_b(v16i16_a, v32u8_b, v32i8_c); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmaddwev.h.bu.b(
+
+  // __lasx_xvmaddwod_q_du_d
+  // xd, xj, xk
+  // V4DI, V4DI, UV4DI, V4DI
+  v4i64_r = __lasx_xvmaddwod_q_du_d(v4i64_a, v4u64_b, v4i64_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaddwod.q.du.d(
+
+  // __lasx_xvmaddwod_d_wu_w
+  // xd, xj, xk
+  // V4DI, V4DI, UV8SI, V8SI
+  v4i64_r = __lasx_xvmaddwod_d_wu_w(v4i64_a, v8u32_b, v8i32_c); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmaddwod.d.wu.w(
+
+  // __lasx_xvmaddwod_w_hu_h
+  // xd, xj, xk
+  // V8SI, V8SI, UV16HI, V16HI
+  v8i32_r = __lasx_xvmaddwod_w_hu_h(v8i32_a, v16u16_b, v16i16_c); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvmaddwod.w.hu.h(
+
+  // __lasx_xvmaddwod_h_bu_b
+  // xd, xj, xk
+  // V16HI, V16HI, UV32QI, V32QI
+  v16i16_r = __lasx_xvmaddwod_h_bu_b(v16i16_a, v32u8_b, v32i8_c); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvmaddwod.h.bu.b(
+
+  // __lasx_xvrotr_b
+  // xd, xj, xk
+  // V32QI, V32QI, V32QI
+  v32i8_r = __lasx_xvrotr_b(v32i8_a, v32i8_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvrotr.b(
+
+  // __lasx_xvrotr_h
+  // xd, xj, xk
+  // V16HI, V16HI, V16HI
+  v16i16_r = __lasx_xvrotr_h(v16i16_a, v16i16_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvrotr.h(
+
+  // __lasx_xvrotr_w
+  // xd, xj, xk
+  // V8SI, V8SI, V8SI
+  v8i32_r = __lasx_xvrotr_w(v8i32_a, v8i32_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvrotr.w(
+
+  // __lasx_xvrotr_d
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvrotr_d(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvrotr.d(
+
+  // __lasx_xvadd_q
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvadd_q(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvadd.q(
+
+  // __lasx_xvsub_q
+  // xd, xj, xk
+  // V4DI, V4DI, V4DI
+  v4i64_r = __lasx_xvsub_q(v4i64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsub.q(
+
+  // __lasx_xvaddwev_q_du_d
+  // xd, xj, xk
+  // V4DI, UV4DI, V4DI
+  v4i64_r = __lasx_xvaddwev_q_du_d(v4u64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvaddwev.q.du.d(
+
+  // __lasx_xvaddwod_q_du_d
+  // xd, xj, xk
+  // V4DI, UV4DI, V4DI
+  v4i64_r = __lasx_xvaddwod_q_du_d(v4u64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvaddwod.q.du.d(
+
+  // __lasx_xvmulwev_q_du_d
+  // xd, xj, xk
+  // V4DI, UV4DI, V4DI
+  v4i64_r = __lasx_xvmulwev_q_du_d(v4u64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmulwev.q.du.d(
+
+  // __lasx_xvmulwod_q_du_d
+  // xd, xj, xk
+  // V4DI, UV4DI, V4DI
+  v4i64_r = __lasx_xvmulwod_q_du_d(v4u64_a, v4i64_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvmulwod.q.du.d(
+
+  // __lasx_xvmskgez_b
+  // xd, xj
+  // V32QI, V32QI
+  v32i8_r = __lasx_xvmskgez_b(v32i8_a); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmskgez.b(
+
+  // __lasx_xvmsknz_b
+  // xd, xj
+  // V32QI, V32QI
+  v32i8_r = __lasx_xvmsknz_b(v32i8_a); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvmsknz.b(
+
+  // __lasx_xvexth_h_b
+  // xd, xj
+  // V16HI, V32QI
+  v16i16_r = __lasx_xvexth_h_b(v32i8_a); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvexth.h.b(
+
+  // __lasx_xvexth_w_h
+  // xd, xj
+  // V8SI, V16HI
+  v8i32_r = __lasx_xvexth_w_h(v16i16_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvexth.w.h(
+
+  // __lasx_xvexth_d_w
+  // xd, xj
+  // V4DI, V8SI
+  v4i64_r = __lasx_xvexth_d_w(v8i32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvexth.d.w(
+
+  // __lasx_xvexth_q_d
+  // xd, xj
+  // V4DI, V4DI
+  v4i64_r = __lasx_xvexth_q_d(v4i64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvexth.q.d(
+
+  // __lasx_xvexth_hu_bu
+  // xd, xj
+  // UV16HI, UV32QI
+  v16u16_r = __lasx_xvexth_hu_bu(v32u8_a); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvexth.hu.bu(
+
+  // __lasx_xvexth_wu_hu
+  // xd, xj
+  // UV8SI, UV16HI
+  v8u32_r = __lasx_xvexth_wu_hu(v16u16_a); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvexth.wu.hu(
+
+  // __lasx_xvexth_du_wu
+  // xd, xj
+  // UV4DI, UV8SI
+  v4u64_r = __lasx_xvexth_du_wu(v8u32_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvexth.du.wu(
+
+  // __lasx_xvexth_qu_du
+  // xd, xj
+  // UV4DI, UV4DI
+  v4u64_r = __lasx_xvexth_qu_du(v4u64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvexth.qu.du(
+
+  // __lasx_xvextl_q_d
+  // xd, xj
+  // V4DI, V4DI
+  v4i64_r = __lasx_xvextl_q_d(v4i64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvextl.q.d(
+
+  // __lasx_xvextl_qu_du
+  // xd, xj
+  // UV4DI, UV4DI
+  v4u64_r = __lasx_xvextl_qu_du(v4u64_a); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvextl.qu.du(
+
+  // __lasx_xvrotri_b
+  // xd, xj, ui3
+  // V32QI, V32QI, UQI
+  v32i8_r = __lasx_xvrotri_b(v32i8_a, ui3_b); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvrotri.b(
+
+  // __lasx_xvrotri_h
+  // xd, xj, ui4
+  // V16HI, V16HI, UQI
+  v16i16_r = __lasx_xvrotri_h(v16i16_a, ui4_b); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvrotri.h(
+
+  // __lasx_xvrotri_w
+  // xd, xj, ui5
+  // V8SI, V8SI, UQI
+  v8i32_r = __lasx_xvrotri_w(v8i32_a, ui5_b); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvrotri.w(
+
+  // __lasx_xvrotri_d
+  // xd, xj, ui6
+  // V4DI, V4DI, UQI
+  v4i64_r = __lasx_xvrotri_d(v4i64_a, ui6_b); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvrotri.d(
+
+  // __lasx_xvsrlni_b_h
+  // xd, xj, ui4
+  // V32QI, V32QI, V32QI, USI
+  v32i8_r = __lasx_xvsrlni_b_h(v32i8_a, v32i8_b, ui4); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrlni.b.h(
+
+  // __lasx_xvsrlni_h_w
+  // xd, xj, ui5
+  // V16HI, V16HI, V16HI, USI
+  v16i16_r = __lasx_xvsrlni_h_w(v16i16_a, v16i16_b, ui5); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrlni.h.w(
+
+  // __lasx_xvsrlni_w_d
+  // xd, xj, ui6
+  // V8SI, V8SI, V8SI, USI
+  v8i32_r = __lasx_xvsrlni_w_d(v8i32_a, v8i32_b, ui6); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrlni.w.d(
+
+  // __lasx_xvsrlni_d_q
+  // xd, xj, ui7
+  // V4DI, V4DI, V4DI, USI
+  v4i64_r = __lasx_xvsrlni_d_q(v4i64_a, v4i64_b, ui7); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsrlni.d.q(
+
+  // __lasx_xvsrlrni_b_h
+  // xd, xj, ui4
+  // V32QI, V32QI, V32QI, USI
+  v32i8_r = __lasx_xvsrlrni_b_h(v32i8_a, v32i8_b, ui4); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrlrni.b.h(
+
+  // __lasx_xvsrlrni_h_w
+  // xd, xj, ui5
+  // V16HI, V16HI, V16HI, USI
+  v16i16_r = __lasx_xvsrlrni_h_w(v16i16_a, v16i16_b, ui5); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrlrni.h.w(
+
+  // __lasx_xvsrlrni_w_d
+  // xd, xj, ui6
+  // V8SI, V8SI, V8SI, USI
+  v8i32_r = __lasx_xvsrlrni_w_d(v8i32_a, v8i32_b, ui6); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrlrni.w.d(
+
+  // __lasx_xvsrlrni_d_q
+  // xd, xj, ui7
+  // V4DI, V4DI, V4DI, USI
+  v4i64_r = __lasx_xvsrlrni_d_q(v4i64_a, v4i64_b, ui7); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsrlrni.d.q(
+
+  // __lasx_xvssrlni_b_h
+  // xd, xj, ui4
+  // V32QI, V32QI, V32QI, USI
+  v32i8_r = __lasx_xvssrlni_b_h(v32i8_a, v32i8_b, ui4); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrlni.b.h(
+
+  // __lasx_xvssrlni_h_w
+  // xd, xj, ui5
+  // V16HI, V16HI, V16HI, USI
+  v16i16_r = __lasx_xvssrlni_h_w(v16i16_a, v16i16_b, ui5); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrlni.h.w(
+
+  // __lasx_xvssrlni_w_d
+  // xd, xj, ui6
+  // V8SI, V8SI, V8SI, USI
+  v8i32_r = __lasx_xvssrlni_w_d(v8i32_a, v8i32_b, ui6); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrlni.w.d(
+
+  // __lasx_xvssrlni_d_q
+  // xd, xj, ui7
+  // V4DI, V4DI, V4DI, USI
+  v4i64_r = __lasx_xvssrlni_d_q(v4i64_a, v4i64_b, ui7); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvssrlni.d.q(
+
+  // __lasx_xvssrlni_bu_h
+  // xd, xj, ui4
+  // UV32QI, UV32QI, V32QI, USI
+  v32u8_r = __lasx_xvssrlni_bu_h(v32u8_a, v32i8_b, ui4); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrlni.bu.h(
+
+  // __lasx_xvssrlni_hu_w
+  // xd, xj, ui5
+  // UV16HI, UV16HI, V16HI, USI
+  v16u16_r = __lasx_xvssrlni_hu_w(v16u16_a, v16i16_b, ui5); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrlni.hu.w(
+
+  // __lasx_xvssrlni_wu_d
+  // xd, xj, ui6
+  // UV8SI, UV8SI, V8SI, USI
+  v8u32_r = __lasx_xvssrlni_wu_d(v8u32_a, v8i32_b, ui6); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrlni.wu.d(
+
+  // __lasx_xvssrlni_du_q
+  // xd, xj, ui7
+  // UV4DI, UV4DI, V4DI, USI
+  v4u64_r = __lasx_xvssrlni_du_q(v4u64_a, v4i64_b, ui7); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvssrlni.du.q(
+
+  // __lasx_xvssrlrni_b_h
+  // xd, xj, ui4
+  // V32QI, V32QI, V32QI, USI
+  v32i8_r = __lasx_xvssrlrni_b_h(v32i8_a, v32i8_b, ui4); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrlrni.b.h(
+
+  // __lasx_xvssrlrni_h_w
+  // xd, xj, ui5
+  // V16HI, V16HI, V16HI, USI
+  v16i16_r = __lasx_xvssrlrni_h_w(v16i16_a, v16i16_b, ui5); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrlrni.h.w(
+
+  // __lasx_xvssrlrni_w_d
+  // xd, xj, ui6
+  // V8SI, V8SI, V8SI, USI
+  v8i32_r = __lasx_xvssrlrni_w_d(v8i32_a, v8i32_b, ui6); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrlrni.w.d(
+
+  // __lasx_xvssrlrni_d_q
+  // xd, xj, ui7
+  // V4DI, V4DI, V4DI, USI
+  v4i64_r = __lasx_xvssrlrni_d_q(v4i64_a, v4i64_b, ui7); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvssrlrni.d.q(
+
+  // __lasx_xvssrlrni_bu_h
+  // xd, xj, ui4
+  // UV32QI, UV32QI, V32QI, USI
+  v32u8_r = __lasx_xvssrlrni_bu_h(v32u8_a, v32i8_b, ui4); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrlrni.bu.h(
+
+  // __lasx_xvssrlrni_hu_w
+  // xd, xj, ui5
+  // UV16HI, UV16HI, V16HI, USI
+  v16u16_r = __lasx_xvssrlrni_hu_w(v16u16_a, v16i16_b, ui5); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrlrni.hu.w(
+
+  // __lasx_xvssrlrni_wu_d
+  // xd, xj, ui6
+  // UV8SI, UV8SI, V8SI, USI
+  v8u32_r = __lasx_xvssrlrni_wu_d(v8u32_a, v8i32_b, ui6); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrlrni.wu.d(
+
+  // __lasx_xvssrlrni_du_q
+  // xd, xj, ui7
+  // UV4DI, UV4DI, V4DI, USI
+  v4u64_r = __lasx_xvssrlrni_du_q(v4u64_a, v4i64_b, ui7); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvssrlrni.du.q(
+
+  // __lasx_xvsrani_b_h
+  // xd, xj, ui4
+  // V32QI, V32QI, V32QI, USI
+  v32i8_r = __lasx_xvsrani_b_h(v32i8_a, v32i8_b, ui4); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrani.b.h(
+
+  // __lasx_xvsrani_h_w
+  // xd, xj, ui5
+  // V16HI, V16HI, V16HI, USI
+  v16i16_r = __lasx_xvsrani_h_w(v16i16_a, v16i16_b, ui5); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrani.h.w(
+
+  // __lasx_xvsrani_w_d
+  // xd, xj, ui6
+  // V8SI, V8SI, V8SI, USI
+  v8i32_r = __lasx_xvsrani_w_d(v8i32_a, v8i32_b, ui6); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrani.w.d(
+
+  // __lasx_xvsrani_d_q
+  // xd, xj, ui7
+  // V4DI, V4DI, V4DI, USI
+  v4i64_r = __lasx_xvsrani_d_q(v4i64_a, v4i64_b, ui7); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsrani.d.q(
+
+  // __lasx_xvsrarni_b_h
+  // xd, xj, ui4
+  // V32QI, V32QI, V32QI, USI
+  v32i8_r = __lasx_xvsrarni_b_h(v32i8_a, v32i8_b, ui4); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvsrarni.b.h(
+
+  // __lasx_xvsrarni_h_w
+  // xd, xj, ui5
+  // V16HI, V16HI, V16HI, USI
+  v16i16_r = __lasx_xvsrarni_h_w(v16i16_a, v16i16_b, ui5); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvsrarni.h.w(
+
+  // __lasx_xvsrarni_w_d
+  // xd, xj, ui6
+  // V8SI, V8SI, V8SI, USI
+  v8i32_r = __lasx_xvsrarni_w_d(v8i32_a, v8i32_b, ui6); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvsrarni.w.d(
+
+  // __lasx_xvsrarni_d_q
+  // xd, xj, ui7
+  // V4DI, V4DI, V4DI, USI
+  v4i64_r = __lasx_xvsrarni_d_q(v4i64_a, v4i64_b, ui7); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvsrarni.d.q(
+
+  // __lasx_xvssrani_b_h
+  // xd, xj, ui4
+  // V32QI, V32QI, V32QI, USI
+  v32i8_r = __lasx_xvssrani_b_h(v32i8_a, v32i8_b, ui4); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrani.b.h(
+
+  // __lasx_xvssrani_h_w
+  // xd, xj, ui5
+  // V16HI, V16HI, V16HI, USI
+  v16i16_r = __lasx_xvssrani_h_w(v16i16_a, v16i16_b, ui5); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrani.h.w(
+
+  // __lasx_xvssrani_w_d
+  // xd, xj, ui6
+  // V8SI, V8SI, V8SI, USI
+  v8i32_r = __lasx_xvssrani_w_d(v8i32_a, v8i32_b, ui6); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrani.w.d(
+
+  // __lasx_xvssrani_d_q
+  // xd, xj, ui7
+  // V4DI, V4DI, V4DI, USI
+  v4i64_r = __lasx_xvssrani_d_q(v4i64_a, v4i64_b, ui7); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvssrani.d.q(
+
+  // __lasx_xvssrani_bu_h
+  // xd, xj, ui4
+  // UV32QI, UV32QI, V32QI, USI
+  v32u8_r = __lasx_xvssrani_bu_h(v32u8_a, v32i8_b, ui4); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrani.bu.h(
+
+  // __lasx_xvssrani_hu_w
+  // xd, xj, ui5
+  // UV16HI, UV16HI, V16HI, USI
+  v16u16_r = __lasx_xvssrani_hu_w(v16u16_a, v16i16_b, ui5); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrani.hu.w(
+
+  // __lasx_xvssrani_wu_d
+  // xd, xj, ui6
+  // UV8SI, UV8SI, V8SI, USI
+  v8u32_r = __lasx_xvssrani_wu_d(v8u32_a, v8i32_b, ui6); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrani.wu.d(
+
+  // __lasx_xvssrani_du_q
+  // xd, xj, ui7
+  // UV4DI, UV4DI, V4DI, USI
+  v4u64_r = __lasx_xvssrani_du_q(v4u64_a, v4i64_b, ui7); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvssrani.du.q(
+
+  // __lasx_xvssrarni_b_h
+  // xd, xj, ui4
+  // V32QI, V32QI, V32QI, USI
+  v32i8_r = __lasx_xvssrarni_b_h(v32i8_a, v32i8_b, ui4); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrarni.b.h(
+
+  // __lasx_xvssrarni_h_w
+  // xd, xj, ui5
+  // V16HI, V16HI, V16HI, USI
+  v16i16_r = __lasx_xvssrarni_h_w(v16i16_a, v16i16_b, ui5); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrarni.h.w(
+
+  // __lasx_xvssrarni_w_d
+  // xd, xj, ui6
+  // V8SI, V8SI, V8SI, USI
+  v8i32_r = __lasx_xvssrarni_w_d(v8i32_a, v8i32_b, ui6); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrarni.w.d(
+
+  // __lasx_xvssrarni_d_q
+  // xd, xj, ui7
+  // V4DI, V4DI, V4DI, USI
+  v4i64_r = __lasx_xvssrarni_d_q(v4i64_a, v4i64_b, ui7); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvssrarni.d.q(
+
+  // __lasx_xvssrarni_bu_h
+  // xd, xj, ui4
+  // UV32QI, UV32QI, V32QI, USI
+  v32u8_r = __lasx_xvssrarni_bu_h(v32u8_a, v32i8_b, ui4); // CHECK: call <32 x i8> @llvm.loongarch.lasx.xvssrarni.bu.h(
+
+  // __lasx_xvssrarni_hu_w
+  // xd, xj, ui5
+  // UV16HI, UV16HI, V16HI, USI
+  v16u16_r = __lasx_xvssrarni_hu_w(v16u16_a, v16i16_b, ui5); // CHECK: call <16 x i16> @llvm.loongarch.lasx.xvssrarni.hu.w(
+
+  // __lasx_xvssrarni_wu_d
+  // xd, xj, ui6
+  // UV8SI, UV8SI, V8SI, USI
+  v8u32_r = __lasx_xvssrarni_wu_d(v8u32_a, v8i32_b, ui6); // CHECK: call <8 x i32> @llvm.loongarch.lasx.xvssrarni.wu.d(
+
+  // __lasx_xvssrarni_du_q
+  // xd, xj, ui7
+  // UV4DI, UV4DI, V4DI, USI
+  v4u64_r = __lasx_xvssrarni_du_q(v4u64_a, v4i64_b, ui7); // CHECK: call <4 x i64> @llvm.loongarch.lasx.xvssrarni.du.q(
+
+  // __lasx_xbnz_v
+  // rd, xj
+  // SI, UV32QI
+  i32_r = __lasx_xbnz_v(v32u8_a); // CHECK: call i32 @llvm.loongarch.lasx.xbnz.v(
+
+  // __lasx_xbz_v
+  // rd, xj
+  // SI, UV32QI
+  i32_r = __lasx_xbz_v(v32u8_a); // CHECK: call i32 @llvm.loongarch.lasx.xbz.v(
+
+  // __lasx_xbnz_b
+  // rd, xj
+  // SI, UV32QI
+  i32_r = __lasx_xbnz_b(v32u8_a); // CHECK: call i32 @llvm.loongarch.lasx.xbnz.b(
+
+  // __lasx_xbnz_h
+  // rd, xj
+  // SI, UV16HI
+  i32_r = __lasx_xbnz_h(v16u16_a); // CHECK: call i32 @llvm.loongarch.lasx.xbnz.h(
+
+  // __lasx_xbnz_w
+  // rd, xj
+  // SI, UV8SI
+  i32_r = __lasx_xbnz_w(v8u32_a); // CHECK: call i32 @llvm.loongarch.lasx.xbnz.w(
+
+  // __lasx_xbnz_d
+  // rd, xj
+  // SI, UV4DI
+  i32_r = __lasx_xbnz_d(v4u64_a); // CHECK: call i32 @llvm.loongarch.lasx.xbnz.d(
+
+  // __lasx_xbz_b
+  // rd, xj
+  // SI, UV32QI
+  i32_r = __lasx_xbz_b(v32u8_a); // CHECK: call i32 @llvm.loongarch.lasx.xbz.b(
+
+  // __lasx_xbz_h
+  // rd, xj
+  // SI, UV16HI
+  i32_r = __lasx_xbz_h(v16u16_a); // CHECK: call i32 @llvm.loongarch.lasx.xbz.h(
+
+  // __lasx_xbz_w
+  // rd, xj
+  // SI, UV8SI
+  i32_r = __lasx_xbz_w(v8u32_a); // CHECK: call i32 @llvm.loongarch.lasx.xbz.w(
+
+  // __lasx_xbz_d
+  // rd, xj
+  // SI, UV4DI
+  i32_r = __lasx_xbz_d(v4u64_a); // CHECK: call i32 @llvm.loongarch.lasx.xbz.d(
+}
diff --git a/test/CodeGen/builtins-loongarch-lsx-error.c b/test/CodeGen/builtins-loongarch-lsx-error.c
new file mode 100644
index 00000000..f566a736
--- /dev/null
+++ b/test/CodeGen/builtins-loongarch-lsx-error.c
@@ -0,0 +1,250 @@
+// REQUIRES: loongarch-registered-target
+// RUN: %clang_cc1 -triple loongarch64-unknown-linux-gnu -fsyntax-only %s \
+// RUN:            -target-feature +lsx \
+// RUN:            -verify -o - 2>&1
+
+#include <lsxintrin.h>
+
+void test() {
+  v16i8 v16i8_a = (v16i8){0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
+  v16i8 v16i8_b = (v16i8){1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
+  v16i8 v16i8_c = (v16i8){2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17};
+  v16i8 v16i8_r;
+  v8i16 v8i16_a = (v8i16){0, 1, 2, 3, 4, 5, 6, 7};
+  v8i16 v8i16_b = (v8i16){1, 2, 3, 4, 5, 6, 7, 8};
+  v8i16 v8i16_c = (v8i16){2, 3, 4, 5, 6, 7, 8, 9};
+  v8i16 v8i16_r;
+  v4i32 v4i32_a = (v4i32){0, 1, 2, 3};
+  v4i32 v4i32_b = (v4i32){1, 2, 3, 4};
+  v4i32 v4i32_c = (v4i32){2, 3, 4, 5};
+  v4i32 v4i32_r;
+  v2i64 v2i64_a = (v2i64){0, 1};
+  v2i64 v2i64_b = (v2i64){1, 2};
+  v2i64 v2i64_c = (v2i64){2, 3};
+  v2i64 v2i64_r;
+
+  v16u8 v16u8_a = (v16u8){0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
+  v16u8 v16u8_b = (v16u8){1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
+  v16u8 v16u8_c = (v16u8){2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17};
+  v16u8 v16u8_r;
+  v8u16 v8u16_a = (v8u16){0, 1, 2, 3, 4, 5, 6, 7};
+  v8u16 v8u16_b = (v8u16){1, 2, 3, 4, 5, 6, 7, 8};
+  v8u16 v8u16_c = (v8u16){2, 3, 4, 5, 6, 7, 8, 9};
+  v8u16 v8u16_r;
+  v4u32 v4u32_a = (v4u32){0, 1, 2, 3};
+  v4u32 v4u32_b = (v4u32){1, 2, 3, 4};
+  v4u32 v4u32_c = (v4u32){2, 3, 4, 5};
+  v4u32 v4u32_r;
+  v2u64 v2u64_a = (v2u64){0, 1};
+  v2u64 v2u64_b = (v2u64){1, 2};
+  v2u64 v2u64_c = (v2u64){2, 3};
+  v2u64 v2u64_r;
+
+  v4f32 v4f32_a = (v4f32){0.5, 1, 2, 3};
+  v4f32 v4f32_b = (v4f32){1.5, 2, 3, 4};
+  v4f32 v4f32_c = (v4f32){2.5, 3, 4, 5};
+  v4f32 v4f32_r;
+  v2f64 v2f64_a = (v2f64){0.5, 1};
+  v2f64 v2f64_b = (v2f64){1.5, 2};
+  v2f64 v2f64_c = (v2f64){2.5, 3};
+  v2f64 v2f64_r;
+
+  int i32_r;
+  int i32_a = 1;
+  int i32_b = 2;
+  unsigned int u32_r;
+  unsigned int u32_a = 1;
+  unsigned int u32_b = 2;
+  long long i64_r;
+  long long i64_a = 1;
+  long long i64_b = 2;
+  long long i64_c = 3;
+  unsigned long long u64_r;
+  unsigned long long u64_a = 1;
+  unsigned long long u64_b = 2;
+  unsigned long long u64_c = 3;
+
+  v16i8_r = __lsx_vslli_b(v16i8_a, 8);                  // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8i16_r = __lsx_vslli_h(v8i16_a, 16);                 // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v4i32_r = __lsx_vslli_w(v4i32_a, 32);                 // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2i64_r = __lsx_vslli_d(v2i64_a, 64);                 // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v16i8_r = __lsx_vsrai_b(v16i8_a, 8);                  // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8i16_r = __lsx_vsrai_h(v8i16_a, 16);                 // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v4i32_r = __lsx_vsrai_w(v4i32_a, 32);                 // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2i64_r = __lsx_vsrai_d(v2i64_a, 64);                 // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v16i8_r = __lsx_vsrari_b(v16i8_a, 8);                 // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8i16_r = __lsx_vsrari_h(v8i16_a, 16);                // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v4i32_r = __lsx_vsrari_w(v4i32_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2i64_r = __lsx_vsrari_d(v2i64_a, 64);                // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v16i8_r = __lsx_vsrli_b(v16i8_a, 8);                  // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8i16_r = __lsx_vsrli_h(v8i16_a, 16);                 // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v4i32_r = __lsx_vsrli_w(v4i32_a, 32);                 // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2i64_r = __lsx_vsrli_d(v2i64_a, 64);                 // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v16i8_r = __lsx_vsrlri_b(v16i8_a, 8);                 // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8i16_r = __lsx_vsrlri_h(v8i16_a, 16);                // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v4i32_r = __lsx_vsrlri_w(v4i32_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2i64_r = __lsx_vsrlri_d(v2i64_a, 64);                // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v16u8_r = __lsx_vbitclri_b(v16u8_a, 8);               // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8u16_r = __lsx_vbitclri_h(v8u16_a, 16);              // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v4u32_r = __lsx_vbitclri_w(v4u32_a, 32);              // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2u64_r = __lsx_vbitclri_d(v2u64_a, 64);              // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v16u8_r = __lsx_vbitseti_b(v16u8_a, 8);               // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8u16_r = __lsx_vbitseti_h(v8u16_a, 16);              // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v4u32_r = __lsx_vbitseti_w(v4u32_a, 32);              // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2u64_r = __lsx_vbitseti_d(v2u64_a, 64);              // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v16u8_r = __lsx_vbitrevi_b(v16u8_a, 8);               // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8u16_r = __lsx_vbitrevi_h(v8u16_a, 16);              // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v4u32_r = __lsx_vbitrevi_w(v4u32_a, 32);              // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2u64_r = __lsx_vbitrevi_d(v2u64_a, 64);              // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v16i8_r = __lsx_vaddi_bu(v16i8_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i16_r = __lsx_vaddi_hu(v8i16_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i32_r = __lsx_vaddi_wu(v4i32_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2i64_r = __lsx_vaddi_du(v2i64_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i8_r = __lsx_vsubi_bu(v16i8_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i16_r = __lsx_vsubi_hu(v8i16_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i32_r = __lsx_vsubi_wu(v4i32_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2i64_r = __lsx_vsubi_du(v2i64_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i8_r = __lsx_vmaxi_b(v16i8_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v8i16_r = __lsx_vmaxi_h(v8i16_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v4i32_r = __lsx_vmaxi_w(v4i32_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v2i64_r = __lsx_vmaxi_d(v2i64_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v16u8_r = __lsx_vmaxi_bu(v16u8_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8u16_r = __lsx_vmaxi_hu(v8u16_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4u32_r = __lsx_vmaxi_wu(v4u32_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2u64_r = __lsx_vmaxi_du(v2u64_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i8_r = __lsx_vmini_b(v16i8_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v8i16_r = __lsx_vmini_h(v8i16_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v4i32_r = __lsx_vmini_w(v4i32_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v2i64_r = __lsx_vmini_d(v2i64_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v16u8_r = __lsx_vmini_bu(v16u8_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8u16_r = __lsx_vmini_hu(v8u16_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4u32_r = __lsx_vmini_wu(v4u32_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2u64_r = __lsx_vmini_du(v2u64_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i8_r = __lsx_vseqi_b(v16i8_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v8i16_r = __lsx_vseqi_h(v8i16_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v4i32_r = __lsx_vseqi_w(v4i32_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v2i64_r = __lsx_vseqi_d(v2i64_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v16i8_r = __lsx_vslti_b(v16i8_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v8i16_r = __lsx_vslti_h(v8i16_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v4i32_r = __lsx_vslti_w(v4i32_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v2i64_r = __lsx_vslti_d(v2i64_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v16i8_r = __lsx_vslti_bu(v16u8_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i16_r = __lsx_vslti_hu(v8u16_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i32_r = __lsx_vslti_wu(v4u32_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2i64_r = __lsx_vslti_du(v2u64_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i8_r = __lsx_vslei_b(v16i8_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v8i16_r = __lsx_vslei_h(v8i16_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v4i32_r = __lsx_vslei_w(v4i32_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v2i64_r = __lsx_vslei_d(v2i64_a, -17);                // expected-error {{argument value -17 is outside the valid range [-16, 15]}}
+  v16i8_r = __lsx_vslei_bu(v16u8_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i16_r = __lsx_vslei_hu(v8u16_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i32_r = __lsx_vslei_wu(v4u32_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2i64_r = __lsx_vslei_du(v2u64_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i8_r = __lsx_vsat_b(v16i8_a, 8);                   // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8i16_r = __lsx_vsat_h(v8i16_a, 16);                  // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v4i32_r = __lsx_vsat_w(v4i32_a, 32);                  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2i64_r = __lsx_vsat_d(v2i64_a, 64);                  // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v16u8_r = __lsx_vsat_bu(v16u8_a, 8);                  // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8u16_r = __lsx_vsat_hu(v8u16_a, 16);                 // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v4u32_r = __lsx_vsat_wu(v4u32_a, 32);                 // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2u64_r = __lsx_vsat_du(v2u64_a, 64);                 // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v16i8_r = __lsx_vreplvei_b(v16i8_a, 16);              // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i16_r = __lsx_vreplvei_h(v8i16_a, 8);               // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v4i32_r = __lsx_vreplvei_w(v4i32_a, 4);               // expected-error {{argument value 4 is outside the valid range [0, 3]}}
+  v2i64_r = __lsx_vreplvei_d(v2i64_a, 2);               // expected-error {{argument value 2 is outside the valid range [0, 1]}}
+  v16u8_r = __lsx_vandi_b(v16u8_a, 256);                // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v16u8_r = __lsx_vori_b(v16u8_a, 256);                 // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v16u8_r = __lsx_vnori_b(v16u8_a, 256);                // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v16u8_r = __lsx_vxori_b(v16u8_a, 256);                // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v16u8_r = __lsx_vbitseli_b(v16u8_a, v16u8_b, 256);    // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v16i8_r = __lsx_vshuf4i_b(v16i8_a, 256);              // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v8i16_r = __lsx_vshuf4i_h(v8i16_a, 256);              // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v4i32_r = __lsx_vshuf4i_w(v4i32_a, 256);              // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  i32_r = __lsx_vpickve2gr_b(v16i8_a, 16);              // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  i32_r = __lsx_vpickve2gr_h(v8i16_a, 8);               // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  i32_r = __lsx_vpickve2gr_w(v4i32_a, 4);               // expected-error {{argument value 4 is outside the valid range [0, 3]}}
+  i64_r = __lsx_vpickve2gr_d(v2i64_a, 2);               // expected-error {{argument value 2 is outside the valid range [0, 1]}}
+  u32_r = __lsx_vpickve2gr_bu(v16i8_a, 16);             // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  u32_r = __lsx_vpickve2gr_hu(v8i16_a, 8);              // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  u32_r = __lsx_vpickve2gr_wu(v4i32_a, 4);              // expected-error {{argument value 4 is outside the valid range [0, 3]}}
+  u64_r = __lsx_vpickve2gr_du(v2i64_a, 2);              // expected-error {{argument value 2 is outside the valid range [0, 1]}}
+  v16i8_r = __lsx_vinsgr2vr_b(v16i8_a, i32_b, 16);      // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i16_r = __lsx_vinsgr2vr_h(v8i16_a, i32_b, 8);       // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v4i32_r = __lsx_vinsgr2vr_w(v4i32_a, i32_b, 4);       // expected-error {{argument value 4 is outside the valid range [0, 3]}}
+  v2i64_r = __lsx_vinsgr2vr_d(v2i64_a, i32_b, 2);       // expected-error {{argument value 2 is outside the valid range [0, 1]}}
+  v8i16_r = __lsx_vsllwil_h_b(v16i8_a, 8);              // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v4i32_r = __lsx_vsllwil_w_h(v8i16_a, 16);             // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v2i64_r = __lsx_vsllwil_d_w(v4i32_a, 32);             // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8u16_r = __lsx_vsllwil_hu_bu(v16u8_a, 8);            // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v4u32_r = __lsx_vsllwil_wu_hu(v8u16_a, 16);           // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v2u64_r = __lsx_vsllwil_du_wu(v4u32_a, 32);           // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i8_r = __lsx_vfrstpi_b(v16i8_a, v16i8_b, 32);      // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v8i16_r = __lsx_vfrstpi_h(v8i16_a, v8i16_b, 32);      // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2i64_r = __lsx_vshuf4i_d(v2i64_a, v2i64_b, 256);     // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v16i8_r = __lsx_vbsrl_v(v16i8_a, 32);                 // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i8_r = __lsx_vbsll_v(v16i8_a, 32);                 // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v16i8_r = __lsx_vextrins_b(v16i8_a, v16i8_b, 256);    // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v8i16_r = __lsx_vextrins_h(v8i16_a, v8i16_b, 256);    // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v4i32_r = __lsx_vextrins_w(v4i32_a, v4i32_b, 256);    // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v2i64_r = __lsx_vextrins_d(v2i64_a, v2i64_b, 256);    // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  __lsx_vstelm_b(v16i8_a, &v16i8_b, 0, 16);             // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  __lsx_vstelm_h(v8i16_a, &v8i16_b, 0, 8);              // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  __lsx_vstelm_w(v4i32_a, &v4i32_b, 0, 4);              // expected-error {{argument value 4 is outside the valid range [0, 3]}}
+  __lsx_vstelm_d(v2i64_a, &v2i64_b, 0, 2);              // expected-error {{argument value 2 is outside the valid range [0, 1]}}
+  v16i8_r = __lsx_vldrepl_b(&v16i8_a, -2049);           // expected-error {{argument value -2049 is outside the valid range [-2048, 2047]}}
+  v8i16_r = __lsx_vldrepl_h(&v8i16_a, -1025);           // expected-error {{argument value -1025 is outside the valid range [-1024, 1023]}}
+  v4i32_r = __lsx_vldrepl_w(&v4i32_a, -513);            // expected-error {{argument value -513 is outside the valid range [-512, 511]}}
+  v2i64_r = __lsx_vldrepl_d(&v2i64_a, -257);            // expected-error {{argument value -257 is outside the valid range [-256, 255]}}
+  v16i8_r = __lsx_vrotri_b(v16i8_a, 8);                 // expected-error {{argument value 8 is outside the valid range [0, 7]}}
+  v8i16_r = __lsx_vrotri_h(v8i16_a, 16);                // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v4i32_r = __lsx_vrotri_w(v4i32_a, 32);                // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v2i64_r = __lsx_vrotri_d(v2i64_a, 64);                // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v16i8_r = __lsx_vsrlni_b_h(v16i8_a, v16i8_b, 16);     // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i16_r = __lsx_vsrlni_h_w(v8i16_a, v8i16_b, 32);     // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i32_r = __lsx_vsrlni_w_d(v4i32_a, v4i32_b, 64);     // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v2i64_r = __lsx_vsrlni_d_q(v2i64_a, v2i64_b, 128);    // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v16i8_r = __lsx_vssrlni_b_h(v16i8_a, v16i8_b, 16);    // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i16_r = __lsx_vssrlni_h_w(v8i16_a, v8i16_b, 32);    // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i32_r = __lsx_vssrlni_w_d(v4i32_a, v4i32_b, 64);    // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v2i64_r = __lsx_vssrlni_d_q(v2i64_a, v2i64_b, 128);   // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v16u8_r = __lsx_vssrlni_bu_h(v16u8_a, v16i8_b, 16);   // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8u16_r = __lsx_vssrlni_hu_w(v8u16_a, v8i16_b, 32);   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4u32_r = __lsx_vssrlni_wu_d(v4u32_a, v4i32_b, 64);   // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v2u64_r = __lsx_vssrlni_du_q(v2u64_a, v2i64_b, 128);  // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v16i8_r = __lsx_vssrlrni_b_h(v16i8_a, v16i8_b, 16);   // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i16_r = __lsx_vssrlrni_h_w(v8i16_a, v8i16_b, 32);   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i32_r = __lsx_vssrlrni_w_d(v4i32_a, v4i32_b, 64);   // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v2i64_r = __lsx_vssrlrni_d_q(v2i64_a, v2i64_b, 128);  // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v16u8_r = __lsx_vssrlrni_bu_h(v16u8_a, v16i8_b, 16);  // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8u16_r = __lsx_vssrlrni_hu_w(v8u16_a, v8i16_b, 32);  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4u32_r = __lsx_vssrlrni_wu_d(v4u32_a, v4i32_b, 64);  // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v2u64_r = __lsx_vssrlrni_du_q(v2u64_a, v2i64_b, 128); // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v16i8_r = __lsx_vsrani_b_h(v16i8_a, v16i8_b, 16);     // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i16_r = __lsx_vsrani_h_w(v8i16_a, v8i16_b, 32);     // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i32_r = __lsx_vsrani_w_d(v4i32_a, v4i32_b, 64);     // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v2i64_r = __lsx_vsrani_d_q(v2i64_a, v2i64_b, 128);    // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v16i8_r = __lsx_vsrarni_b_h(v16i8_a, v16i8_b, 16);    // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i16_r = __lsx_vsrarni_h_w(v8i16_a, v8i16_b, 32);    // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i32_r = __lsx_vsrarni_w_d(v4i32_a, v4i32_b, 64);    // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v2i64_r = __lsx_vsrarni_d_q(v2i64_a, v2i64_b, 128);   // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v16i8_r = __lsx_vssrani_b_h(v16i8_a, v16i8_b, 16);    // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i16_r = __lsx_vssrani_h_w(v8i16_a, v8i16_b, 32);    // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i32_r = __lsx_vssrani_w_d(v4i32_a, v4i32_b, 64);    // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v2i64_r = __lsx_vssrani_d_q(v2i64_a, v2i64_b, 128);   // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v16u8_r = __lsx_vssrani_bu_h(v16u8_a, v16i8_b, 16);   // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8u16_r = __lsx_vssrani_hu_w(v8u16_a, v8i16_b, 32);   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4u32_r = __lsx_vssrani_wu_d(v4u32_a, v4i32_b, 64);   // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v2u64_r = __lsx_vssrani_du_q(v2u64_a, v2i64_b, 128);  // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v16i8_r = __lsx_vssrarni_b_h(v16i8_a, v16i8_b, 16);   // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8i16_r = __lsx_vssrarni_h_w(v8i16_a, v8i16_b, 32);   // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4i32_r = __lsx_vssrarni_w_d(v4i32_a, v4i32_b, 64);   // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v2i64_r = __lsx_vssrarni_d_q(v2i64_a, v2i64_b, 128);  // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v16u8_r = __lsx_vssrarni_bu_h(v16u8_a, v16i8_b, 16);  // expected-error {{argument value 16 is outside the valid range [0, 15]}}
+  v8u16_r = __lsx_vssrarni_hu_w(v8u16_a, v8i16_b, 32);  // expected-error {{argument value 32 is outside the valid range [0, 31]}}
+  v4u32_r = __lsx_vssrarni_wu_d(v4u32_a, v4i32_b, 64);  // expected-error {{argument value 64 is outside the valid range [0, 63]}}
+  v2u64_r = __lsx_vssrarni_du_q(v2u64_a, v2i64_b, 128); // expected-error {{argument value 128 is outside the valid range [0, 127]}}
+  v4i32_r = __lsx_vpermi_w(v4i32_a, v4i32_b, 256);      // expected-error {{argument value 256 is outside the valid range [0, 255]}}
+  v16i8_r = __lsx_vld(&v16i8_a, -2049);                 // expected-error {{argument value -2049 is outside the valid range [-2048, 2047]}}
+  __lsx_vst(v16i8_a, &v16i8_b, -2049);                  // expected-error {{argument value -2049 is outside the valid range [-2048, 2047]}}
+  v2i64_r = __lsx_vldi(-4097);                          // expected-error {{argument value -4097 is outside the valid range [-4096, 4095]}}
+}
diff --git a/test/CodeGen/builtins-loongarch-lsx.c b/test/CodeGen/builtins-loongarch-lsx.c
new file mode 100644
index 00000000..0cfc2105
--- /dev/null
+++ b/test/CodeGen/builtins-loongarch-lsx.c
@@ -0,0 +1,3630 @@
+// REQUIRES: loongarch-registered-target
+// RUN: %clang_cc1 -triple loongarch64-unknown-linux-gnu -emit-llvm %s \
+// RUN:            -target-feature +lsx \
+// RUN:            -target-feature +d \
+// RUN:            -o - | FileCheck %s
+
+#include <lsxintrin.h>
+
+#define ui1 0
+#define ui2 1
+#define ui3 4
+#define ui4 7
+#define ui5 25
+#define ui6 44
+#define ui7 100
+#define ui8 127 //200
+#define si5 -4
+#define si8 -100
+#define si9 0
+#define si10 0
+#define si11 0
+#define si12 0
+#define i10 500
+#define i13 4000
+#define mode 11
+#define idx1 1
+#define idx2 2
+#define idx3 4
+#define idx4 8
+
+void test(void) {
+  v16i8 v16i8_a = (v16i8){0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
+  v16i8 v16i8_b = (v16i8){1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
+  v16i8 v16i8_c = (v16i8){2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17};
+  v16i8 v16i8_r;
+  v8i16 v8i16_a = (v8i16){0, 1, 2, 3, 4, 5, 6, 7};
+  v8i16 v8i16_b = (v8i16){1, 2, 3, 4, 5, 6, 7, 8};
+  v8i16 v8i16_c = (v8i16){2, 3, 4, 5, 6, 7, 8, 9};
+  v8i16 v8i16_r;
+  v4i32 v4i32_a = (v4i32){0, 1, 2, 3};
+  v4i32 v4i32_b = (v4i32){1, 2, 3, 4};
+  v4i32 v4i32_c = (v4i32){2, 3, 4, 5};
+  v4i32 v4i32_r;
+  v2i64 v2i64_a = (v2i64){0, 1};
+  v2i64 v2i64_b = (v2i64){1, 2};
+  v2i64 v2i64_c = (v2i64){2, 3};
+  v2i64 v2i64_r;
+
+  v16u8 v16u8_a = (v16u8){0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
+  v16u8 v16u8_b = (v16u8){1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
+  v16u8 v16u8_c = (v16u8){2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17};
+  v16u8 v16u8_r;
+  v8u16 v8u16_a = (v8u16){0, 1, 2, 3, 4, 5, 6, 7};
+  v8u16 v8u16_b = (v8u16){1, 2, 3, 4, 5, 6, 7, 8};
+  v8u16 v8u16_c = (v8u16){2, 3, 4, 5, 6, 7, 8, 9};
+  v8u16 v8u16_r;
+  v4u32 v4u32_a = (v4u32){0, 1, 2, 3};
+  v4u32 v4u32_b = (v4u32){1, 2, 3, 4};
+  v4u32 v4u32_c = (v4u32){2, 3, 4, 5};
+  v4u32 v4u32_r;
+  v2u64 v2u64_a = (v2u64){0, 1};
+  v2u64 v2u64_b = (v2u64){1, 2};
+  v2u64 v2u64_c = (v2u64){2, 3};
+  v2u64 v2u64_r;
+
+  v4f32 v4f32_a = (v4f32){0.5, 1, 2, 3};
+  v4f32 v4f32_b = (v4f32){1.5, 2, 3, 4};
+  v4f32 v4f32_c = (v4f32){2.5, 3, 4, 5};
+  v4f32 v4f32_r;
+  v2f64 v2f64_a = (v2f64){0.5, 1};
+  v2f64 v2f64_b = (v2f64){1.5, 2};
+  v2f64 v2f64_c = (v2f64){2.5, 3};
+  v2f64 v2f64_r;
+
+  int i32_r;
+  int i32_a = 1;
+  int i32_b = 2;
+  unsigned int u32_r;
+  unsigned int u32_a = 1;
+  unsigned int u32_b = 2;
+  long long i64_r;
+  long long i64_a = 1;
+  long long i64_b = 2;
+  long long i64_c = 3;
+  long int i64_d = 0;
+  unsigned long long u64_r;
+  unsigned long long u64_a = 1;
+  unsigned long long u64_b = 2;
+  unsigned long long u64_c = 3;
+
+  // __lsx_vsll_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vsll_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsll.b(
+
+  // __lsx_vsll_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vsll_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsll.h(
+
+  // __lsx_vsll_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vsll_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsll.w(
+
+  // __lsx_vsll_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vsll_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsll.d(
+
+  // __lsx_vslli_b
+  // vd, vj, ui3
+  // V16QI, V16QI, UQI
+  v16i8_r = __lsx_vslli_b(v16i8_a, ui3); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vslli.b(
+
+  // __lsx_vslli_h
+  // vd, vj, ui4
+  // V8HI, V8HI, UQI
+  v8i16_r = __lsx_vslli_h(v8i16_a, ui4); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vslli.h(
+
+  // __lsx_vslli_w
+  // vd, vj, ui5
+  // V4SI, V4SI, UQI
+  v4i32_r = __lsx_vslli_w(v4i32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vslli.w(
+
+  // __lsx_vslli_d
+  // vd, vj, ui6
+  // V2DI, V2DI, UQI
+  v2i64_r = __lsx_vslli_d(v2i64_a, ui6); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vslli.d(
+
+  // __lsx_vsra_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vsra_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsra.b(
+
+  // __lsx_vsra_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vsra_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsra.h(
+
+  // __lsx_vsra_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vsra_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsra.w(
+
+  // __lsx_vsra_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vsra_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsra.d(
+
+  // __lsx_vsrai_b
+  // vd, vj, ui3
+  // V16QI, V16QI, UQI
+  v16i8_r = __lsx_vsrai_b(v16i8_a, ui3); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsrai.b(
+
+  // __lsx_vsrai_h
+  // vd, vj, ui4
+  // V8HI, V8HI, UQI
+  v8i16_r = __lsx_vsrai_h(v8i16_a, ui4); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsrai.h(
+
+  // __lsx_vsrai_w
+  // vd, vj, ui5
+  // V4SI, V4SI, UQI
+  v4i32_r = __lsx_vsrai_w(v4i32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsrai.w(
+
+  // __lsx_vsrai_d
+  // vd, vj, ui6
+  // V2DI, V2DI, UQI
+  v2i64_r = __lsx_vsrai_d(v2i64_a, ui6); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsrai.d(
+
+  // __lsx_vsrar_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vsrar_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsrar.b(
+
+  // __lsx_vsrar_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vsrar_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsrar.h(
+
+  // __lsx_vsrar_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vsrar_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsrar.w(
+
+  // __lsx_vsrar_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vsrar_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsrar.d(
+
+  // __lsx_vsrari_b
+  // vd, vj, ui3
+  // V16QI, V16QI, UQI
+  v16i8_r = __lsx_vsrari_b(v16i8_a, ui3); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsrari.b(
+
+  // __lsx_vsrari_h
+  // vd, vj, ui4
+  // V8HI, V8HI, UQI
+  v8i16_r = __lsx_vsrari_h(v8i16_a, ui4); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsrari.h(
+
+  // __lsx_vsrari_w
+  // vd, vj, ui5
+  // V4SI, V4SI, UQI
+  v4i32_r = __lsx_vsrari_w(v4i32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsrari.w(
+
+  // __lsx_vsrari_d
+  // vd, vj, ui6
+  // V2DI, V2DI, UQI
+  v2i64_r = __lsx_vsrari_d(v2i64_a, ui6); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsrari.d(
+
+  // __lsx_vsrl_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vsrl_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsrl.b(
+
+  // __lsx_vsrl_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vsrl_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsrl.h(
+
+  // __lsx_vsrl_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vsrl_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsrl.w(
+
+  // __lsx_vsrl_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vsrl_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsrl.d(
+
+  // __lsx_vsrli_b
+  // vd, vj, ui3
+  // V16QI, V16QI, UQI
+  v16i8_r = __lsx_vsrli_b(v16i8_a, ui3); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsrli.b(
+
+  // __lsx_vsrli_h
+  // vd, vj, ui4
+  // V8HI, V8HI, UQI
+  v8i16_r = __lsx_vsrli_h(v8i16_a, ui4); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsrli.h(
+
+  // __lsx_vsrli_w
+  // vd, vj, ui5
+  // V4SI, V4SI, UQI
+  v4i32_r = __lsx_vsrli_w(v4i32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsrli.w(
+
+  // __lsx_vsrli_d
+  // vd, vj, ui6
+  // V2DI, V2DI, UQI
+  v2i64_r = __lsx_vsrli_d(v2i64_a, ui6); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsrli.d(
+
+  // __lsx_vsrlr_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vsrlr_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsrlr.b(
+
+  // __lsx_vsrlr_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vsrlr_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsrlr.h(
+
+  // __lsx_vsrlr_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vsrlr_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsrlr.w(
+
+  // __lsx_vsrlr_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vsrlr_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsrlr.d(
+
+  // __lsx_vsrlri_b
+  // vd, vj, ui3
+  // V16QI, V16QI, UQI
+  v16i8_r = __lsx_vsrlri_b(v16i8_a, ui3); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsrlri.b(
+
+  // __lsx_vsrlri_h
+  // vd, vj, ui4
+  // V8HI, V8HI, UQI
+  v8i16_r = __lsx_vsrlri_h(v8i16_a, ui4); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsrlri.h(
+
+  // __lsx_vsrlri_w
+  // vd, vj, ui5
+  // V4SI, V4SI, UQI
+  v4i32_r = __lsx_vsrlri_w(v4i32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsrlri.w(
+
+  // __lsx_vsrlri_d
+  // vd, vj, ui6
+  // V2DI, V2DI, UQI
+  v2i64_r = __lsx_vsrlri_d(v2i64_a, ui6); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsrlri.d(
+
+  // __lsx_vbitclr_b
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vbitclr_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vbitclr.b(
+
+  // __lsx_vbitclr_h
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV8HI
+  v8u16_r = __lsx_vbitclr_h(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vbitclr.h(
+
+  // __lsx_vbitclr_w
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV4SI
+  v4u32_r = __lsx_vbitclr_w(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vbitclr.w(
+
+  // __lsx_vbitclr_d
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vbitclr_d(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vbitclr.d(
+
+  // __lsx_vbitclri_b
+  // vd, vj, ui3
+  // UV16QI, UV16QI, UQI
+  v16u8_r = __lsx_vbitclri_b(v16u8_a, ui3); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vbitclri.b(
+
+  // __lsx_vbitclri_h
+  // vd, vj, ui4
+  // UV8HI, UV8HI, UQI
+  v8u16_r = __lsx_vbitclri_h(v8u16_a, ui4); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vbitclri.h(
+
+  // __lsx_vbitclri_w
+  // vd, vj, ui5
+  // UV4SI, UV4SI, UQI
+  v4u32_r = __lsx_vbitclri_w(v4u32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vbitclri.w(
+
+  // __lsx_vbitclri_d
+  // vd, vj, ui6
+  // UV2DI, UV2DI, UQI
+  v2u64_r = __lsx_vbitclri_d(v2u64_a, ui6); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vbitclri.d(
+
+  // __lsx_vbitset_b
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vbitset_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vbitset.b(
+
+  // __lsx_vbitset_h
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV8HI
+  v8u16_r = __lsx_vbitset_h(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vbitset.h(
+
+  // __lsx_vbitset_w
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV4SI
+  v4u32_r = __lsx_vbitset_w(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vbitset.w(
+
+  // __lsx_vbitset_d
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vbitset_d(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vbitset.d(
+
+  // __lsx_vbitseti_b
+  // vd, vj, ui3
+  // UV16QI, UV16QI, UQI
+  v16u8_r = __lsx_vbitseti_b(v16u8_a, ui3); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vbitseti.b(
+
+  // __lsx_vbitseti_h
+  // vd, vj, ui4
+  // UV8HI, UV8HI, UQI
+  v8u16_r = __lsx_vbitseti_h(v8u16_a, ui4); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vbitseti.h(
+
+  // __lsx_vbitseti_w
+  // vd, vj, ui5
+  // UV4SI, UV4SI, UQI
+  v4u32_r = __lsx_vbitseti_w(v4u32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vbitseti.w(
+
+  // __lsx_vbitseti_d
+  // vd, vj, ui6
+  // UV2DI, UV2DI, UQI
+  v2u64_r = __lsx_vbitseti_d(v2u64_a, ui6); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vbitseti.d(
+
+  // __lsx_vbitrev_b
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vbitrev_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vbitrev.b(
+
+  // __lsx_vbitrev_h
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV8HI
+  v8u16_r = __lsx_vbitrev_h(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vbitrev.h(
+
+  // __lsx_vbitrev_w
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV4SI
+  v4u32_r = __lsx_vbitrev_w(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vbitrev.w(
+
+  // __lsx_vbitrev_d
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vbitrev_d(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vbitrev.d(
+
+  // __lsx_vbitrevi_b
+  // vd, vj, ui3
+  // UV16QI, UV16QI, UQI
+  v16u8_r = __lsx_vbitrevi_b(v16u8_a, ui3); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vbitrevi.b(
+
+  // __lsx_vbitrevi_h
+  // vd, vj, ui4
+  // UV8HI, UV8HI, UQI
+  v8u16_r = __lsx_vbitrevi_h(v8u16_a, ui4); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vbitrevi.h(
+
+  // __lsx_vbitrevi_w
+  // vd, vj, ui5
+  // UV4SI, UV4SI, UQI
+  v4u32_r = __lsx_vbitrevi_w(v4u32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vbitrevi.w(
+
+  // __lsx_vbitrevi_d
+  // vd, vj, ui6
+  // UV2DI, UV2DI, UQI
+  v2u64_r = __lsx_vbitrevi_d(v2u64_a, ui6); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vbitrevi.d(
+
+  // __lsx_vadd_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vadd_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vadd.b(
+
+  // __lsx_vadd_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vadd_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vadd.h(
+
+  // __lsx_vadd_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vadd_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vadd.w(
+
+  // __lsx_vadd_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vadd_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vadd.d(
+
+  // __lsx_vaddi_bu
+  // vd, vj, ui5
+  // V16QI, V16QI, UQI
+  v16i8_r = __lsx_vaddi_bu(v16i8_a, ui5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vaddi.bu(
+
+  // __lsx_vaddi_hu
+  // vd, vj, ui5
+  // V8HI, V8HI, UQI
+  v8i16_r = __lsx_vaddi_hu(v8i16_a, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vaddi.hu(
+
+  // __lsx_vaddi_wu
+  // vd, vj, ui5
+  // V4SI, V4SI, UQI
+  v4i32_r = __lsx_vaddi_wu(v4i32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vaddi.wu(
+
+  // __lsx_vaddi_du
+  // vd, vj, ui5
+  // V2DI, V2DI, UQI
+  v2i64_r = __lsx_vaddi_du(v2i64_a, ui5); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vaddi.du(
+
+  // __lsx_vsub_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vsub_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsub.b(
+
+  // __lsx_vsub_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vsub_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsub.h(
+
+  // __lsx_vsub_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vsub_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsub.w(
+
+  // __lsx_vsub_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vsub_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsub.d(
+
+  // __lsx_vsubi_bu
+  // vd, vj, ui5
+  // V16QI, V16QI, UQI
+  v16i8_r = __lsx_vsubi_bu(v16i8_a, ui5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsubi.bu(
+
+  // __lsx_vsubi_hu
+  // vd, vj, ui5
+  // V8HI, V8HI, UQI
+  v8i16_r = __lsx_vsubi_hu(v8i16_a, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsubi.hu(
+
+  // __lsx_vsubi_wu
+  // vd, vj, ui5
+  // V4SI, V4SI, UQI
+  v4i32_r = __lsx_vsubi_wu(v4i32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsubi.wu(
+
+  // __lsx_vsubi_du
+  // vd, vj, ui5
+  // V2DI, V2DI, UQI
+  v2i64_r = __lsx_vsubi_du(v2i64_a, ui5); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsubi.du(
+
+  // __lsx_vmax_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vmax_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmax.b(
+
+  // __lsx_vmax_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vmax_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmax.h(
+
+  // __lsx_vmax_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vmax_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmax.w(
+
+  // __lsx_vmax_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vmax_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmax.d(
+
+  // __lsx_vmaxi_b
+  // vd, vj, si5
+  // V16QI, V16QI, QI
+  v16i8_r = __lsx_vmaxi_b(v16i8_a, si5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmaxi.b(
+
+  // __lsx_vmaxi_h
+  // vd, vj, si5
+  // V8HI, V8HI, QI
+  v8i16_r = __lsx_vmaxi_h(v8i16_a, si5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmaxi.h(
+
+  // __lsx_vmaxi_w
+  // vd, vj, si5
+  // V4SI, V4SI, QI
+  v4i32_r = __lsx_vmaxi_w(v4i32_a, si5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmaxi.w(
+
+  // __lsx_vmaxi_d
+  // vd, vj, si5
+  // V2DI, V2DI, QI
+  v2i64_r = __lsx_vmaxi_d(v2i64_a, si5); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaxi.d(
+
+  // __lsx_vmax_bu
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vmax_bu(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmax.bu(
+
+  // __lsx_vmax_hu
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV8HI
+  v8u16_r = __lsx_vmax_hu(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmax.hu(
+
+  // __lsx_vmax_wu
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV4SI
+  v4u32_r = __lsx_vmax_wu(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmax.wu(
+
+  // __lsx_vmax_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vmax_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmax.du(
+
+  // __lsx_vmaxi_bu
+  // vd, vj, ui5
+  // UV16QI, UV16QI, UQI
+  v16u8_r = __lsx_vmaxi_bu(v16u8_a, ui5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmaxi.bu(
+
+  // __lsx_vmaxi_hu
+  // vd, vj, ui5
+  // UV8HI, UV8HI, UQI
+  v8u16_r = __lsx_vmaxi_hu(v8u16_a, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmaxi.hu(
+
+  // __lsx_vmaxi_wu
+  // vd, vj, ui5
+  // UV4SI, UV4SI, UQI
+  v4u32_r = __lsx_vmaxi_wu(v4u32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmaxi.wu(
+
+  // __lsx_vmaxi_du
+  // vd, vj, ui5
+  // UV2DI, UV2DI, UQI
+  v2u64_r = __lsx_vmaxi_du(v2u64_a, ui5); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaxi.du(
+
+  // __lsx_vmin_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vmin_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmin.b(
+
+  // __lsx_vmin_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vmin_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmin.h(
+
+  // __lsx_vmin_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vmin_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmin.w(
+
+  // __lsx_vmin_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vmin_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmin.d(
+
+  // __lsx_vmini_b
+  // vd, vj, si5
+  // V16QI, V16QI, QI
+  v16i8_r = __lsx_vmini_b(v16i8_a, si5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmini.b(
+
+  // __lsx_vmini_h
+  // vd, vj, si5
+  // V8HI, V8HI, QI
+  v8i16_r = __lsx_vmini_h(v8i16_a, si5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmini.h(
+
+  // __lsx_vmini_w
+  // vd, vj, si5
+  // V4SI, V4SI, QI
+  v4i32_r = __lsx_vmini_w(v4i32_a, si5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmini.w(
+
+  // __lsx_vmini_d
+  // vd, vj, si5
+  // V2DI, V2DI, QI
+  v2i64_r = __lsx_vmini_d(v2i64_a, si5); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmini.d(
+
+  // __lsx_vmin_bu
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vmin_bu(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmin.bu(
+
+  // __lsx_vmin_hu
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV8HI
+  v8u16_r = __lsx_vmin_hu(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmin.hu(
+
+  // __lsx_vmin_wu
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV4SI
+  v4u32_r = __lsx_vmin_wu(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmin.wu(
+
+  // __lsx_vmin_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vmin_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmin.du(
+
+  // __lsx_vmini_bu
+  // vd, vj, ui5
+  // UV16QI, UV16QI, UQI
+  v16u8_r = __lsx_vmini_bu(v16u8_a, ui5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmini.bu(
+
+  // __lsx_vmini_hu
+  // vd, vj, ui5
+  // UV8HI, UV8HI, UQI
+  v8u16_r = __lsx_vmini_hu(v8u16_a, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmini.hu(
+
+  // __lsx_vmini_wu
+  // vd, vj, ui5
+  // UV4SI, UV4SI, UQI
+  v4u32_r = __lsx_vmini_wu(v4u32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmini.wu(
+
+  // __lsx_vmini_du
+  // vd, vj, ui5
+  // UV2DI, UV2DI, UQI
+  v2u64_r = __lsx_vmini_du(v2u64_a, ui5); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmini.du(
+
+  // __lsx_vseq_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vseq_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vseq.b(
+
+  // __lsx_vseq_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vseq_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vseq.h(
+
+  // __lsx_vseq_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vseq_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vseq.w(
+
+  // __lsx_vseq_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vseq_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vseq.d(
+
+  // __lsx_vseqi_b
+  // vd, vj, si5
+  // V16QI, V16QI, QI
+  v16i8_r = __lsx_vseqi_b(v16i8_a, si5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vseqi.b(
+
+  // __lsx_vseqi_h
+  // vd, vj, si5
+  // V8HI, V8HI, QI
+  v8i16_r = __lsx_vseqi_h(v8i16_a, si5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vseqi.h(
+
+  // __lsx_vseqi_w
+  // vd, vj, si5
+  // V4SI, V4SI, QI
+  v4i32_r = __lsx_vseqi_w(v4i32_a, si5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vseqi.w(
+
+  // __lsx_vseqi_d
+  // vd, vj, si5
+  // V2DI, V2DI, QI
+  v2i64_r = __lsx_vseqi_d(v2i64_a, si5); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vseqi.d(
+
+  // __lsx_vslti_b
+  // vd, vj, si5
+  // V16QI, V16QI, QI
+  v16i8_r = __lsx_vslti_b(v16i8_a, si5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vslti.b(
+
+  // __lsx_vslt_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vslt_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vslt.b(
+
+  // __lsx_vslt_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vslt_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vslt.h(
+
+  // __lsx_vslt_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vslt_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vslt.w(
+
+  // __lsx_vslt_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vslt_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vslt.d(
+
+  // __lsx_vslti_h
+  // vd, vj, si5
+  // V8HI, V8HI, QI
+  v8i16_r = __lsx_vslti_h(v8i16_a, si5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vslti.h(
+
+  // __lsx_vslti_w
+  // vd, vj, si5
+  // V4SI, V4SI, QI
+  v4i32_r = __lsx_vslti_w(v4i32_a, si5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vslti.w(
+
+  // __lsx_vslti_d
+  // vd, vj, si5
+  // V2DI, V2DI, QI
+  v2i64_r = __lsx_vslti_d(v2i64_a, si5); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vslti.d(
+
+  // __lsx_vslt_bu
+  // vd, vj, vk
+  // V16QI, UV16QI, UV16QI
+  v16i8_r = __lsx_vslt_bu(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vslt.bu(
+
+  // __lsx_vslt_hu
+  // vd, vj, vk
+  // V8HI, UV8HI, UV8HI
+  v8i16_r = __lsx_vslt_hu(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vslt.hu(
+
+  // __lsx_vslt_wu
+  // vd, vj, vk
+  // V4SI, UV4SI, UV4SI
+  v4i32_r = __lsx_vslt_wu(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vslt.wu(
+
+  // __lsx_vslt_du
+  // vd, vj, vk
+  // V2DI, UV2DI, UV2DI
+  v2i64_r = __lsx_vslt_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vslt.du(
+
+  // __lsx_vslti_bu
+  // vd, vj, ui5
+  // V16QI, UV16QI, UQI
+  v16i8_r = __lsx_vslti_bu(v16u8_a, ui5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vslti.bu(
+
+  // __lsx_vslti_hu
+  // vd, vj, ui5
+  // V8HI, UV8HI, UQI
+  v8i16_r = __lsx_vslti_hu(v8u16_a, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vslti.hu(
+
+  // __lsx_vslti_wu
+  // vd, vj, ui5
+  // V4SI, UV4SI, UQI
+  v4i32_r = __lsx_vslti_wu(v4u32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vslti.wu(
+
+  // __lsx_vslti_du
+  // vd, vj, ui5
+  // V2DI, UV2DI, UQI
+  v2i64_r = __lsx_vslti_du(v2u64_a, ui5); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vslti.du(
+
+  // __lsx_vsle_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vsle_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsle.b(
+
+  // __lsx_vsle_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vsle_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsle.h(
+
+  // __lsx_vsle_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vsle_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsle.w(
+
+  // __lsx_vsle_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vsle_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsle.d(
+
+  // __lsx_vslei_b
+  // vd, vj, si5
+  // V16QI, V16QI, QI
+  v16i8_r = __lsx_vslei_b(v16i8_a, si5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vslei.b(
+
+  // __lsx_vslei_h
+  // vd, vj, si5
+  // V8HI, V8HI, QI
+  v8i16_r = __lsx_vslei_h(v8i16_a, si5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vslei.h(
+
+  // __lsx_vslei_w
+  // vd, vj, si5
+  // V4SI, V4SI, QI
+  v4i32_r = __lsx_vslei_w(v4i32_a, si5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vslei.w(
+
+  // __lsx_vslei_d
+  // vd, vj, si5
+  // V2DI, V2DI, QI
+  v2i64_r = __lsx_vslei_d(v2i64_a, si5); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vslei.d(
+
+  // __lsx_vsle_bu
+  // vd, vj, vk
+  // V16QI, UV16QI, UV16QI
+  v16i8_r = __lsx_vsle_bu(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsle.bu(
+
+  // __lsx_vsle_hu
+  // vd, vj, vk
+  // V8HI, UV8HI, UV8HI
+  v8i16_r = __lsx_vsle_hu(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsle.hu(
+
+  // __lsx_vsle_wu
+  // vd, vj, vk
+  // V4SI, UV4SI, UV4SI
+  v4i32_r = __lsx_vsle_wu(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsle.wu(
+
+  // __lsx_vsle_du
+  // vd, vj, vk
+  // V2DI, UV2DI, UV2DI
+  v2i64_r = __lsx_vsle_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsle.du(
+
+  // __lsx_vslei_bu
+  // vd, vj, ui5
+  // V16QI, UV16QI, UQI
+  v16i8_r = __lsx_vslei_bu(v16u8_a, ui5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vslei.bu(
+
+  // __lsx_vslei_hu
+  // vd, vj, ui5
+  // V8HI, UV8HI, UQI
+  v8i16_r = __lsx_vslei_hu(v8u16_a, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vslei.hu(
+
+  // __lsx_vslei_wu
+  // vd, vj, ui5
+  // V4SI, UV4SI, UQI
+  v4i32_r = __lsx_vslei_wu(v4u32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vslei.wu(
+
+  // __lsx_vslei_du
+  // vd, vj, ui5
+  // V2DI, UV2DI, UQI
+  v2i64_r = __lsx_vslei_du(v2u64_a, ui5); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vslei.du(
+
+  // __lsx_vsat_b
+  // vd, vj, ui3
+  // V16QI, V16QI, UQI
+  v16i8_r = __lsx_vsat_b(v16i8_a, ui3); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsat.b(
+
+  // __lsx_vsat_h
+  // vd, vj, ui4
+  // V8HI, V8HI, UQI
+  v8i16_r = __lsx_vsat_h(v8i16_a, ui4); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsat.h(
+
+  // __lsx_vsat_w
+  // vd, vj, ui5
+  // V4SI, V4SI, UQI
+  v4i32_r = __lsx_vsat_w(v4i32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsat.w(
+
+  // __lsx_vsat_d
+  // vd, vj, ui6
+  // V2DI, V2DI, UQI
+  v2i64_r = __lsx_vsat_d(v2i64_a, ui6); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsat.d(
+
+  // __lsx_vsat_bu
+  // vd, vj, ui3
+  // UV16QI, UV16QI, UQI
+  v16u8_r = __lsx_vsat_bu(v16u8_a, ui3); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsat.bu(
+
+  // __lsx_vsat_hu
+  // vd, vj, ui4
+  // UV8HI, UV8HI, UQI
+  v8u16_r = __lsx_vsat_hu(v8u16_a, ui4); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsat.hu(
+
+  // __lsx_vsat_wu
+  // vd, vj, ui5
+  // UV4SI, UV4SI, UQI
+  v4u32_r = __lsx_vsat_wu(v4u32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsat.wu(
+
+  // __lsx_vsat_du
+  // vd, vj, ui6
+  // UV2DI, UV2DI, UQI
+  v2u64_r = __lsx_vsat_du(v2u64_a, ui6); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsat.du(
+
+  // __lsx_vadda_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vadda_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vadda.b(
+
+  // __lsx_vadda_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vadda_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vadda.h(
+
+  // __lsx_vadda_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vadda_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vadda.w(
+
+  // __lsx_vadda_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vadda_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vadda.d(
+
+  // __lsx_vsadd_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vsadd_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsadd.b(
+
+  // __lsx_vsadd_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vsadd_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsadd.h(
+
+  // __lsx_vsadd_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vsadd_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsadd.w(
+
+  // __lsx_vsadd_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vsadd_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsadd.d(
+
+  // __lsx_vsadd_bu
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vsadd_bu(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsadd.bu(
+
+  // __lsx_vsadd_hu
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV8HI
+  v8u16_r = __lsx_vsadd_hu(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsadd.hu(
+
+  // __lsx_vsadd_wu
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV4SI
+  v4u32_r = __lsx_vsadd_wu(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsadd.wu(
+
+  // __lsx_vsadd_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vsadd_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsadd.du(
+
+  // __lsx_vavg_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vavg_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vavg.b(
+
+  // __lsx_vavg_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vavg_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vavg.h(
+
+  // __lsx_vavg_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vavg_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vavg.w(
+
+  // __lsx_vavg_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vavg_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vavg.d(
+
+  // __lsx_vavg_bu
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vavg_bu(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vavg.bu(
+
+  // __lsx_vavg_hu
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV8HI
+  v8u16_r = __lsx_vavg_hu(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vavg.hu(
+
+  // __lsx_vavg_wu
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV4SI
+  v4u32_r = __lsx_vavg_wu(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vavg.wu(
+
+  // __lsx_vavg_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vavg_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vavg.du(
+
+  // __lsx_vavgr_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vavgr_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vavgr.b(
+
+  // __lsx_vavgr_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vavgr_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vavgr.h(
+
+  // __lsx_vavgr_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vavgr_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vavgr.w(
+
+  // __lsx_vavgr_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vavgr_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vavgr.d(
+
+  // __lsx_vavgr_bu
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vavgr_bu(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vavgr.bu(
+
+  // __lsx_vavgr_hu
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV8HI
+  v8u16_r = __lsx_vavgr_hu(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vavgr.hu(
+
+  // __lsx_vavgr_wu
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV4SI
+  v4u32_r = __lsx_vavgr_wu(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vavgr.wu(
+
+  // __lsx_vavgr_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vavgr_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vavgr.du(
+
+  // __lsx_vssub_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vssub_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssub.b(
+
+  // __lsx_vssub_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vssub_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssub.h(
+
+  // __lsx_vssub_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vssub_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssub.w(
+
+  // __lsx_vssub_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vssub_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vssub.d(
+
+  // __lsx_vssub_bu
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vssub_bu(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssub.bu(
+
+  // __lsx_vssub_hu
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV8HI
+  v8u16_r = __lsx_vssub_hu(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssub.hu(
+
+  // __lsx_vssub_wu
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV4SI
+  v4u32_r = __lsx_vssub_wu(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssub.wu(
+
+  // __lsx_vssub_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vssub_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vssub.du(
+
+  // __lsx_vabsd_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vabsd_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vabsd.b(
+
+  // __lsx_vabsd_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vabsd_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vabsd.h(
+
+  // __lsx_vabsd_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vabsd_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vabsd.w(
+
+  // __lsx_vabsd_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vabsd_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vabsd.d(
+
+  // __lsx_vabsd_bu
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vabsd_bu(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vabsd.bu(
+
+  // __lsx_vabsd_hu
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV8HI
+  v8u16_r = __lsx_vabsd_hu(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vabsd.hu(
+
+  // __lsx_vabsd_wu
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV4SI
+  v4u32_r = __lsx_vabsd_wu(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vabsd.wu(
+
+  // __lsx_vabsd_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vabsd_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vabsd.du(
+
+  // __lsx_vmul_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vmul_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmul.b(
+
+  // __lsx_vmul_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vmul_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmul.h(
+
+  // __lsx_vmul_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vmul_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmul.w(
+
+  // __lsx_vmul_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vmul_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmul.d(
+
+  // __lsx_vmadd_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vmadd_b(v16i8_a, v16i8_b, v16i8_c); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmadd.b(
+
+  // __lsx_vmadd_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vmadd_h(v8i16_a, v8i16_b, v8i16_c); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmadd.h(
+
+  // __lsx_vmadd_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vmadd_w(v4i32_a, v4i32_b, v4i32_c); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmadd.w(
+
+  // __lsx_vmadd_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vmadd_d(v2i64_a, v2i64_b, v2i64_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmadd.d(
+
+  // __lsx_vmsub_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vmsub_b(v16i8_a, v16i8_b, v16i8_c); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmsub.b(
+
+  // __lsx_vmsub_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vmsub_h(v8i16_a, v8i16_b, v8i16_c); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmsub.h(
+
+  // __lsx_vmsub_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vmsub_w(v4i32_a, v4i32_b, v4i32_c); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmsub.w(
+
+  // __lsx_vmsub_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vmsub_d(v2i64_a, v2i64_b, v2i64_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmsub.d(
+
+  // __lsx_vdiv_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vdiv_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vdiv.b(
+
+  // __lsx_vdiv_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vdiv_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vdiv.h(
+
+  // __lsx_vdiv_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vdiv_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vdiv.w(
+
+  // __lsx_vdiv_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vdiv_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vdiv.d(
+
+  // __lsx_vdiv_bu
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vdiv_bu(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vdiv.bu(
+
+  // __lsx_vdiv_hu
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV8HI
+  v8u16_r = __lsx_vdiv_hu(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vdiv.hu(
+
+  // __lsx_vdiv_wu
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV4SI
+  v4u32_r = __lsx_vdiv_wu(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vdiv.wu(
+
+  // __lsx_vdiv_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vdiv_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vdiv.du(
+
+  // __lsx_vhaddw_h_b
+  // vd, vj, vk
+  // V8HI, V16QI, V16QI
+  v8i16_r = __lsx_vhaddw_h_b(v16i8_a, v16i8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vhaddw.h.b(
+
+  // __lsx_vhaddw_w_h
+  // vd, vj, vk
+  // V4SI, V8HI, V8HI
+  v4i32_r = __lsx_vhaddw_w_h(v8i16_a, v8i16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vhaddw.w.h(
+
+  // __lsx_vhaddw_d_w
+  // vd, vj, vk
+  // V2DI, V4SI, V4SI
+  v2i64_r = __lsx_vhaddw_d_w(v4i32_a, v4i32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vhaddw.d.w(
+
+  // __lsx_vhaddw_hu_bu
+  // vd, vj, vk
+  // UV8HI, UV16QI, UV16QI
+  v8u16_r = __lsx_vhaddw_hu_bu(v16u8_a, v16u8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vhaddw.hu.bu(
+
+  // __lsx_vhaddw_wu_hu
+  // vd, vj, vk
+  // UV4SI, UV8HI, UV8HI
+  v4u32_r = __lsx_vhaddw_wu_hu(v8u16_a, v8u16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vhaddw.wu.hu(
+
+  // __lsx_vhaddw_du_wu
+  // vd, vj, vk
+  // UV2DI, UV4SI, UV4SI
+  v2u64_r = __lsx_vhaddw_du_wu(v4u32_a, v4u32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vhaddw.du.wu(
+
+  // __lsx_vhsubw_h_b
+  // vd, vj, vk
+  // V8HI, V16QI, V16QI
+  v8i16_r = __lsx_vhsubw_h_b(v16i8_a, v16i8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vhsubw.h.b(
+
+  // __lsx_vhsubw_w_h
+  // vd, vj, vk
+  // V4SI, V8HI, V8HI
+  v4i32_r = __lsx_vhsubw_w_h(v8i16_a, v8i16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vhsubw.w.h(
+
+  // __lsx_vhsubw_d_w
+  // vd, vj, vk
+  // V2DI, V4SI, V4SI
+  v2i64_r = __lsx_vhsubw_d_w(v4i32_a, v4i32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vhsubw.d.w(
+
+  // __lsx_vhsubw_hu_bu
+  // vd, vj, vk
+  // V8HI, UV16QI, UV16QI
+  v8i16_r = __lsx_vhsubw_hu_bu(v16u8_a, v16u8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vhsubw.hu.bu(
+
+  // __lsx_vhsubw_wu_hu
+  // vd, vj, vk
+  // V4SI, UV8HI, UV8HI
+  v4i32_r = __lsx_vhsubw_wu_hu(v8u16_a, v8u16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vhsubw.wu.hu(
+
+  // __lsx_vhsubw_du_wu
+  // vd, vj, vk
+  // V2DI, UV4SI, UV4SI
+  v2i64_r = __lsx_vhsubw_du_wu(v4u32_a, v4u32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vhsubw.du.wu(
+
+  // __lsx_vmod_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vmod_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmod.b(
+
+  // __lsx_vmod_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vmod_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmod.h(
+
+  // __lsx_vmod_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vmod_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmod.w(
+
+  // __lsx_vmod_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vmod_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmod.d(
+
+  // __lsx_vmod_bu
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vmod_bu(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmod.bu(
+
+  // __lsx_vmod_hu
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV8HI
+  v8u16_r = __lsx_vmod_hu(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmod.hu(
+
+  // __lsx_vmod_wu
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV4SI
+  v4u32_r = __lsx_vmod_wu(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmod.wu(
+
+  // __lsx_vmod_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vmod_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmod.du(
+
+  // __lsx_vreplve_b
+  // vd, vj, rk
+  // V16QI, V16QI, SI
+  v16i8_r = __lsx_vreplve_b(v16i8_a, i32_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vreplve.b(
+
+  // __lsx_vreplve_h
+  // vd, vj, rk
+  // V8HI, V8HI, SI
+  v8i16_r = __lsx_vreplve_h(v8i16_a, i32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vreplve.h(
+
+  // __lsx_vreplve_w
+  // vd, vj, rk
+  // V4SI, V4SI, SI
+  v4i32_r = __lsx_vreplve_w(v4i32_a, i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vreplve.w(
+
+  // __lsx_vreplve_d
+  // vd, vj, rk
+  // V2DI, V2DI, SI
+  v2i64_r = __lsx_vreplve_d(v2i64_a, i32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vreplve.d(
+
+  // __lsx_vreplvei_b
+  // vd, vj, ui4
+  // V16QI, V16QI, UQI
+  v16i8_r = __lsx_vreplvei_b(v16i8_a, ui4); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vreplvei.b(
+
+  // __lsx_vreplvei_h
+  // vd, vj, ui3
+  // V8HI, V8HI, UQI
+  v8i16_r = __lsx_vreplvei_h(v8i16_a, ui3); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vreplvei.h(
+
+  // __lsx_vreplvei_w
+  // vd, vj, ui2
+  // V4SI, V4SI, UQI
+  v4i32_r = __lsx_vreplvei_w(v4i32_a, ui2); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vreplvei.w(
+
+  // __lsx_vreplvei_d
+  // vd, vj, ui1
+  // V2DI, V2DI, UQI
+  v2i64_r = __lsx_vreplvei_d(v2i64_a, ui1); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vreplvei.d(
+
+  // __lsx_vpickev_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vpickev_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vpickev.b(
+
+  // __lsx_vpickev_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vpickev_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vpickev.h(
+
+  // __lsx_vpickev_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vpickev_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vpickev.w(
+
+  // __lsx_vpickev_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vpickev_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vpickev.d(
+
+  // __lsx_vpickod_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vpickod_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vpickod.b(
+
+  // __lsx_vpickod_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vpickod_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vpickod.h(
+
+  // __lsx_vpickod_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vpickod_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vpickod.w(
+
+  // __lsx_vpickod_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vpickod_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vpickod.d(
+
+  // __lsx_vilvh_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vilvh_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vilvh.b(
+
+  // __lsx_vilvh_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vilvh_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vilvh.h(
+
+  // __lsx_vilvh_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vilvh_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vilvh.w(
+
+  // __lsx_vilvh_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vilvh_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vilvh.d(
+
+  // __lsx_vilvl_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vilvl_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vilvl.b(
+
+  // __lsx_vilvl_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vilvl_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vilvl.h(
+
+  // __lsx_vilvl_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vilvl_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vilvl.w(
+
+  // __lsx_vilvl_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vilvl_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vilvl.d(
+
+  // __lsx_vpackev_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vpackev_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vpackev.b(
+
+  // __lsx_vpackev_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vpackev_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vpackev.h(
+
+  // __lsx_vpackev_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vpackev_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vpackev.w(
+
+  // __lsx_vpackev_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vpackev_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vpackev.d(
+
+  // __lsx_vpackod_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vpackod_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vpackod.b(
+
+  // __lsx_vpackod_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vpackod_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vpackod.h(
+
+  // __lsx_vpackod_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vpackod_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vpackod.w(
+
+  // __lsx_vpackod_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vpackod_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vpackod.d(
+
+  // __lsx_vshuf_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vshuf_h(v8i16_a, v8i16_b, v8i16_c); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vshuf.h(
+
+  // __lsx_vshuf_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vshuf_w(v4i32_a, v4i32_b, v4i32_c); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vshuf.w(
+
+  // __lsx_vshuf_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vshuf_d(v2i64_a, v2i64_b, v2i64_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vshuf.d(
+
+  // __lsx_vand_v
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vand_v(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vand.v(
+
+  // __lsx_vandi_b
+  // vd, vj, ui8
+  // UV16QI, UV16QI, UQI
+  v16u8_r = __lsx_vandi_b(v16u8_a, ui8); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vandi.b(
+
+  // __lsx_vor_v
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vor_v(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vor.v(
+
+  // __lsx_vori_b
+  // vd, vj, ui8
+  // UV16QI, UV16QI, UQI
+  v16u8_r = __lsx_vori_b(v16u8_a, ui8); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vori.b(
+
+  // __lsx_vnor_v
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vnor_v(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vnor.v(
+
+  // __lsx_vnori_b
+  // vd, vj, ui8
+  // UV16QI, UV16QI, UQI
+  v16u8_r = __lsx_vnori_b(v16u8_a, ui8); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vnori.b(
+
+  // __lsx_vxor_v
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vxor_v(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vxor.v(
+
+  // __lsx_vxori_b
+  // vd, vj, ui8
+  // UV16QI, UV16QI, UQI
+  v16u8_r = __lsx_vxori_b(v16u8_a, ui8); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vxori.b(
+
+  // __lsx_vbitsel_v
+  // vd, vj, vk, va
+  // UV16QI, UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vbitsel_v(v16u8_a, v16u8_b, v16u8_c); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vbitsel.v(
+
+  // __lsx_vbitseli_b
+  // vd, vj, ui8
+  // UV16QI, UV16QI, UV16QI, UQI
+  v16u8_r = __lsx_vbitseli_b(v16u8_a, v16u8_b, ui8); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vbitseli.b(
+
+  // __lsx_vshuf4i_b
+  // vd, vj, ui8
+  // V16QI, V16QI, UQI
+  v16i8_r = __lsx_vshuf4i_b(v16i8_a, ui8); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vshuf4i.b(
+
+  // __lsx_vshuf4i_h
+  // vd, vj, ui8
+  // V8HI, V8HI, UQI
+  v8i16_r = __lsx_vshuf4i_h(v8i16_a, ui8); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vshuf4i.h(
+
+  // __lsx_vshuf4i_w
+  // vd, vj, ui8
+  // V4SI, V4SI, UQI
+  v4i32_r = __lsx_vshuf4i_w(v4i32_a, ui8); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vshuf4i.w(
+
+  // __lsx_vreplgr2vr_b
+  // vd, rj
+  // V16QI, SI
+  v16i8_r = __lsx_vreplgr2vr_b(i32_a); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vreplgr2vr.b(
+
+  // __lsx_vreplgr2vr_h
+  // vd, rj
+  // V8HI, SI
+  v8i16_r = __lsx_vreplgr2vr_h(i32_a); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vreplgr2vr.h(
+
+  // __lsx_vreplgr2vr_w
+  // vd, rj
+  // V4SI, SI
+  v4i32_r = __lsx_vreplgr2vr_w(i32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vreplgr2vr.w(
+
+  // __lsx_vreplgr2vr_d
+  // vd, rj
+  // V2DI, DI
+  v2i64_r = __lsx_vreplgr2vr_d(i64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vreplgr2vr.d(
+
+  // __lsx_vpcnt_b
+  // vd, vj
+  // V16QI, V16QI
+  v16i8_r = __lsx_vpcnt_b(v16i8_a); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vpcnt.b(
+
+  // __lsx_vpcnt_h
+  // vd, vj
+  // V8HI, V8HI
+  v8i16_r = __lsx_vpcnt_h(v8i16_a); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vpcnt.h(
+
+  // __lsx_vpcnt_w
+  // vd, vj
+  // V4SI, V4SI
+  v4i32_r = __lsx_vpcnt_w(v4i32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vpcnt.w(
+
+  // __lsx_vpcnt_d
+  // vd, vj
+  // V2DI, V2DI
+  v2i64_r = __lsx_vpcnt_d(v2i64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vpcnt.d(
+
+  // __lsx_vclo_b
+  // vd, vj
+  // V16QI, V16QI
+  v16i8_r = __lsx_vclo_b(v16i8_a); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vclo.b(
+
+  // __lsx_vclo_h
+  // vd, vj
+  // V8HI, V8HI
+  v8i16_r = __lsx_vclo_h(v8i16_a); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vclo.h(
+
+  // __lsx_vclo_w
+  // vd, vj
+  // V4SI, V4SI
+  v4i32_r = __lsx_vclo_w(v4i32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vclo.w(
+
+  // __lsx_vclo_d
+  // vd, vj
+  // V2DI, V2DI
+  v2i64_r = __lsx_vclo_d(v2i64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vclo.d(
+
+  // __lsx_vclz_b
+  // vd, vj
+  // V16QI, V16QI
+  v16i8_r = __lsx_vclz_b(v16i8_a); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vclz.b(
+
+  // __lsx_vclz_h
+  // vd, vj
+  // V8HI, V8HI
+  v8i16_r = __lsx_vclz_h(v8i16_a); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vclz.h(
+
+  // __lsx_vclz_w
+  // vd, vj
+  // V4SI, V4SI
+  v4i32_r = __lsx_vclz_w(v4i32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vclz.w(
+
+  // __lsx_vclz_d
+  // vd, vj
+  // V2DI, V2DI
+  v2i64_r = __lsx_vclz_d(v2i64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vclz.d(
+
+  // __lsx_vpickve2gr_b
+  // rd, vj, ui4
+  // SI, V16QI, UQI
+  i32_r = __lsx_vpickve2gr_b(v16i8_a, ui4); // CHECK: call i32 @llvm.loongarch.lsx.vpickve2gr.b(
+
+  // __lsx_vpickve2gr_h
+  // rd, vj, ui3
+  // SI, V8HI, UQI
+  i32_r = __lsx_vpickve2gr_h(v8i16_a, ui3); // CHECK: call i32 @llvm.loongarch.lsx.vpickve2gr.h(
+
+  // __lsx_vpickve2gr_w
+  // rd, vj, ui2
+  // SI, V4SI, UQI
+  i32_r = __lsx_vpickve2gr_w(v4i32_a, ui2); // CHECK: call i32 @llvm.loongarch.lsx.vpickve2gr.w(
+
+  // __lsx_vpickve2gr_d
+  // rd, vj, ui1
+  // DI, V2DI, UQI
+  i64_r = __lsx_vpickve2gr_d(v2i64_a, ui1); // CHECK: call i64 @llvm.loongarch.lsx.vpickve2gr.d(
+
+  // __lsx_vpickve2gr_bu
+  // rd, vj, ui4
+  // USI, V16QI, UQI
+  u32_r = __lsx_vpickve2gr_bu(v16i8_a, ui4); // CHECK: call i32 @llvm.loongarch.lsx.vpickve2gr.bu(
+
+  // __lsx_vpickve2gr_hu
+  // rd, vj, ui3
+  // USI, V8HI, UQI
+  u32_r = __lsx_vpickve2gr_hu(v8i16_a, ui3); // CHECK: call i32 @llvm.loongarch.lsx.vpickve2gr.hu(
+
+  // __lsx_vpickve2gr_wu
+  // rd, vj, ui2
+  // USI, V4SI, UQI
+  u32_r = __lsx_vpickve2gr_wu(v4i32_a, ui2); // CHECK: call i32 @llvm.loongarch.lsx.vpickve2gr.wu(
+
+  // __lsx_vpickve2gr_du
+  // rd, vj, ui1
+  // UDI, V2DI, UQI
+  u64_r = __lsx_vpickve2gr_du(v2i64_a, ui1); // CHECK: call i64 @llvm.loongarch.lsx.vpickve2gr.du(
+
+  // __lsx_vinsgr2vr_b
+  // vd, rj, ui4
+  // V16QI, V16QI, SI, UQI
+  v16i8_r = __lsx_vinsgr2vr_b(v16i8_a, i32_b, ui4); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vinsgr2vr.b(
+
+  // __lsx_vinsgr2vr_h
+  // vd, rj, ui3
+  // V8HI, V8HI, SI, UQI
+  v8i16_r = __lsx_vinsgr2vr_h(v8i16_a, i32_b, ui3); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vinsgr2vr.h(
+
+  // __lsx_vinsgr2vr_w
+  // vd, rj, ui2
+  // V4SI, V4SI, SI, UQI
+  v4i32_r = __lsx_vinsgr2vr_w(v4i32_a, i32_b, ui2); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vinsgr2vr.w(
+
+  // __lsx_vinsgr2vr_d
+  // vd, rj, ui1
+  // V2DI, V2DI, SI, UQI
+  v2i64_r = __lsx_vinsgr2vr_d(v2i64_a, i32_b, ui1); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vinsgr2vr.d(
+
+  // __lsx_vfcmp_caf_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_caf_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.caf.s(
+
+  // __lsx_vfcmp_caf_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_caf_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.caf.d(
+
+  // __lsx_vfcmp_cor_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_cor_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.cor.s(
+
+  // __lsx_vfcmp_cor_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_cor_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.cor.d(
+
+  // __lsx_vfcmp_cun_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_cun_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.cun.s(
+
+  // __lsx_vfcmp_cun_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_cun_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.cun.d(
+
+  // __lsx_vfcmp_cune_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_cune_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.cune.s(
+
+  // __lsx_vfcmp_cune_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_cune_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.cune.d(
+
+  // __lsx_vfcmp_cueq_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_cueq_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.cueq.s(
+
+  // __lsx_vfcmp_cueq_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_cueq_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.cueq.d(
+
+  // __lsx_vfcmp_ceq_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_ceq_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.ceq.s(
+
+  // __lsx_vfcmp_ceq_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_ceq_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.ceq.d(
+
+  // __lsx_vfcmp_cne_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_cne_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.cne.s(
+
+  // __lsx_vfcmp_cne_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_cne_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.cne.d(
+
+  // __lsx_vfcmp_clt_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_clt_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.clt.s(
+
+  // __lsx_vfcmp_clt_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_clt_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.clt.d(
+
+  // __lsx_vfcmp_cult_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_cult_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.cult.s(
+
+  // __lsx_vfcmp_cult_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_cult_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.cult.d(
+
+  // __lsx_vfcmp_cle_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_cle_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.cle.s(
+
+  // __lsx_vfcmp_cle_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_cle_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.cle.d(
+
+  // __lsx_vfcmp_cule_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_cule_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.cule.s(
+
+  // __lsx_vfcmp_cule_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_cule_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.cule.d(
+
+  // __lsx_vfcmp_saf_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_saf_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.saf.s(
+
+  // __lsx_vfcmp_saf_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_saf_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.saf.d(
+
+  // __lsx_vfcmp_sor_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_sor_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.sor.s(
+
+  // __lsx_vfcmp_sor_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_sor_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.sor.d(
+
+  // __lsx_vfcmp_sun_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_sun_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.sun.s(
+
+  // __lsx_vfcmp_sun_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_sun_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.sun.d(
+
+  // __lsx_vfcmp_sune_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_sune_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.sune.s(
+
+  // __lsx_vfcmp_sune_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_sune_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.sune.d(
+
+  // __lsx_vfcmp_sueq_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_sueq_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.sueq.s(
+
+  // __lsx_vfcmp_sueq_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_sueq_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.sueq.d(
+
+  // __lsx_vfcmp_seq_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_seq_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.seq.s(
+
+  // __lsx_vfcmp_seq_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_seq_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.seq.d(
+
+  // __lsx_vfcmp_sne_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_sne_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.sne.s(
+
+  // __lsx_vfcmp_sne_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_sne_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.sne.d(
+
+  // __lsx_vfcmp_slt_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_slt_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.slt.s(
+
+  // __lsx_vfcmp_slt_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_slt_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.slt.d(
+
+  // __lsx_vfcmp_sult_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_sult_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.sult.s(
+
+  // __lsx_vfcmp_sult_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_sult_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.sult.d(
+
+  // __lsx_vfcmp_sle_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_sle_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.sle.s(
+
+  // __lsx_vfcmp_sle_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_sle_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.sle.d(
+
+  // __lsx_vfcmp_sule_s
+  // vd, vj, vk
+  // V4SI, V4SF, V4SF
+  v4i32_r = __lsx_vfcmp_sule_s(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfcmp.sule.s(
+
+  // __lsx_vfcmp_sule_d
+  // vd, vj, vk
+  // V2DI, V2DF, V2DF
+  v2i64_r = __lsx_vfcmp_sule_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfcmp.sule.d(
+
+  // __lsx_vfadd_s
+  // vd, vj, vk
+  // V4SF, V4SF, V4SF
+  v4f32_r = __lsx_vfadd_s(v4f32_a, v4f32_b); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfadd.s(
+  // __lsx_vfadd_d
+  // vd, vj, vk
+  // V2DF, V2DF, V2DF
+  v2f64_r = __lsx_vfadd_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfadd.d(
+
+  // __lsx_vfsub_s
+  // vd, vj, vk
+  // V4SF, V4SF, V4SF
+  v4f32_r = __lsx_vfsub_s(v4f32_a, v4f32_b); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfsub.s(
+
+  // __lsx_vfsub_d
+  // vd, vj, vk
+  // V2DF, V2DF, V2DF
+  v2f64_r = __lsx_vfsub_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfsub.d(
+
+  // __lsx_vfmul_s
+  // vd, vj, vk
+  // V4SF, V4SF, V4SF
+  v4f32_r = __lsx_vfmul_s(v4f32_a, v4f32_b); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfmul.s(
+
+  // __lsx_vfmul_d
+  // vd, vj, vk
+  // V2DF, V2DF, V2DF
+  v2f64_r = __lsx_vfmul_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfmul.d(
+
+  // __lsx_vfdiv_s
+  // vd, vj, vk
+  // V4SF, V4SF, V4SF
+  v4f32_r = __lsx_vfdiv_s(v4f32_a, v4f32_b); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfdiv.s(
+
+  // __lsx_vfdiv_d
+  // vd, vj, vk
+  // V2DF, V2DF, V2DF
+  v2f64_r = __lsx_vfdiv_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfdiv.d(
+
+  // __lsx_vfcvt_h_s
+  // vd, vj, vk
+  // V8HI, V4SF, V4SF
+  v8i16_r = __lsx_vfcvt_h_s(v4f32_a, v4f32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vfcvt.h.s(
+
+  // __lsx_vfcvt_s_d
+  // vd, vj, vk
+  // V4SF, V2DF, V2DF
+  v4f32_r = __lsx_vfcvt_s_d(v2f64_a, v2f64_b); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfcvt.s.d(
+
+  // __lsx_vfmin_s
+  // vd, vj, vk
+  // V4SF, V4SF, V4SF
+  v4f32_r = __lsx_vfmin_s(v4f32_a, v4f32_b); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfmin.s(
+
+  // __lsx_vfmin_d
+  // vd, vj, vk
+  // V2DF, V2DF, V2DF
+  v2f64_r = __lsx_vfmin_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfmin.d(
+
+  // __lsx_vfmina_s
+  // vd, vj, vk
+  // V4SF, V4SF, V4SF
+  v4f32_r = __lsx_vfmina_s(v4f32_a, v4f32_b); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfmina.s(
+
+  // __lsx_vfmina_d
+  // vd, vj, vk
+  // V2DF, V2DF, V2DF
+  v2f64_r = __lsx_vfmina_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfmina.d(
+
+  // __lsx_vfmax_s
+  // vd, vj, vk
+  // V4SF, V4SF, V4SF
+  v4f32_r = __lsx_vfmax_s(v4f32_a, v4f32_b); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfmax.s(
+
+  // __lsx_vfmax_d
+  // vd, vj, vk
+  // V2DF, V2DF, V2DF
+  v2f64_r = __lsx_vfmax_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfmax.d(
+
+  // __lsx_vfmaxa_s
+  // vd, vj, vk
+  // V4SF, V4SF, V4SF
+  v4f32_r = __lsx_vfmaxa_s(v4f32_a, v4f32_b); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfmaxa.s(
+
+  // __lsx_vfmaxa_d
+  // vd, vj, vk
+  // V2DF, V2DF, V2DF
+  v2f64_r = __lsx_vfmaxa_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfmaxa.d(
+
+  // __lsx_vfclass_s
+  // vd, vj
+  // V4SI, V4SF
+  v4i32_r = __lsx_vfclass_s(v4f32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfclass.s(
+
+  // __lsx_vfclass_d
+  // vd, vj
+  // V2DI, V2DF
+  v2i64_r = __lsx_vfclass_d(v2f64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfclass.d(
+
+  // __lsx_vfsqrt_s
+  // vd, vj
+  // V4SF, V4SF
+  v4f32_r = __lsx_vfsqrt_s(v4f32_a); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfsqrt.s(
+
+  // __lsx_vfsqrt_d
+  // vd, vj
+  // V2DF, V2DF
+  v2f64_r = __lsx_vfsqrt_d(v2f64_a); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfsqrt.d(
+
+  // __lsx_vfrecip_s
+  // vd, vj
+  // V4SF, V4SF
+  v4f32_r = __lsx_vfrecip_s(v4f32_a); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfrecip.s(
+
+  // __lsx_vfrecip_d
+  // vd, vj
+  // V2DF, V2DF
+  v2f64_r = __lsx_vfrecip_d(v2f64_a); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfrecip.d(
+
+  // __lsx_vfrint_s
+  // vd, vj
+  // V4SF, V4SF
+  v4f32_r = __lsx_vfrint_s(v4f32_a); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfrint.s(
+
+  // __lsx_vfrint_d
+  // vd, vj
+  // V2DF, V2DF
+  v2f64_r = __lsx_vfrint_d(v2f64_a); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfrint.d(
+
+  // __lsx_vfrsqrt_s
+  // vd, vj
+  // V4SF, V4SF
+  v4f32_r = __lsx_vfrsqrt_s(v4f32_a); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfrsqrt.s(
+
+  // __lsx_vfrsqrt_d
+  // vd, vj
+  // V2DF, V2DF
+  v2f64_r = __lsx_vfrsqrt_d(v2f64_a); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfrsqrt.d(
+
+  // __lsx_vflogb_s
+  // vd, vj
+  // V4SF, V4SF
+  v4f32_r = __lsx_vflogb_s(v4f32_a); // CHECK: call <4 x float> @llvm.loongarch.lsx.vflogb.s(
+
+  // __lsx_vflogb_d
+  // vd, vj
+  // V2DF, V2DF
+  v2f64_r = __lsx_vflogb_d(v2f64_a); // CHECK: call <2 x double> @llvm.loongarch.lsx.vflogb.d(
+
+  // __lsx_vfcvth_s_h
+  // vd, vj
+  // V4SF, V8HI
+  v4f32_r = __lsx_vfcvth_s_h(v8i16_a); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfcvth.s.h(
+
+  // __lsx_vfcvth_d_s
+  // vd, vj
+  // V2DF, V4SF
+  v2f64_r = __lsx_vfcvth_d_s(v4f32_a); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfcvth.d.s(
+
+  //gcc build fail
+
+  // __lsx_vfcvtl_s_h
+  // vd, vj
+  // V4SF, V8HI
+  v4f32_r = __lsx_vfcvtl_s_h(v8i16_a); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfcvtl.s.h(
+
+  // __lsx_vfcvtl_d_s
+  // vd, vj
+  // V2DF, V4SF
+  v2f64_r = __lsx_vfcvtl_d_s(v4f32_a); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfcvtl.d.s(
+
+  // __lsx_vftint_w_s
+  // vd, vj
+  // V4SI, V4SF
+  v4i32_r = __lsx_vftint_w_s(v4f32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vftint.w.s(
+
+  // __lsx_vftint_l_d
+  // vd, vj
+  // V2DI, V2DF
+  v2i64_r = __lsx_vftint_l_d(v2f64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftint.l.d(
+
+  // __lsx_vftint_wu_s
+  // vd, vj
+  // UV4SI, V4SF
+  v4u32_r = __lsx_vftint_wu_s(v4f32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vftint.wu.s(
+
+  // __lsx_vftint_lu_d
+  // vd, vj
+  // UV2DI, V2DF
+  v2u64_r = __lsx_vftint_lu_d(v2f64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftint.lu.d(
+
+  // __lsx_vftintrz_w_s
+  // vd, vj
+  // V4SI, V4SF
+  v4i32_r = __lsx_vftintrz_w_s(v4f32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vftintrz.w.s(
+
+  // __lsx_vftintrz_l_d
+  // vd, vj
+  // V2DI, V2DF
+  v2i64_r = __lsx_vftintrz_l_d(v2f64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintrz.l.d(
+
+  // __lsx_vftintrz_wu_s
+  // vd, vj
+  // UV4SI, V4SF
+  v4u32_r = __lsx_vftintrz_wu_s(v4f32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vftintrz.wu.s(
+
+  // __lsx_vftintrz_lu_d
+  // vd, vj
+  // UV2DI, V2DF
+  v2u64_r = __lsx_vftintrz_lu_d(v2f64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintrz.lu.d(
+
+  // __lsx_vffint_s_w
+  // vd, vj
+  // V4SF, V4SI
+  v4f32_r = __lsx_vffint_s_w(v4i32_a); // CHECK: call <4 x float> @llvm.loongarch.lsx.vffint.s.w(
+
+  // __lsx_vffint_d_l
+  // vd, vj
+  // V2DF, V2DI
+  v2f64_r = __lsx_vffint_d_l(v2i64_a); // CHECK: call <2 x double> @llvm.loongarch.lsx.vffint.d.l(
+
+  // __lsx_vffint_s_wu
+  // vd, vj
+  // V4SF, UV4SI
+  v4f32_r = __lsx_vffint_s_wu(v4u32_a); // CHECK: call <4 x float> @llvm.loongarch.lsx.vffint.s.wu(
+
+  // __lsx_vffint_d_lu
+  // vd, vj
+  // V2DF, UV2DI
+  v2f64_r = __lsx_vffint_d_lu(v2u64_a); // CHECK: call <2 x double> @llvm.loongarch.lsx.vffint.d.lu(
+
+  // __lsx_vandn_v
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vandn_v(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vandn.v(
+
+  // __lsx_vneg_b
+  // vd, vj
+  // V16QI, V16QI
+  v16i8_r = __lsx_vneg_b(v16i8_a); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vneg.b(
+
+  // __lsx_vneg_h
+  // vd, vj
+  // V8HI, V8HI
+  v8i16_r = __lsx_vneg_h(v8i16_a); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vneg.h(
+
+  // __lsx_vneg_w
+  // vd, vj
+  // V4SI, V4SI
+  v4i32_r = __lsx_vneg_w(v4i32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vneg.w(
+
+  // __lsx_vneg_d
+  // vd, vj
+  // V2DI, V2DI
+  v2i64_r = __lsx_vneg_d(v2i64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vneg.d(
+
+  // __lsx_vmuh_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vmuh_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmuh.b(
+
+  // __lsx_vmuh_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vmuh_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmuh.h(
+
+  // __lsx_vmuh_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vmuh_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmuh.w(
+
+  // __lsx_vmuh_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vmuh_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmuh.d(
+
+  // __lsx_vmuh_bu
+  // vd, vj, vk
+  // UV16QI, UV16QI, UV16QI
+  v16u8_r = __lsx_vmuh_bu(v16u8_a, v16u8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmuh.bu(
+
+  // __lsx_vmuh_hu
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV8HI
+  v8u16_r = __lsx_vmuh_hu(v8u16_a, v8u16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmuh.hu(
+
+  // __lsx_vmuh_wu
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV4SI
+  v4u32_r = __lsx_vmuh_wu(v4u32_a, v4u32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmuh.wu(
+
+  // __lsx_vmuh_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vmuh_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmuh.du(
+
+  // __lsx_vsllwil_h_b
+  // vd, vj, ui3
+  // V8HI, V16QI, UQI
+  v8i16_r = __lsx_vsllwil_h_b(v16i8_a, ui3); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsllwil.h.b(
+
+  // __lsx_vsllwil_w_h
+  // vd, vj, ui4
+  // V4SI, V8HI, UQI
+  v4i32_r = __lsx_vsllwil_w_h(v8i16_a, ui4); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsllwil.w.h(
+
+  // __lsx_vsllwil_d_w
+  // vd, vj, ui5
+  // V2DI, V4SI, UQI
+  v2i64_r = __lsx_vsllwil_d_w(v4i32_a, ui5); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsllwil.d.w(
+
+  // __lsx_vsllwil_hu_bu
+  // vd, vj, ui3
+  // UV8HI, UV16QI, UQI
+  v8u16_r = __lsx_vsllwil_hu_bu(v16u8_a, ui3); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsllwil.hu.bu(
+
+  // __lsx_vsllwil_wu_hu
+  // vd, vj, ui4
+  // UV4SI, UV8HI, UQI
+  v4u32_r = __lsx_vsllwil_wu_hu(v8u16_a, ui4); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsllwil.wu.hu(
+
+  // __lsx_vsllwil_du_wu
+  // vd, vj, ui5
+  // UV2DI, UV4SI, UQI
+  v2u64_r = __lsx_vsllwil_du_wu(v4u32_a, ui5); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsllwil.du.wu(
+
+  // __lsx_vsran_b_h
+  // vd, vj, vk
+  // V16QI, V8HI, V8HI
+  v16i8_r = __lsx_vsran_b_h(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsran.b.h(
+
+  // __lsx_vsran_h_w
+  // vd, vj, vk
+  // V8HI, V4SI, V4SI
+  v8i16_r = __lsx_vsran_h_w(v4i32_a, v4i32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsran.h.w(
+
+  // __lsx_vsran_w_d
+  // vd, vj, vk
+  // V4SI, V2DI, V2DI
+  v4i32_r = __lsx_vsran_w_d(v2i64_a, v2i64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsran.w.d(
+
+  // __lsx_vssran_b_h
+  // vd, vj, vk
+  // V16QI, V8HI, V8HI
+  v16i8_r = __lsx_vssran_b_h(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssran.b.h(
+
+  // __lsx_vssran_h_w
+  // vd, vj, vk
+  // V8HI, V4SI, V4SI
+  v8i16_r = __lsx_vssran_h_w(v4i32_a, v4i32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssran.h.w(
+
+  // __lsx_vssran_w_d
+  // vd, vj, vk
+  // V4SI, V2DI, V2DI
+  v4i32_r = __lsx_vssran_w_d(v2i64_a, v2i64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssran.w.d(
+
+  // __lsx_vssran_bu_h
+  // vd, vj, vk
+  // UV16QI, UV8HI, UV8HI
+  v16u8_r = __lsx_vssran_bu_h(v8u16_a, v8u16_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssran.bu.h(
+
+  // __lsx_vssran_hu_w
+  // vd, vj, vk
+  // UV8HI, UV4SI, UV4SI
+  v8u16_r = __lsx_vssran_hu_w(v4u32_a, v4u32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssran.hu.w(
+
+  // __lsx_vssran_wu_d
+  // vd, vj, vk
+  // UV4SI, UV2DI, UV2DI
+  v4u32_r = __lsx_vssran_wu_d(v2u64_a, v2u64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssran.wu.d(
+
+  // __lsx_vsrarn_b_h
+  // vd, vj, vk
+  // V16QI, V8HI, V8HI
+  v16i8_r = __lsx_vsrarn_b_h(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsrarn.b.h(
+
+  // __lsx_vsrarn_h_w
+  // vd, vj, vk
+  // V8HI, V4SI, V4SI
+  v8i16_r = __lsx_vsrarn_h_w(v4i32_a, v4i32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsrarn.h.w(
+
+  // __lsx_vsrarn_w_d
+  // vd, vj, vk
+  // V4SI, V2DI, V2DI
+  v4i32_r = __lsx_vsrarn_w_d(v2i64_a, v2i64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsrarn.w.d(
+
+  // __lsx_vssrarn_b_h
+  // vd, vj, vk
+  // V16QI, V8HI, V8HI
+  v16i8_r = __lsx_vssrarn_b_h(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrarn.b.h(
+
+  // __lsx_vssrarn_h_w
+  // vd, vj, vk
+  // V8HI, V4SI, V4SI
+  v8i16_r = __lsx_vssrarn_h_w(v4i32_a, v4i32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrarn.h.w(
+
+  // __lsx_vssrarn_w_d
+  // vd, vj, vk
+  // V4SI, V2DI, V2DI
+  v4i32_r = __lsx_vssrarn_w_d(v2i64_a, v2i64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrarn.w.d(
+
+  // __lsx_vssrarn_bu_h
+  // vd, vj, vk
+  // UV16QI, UV8HI, UV8HI
+  v16u8_r = __lsx_vssrarn_bu_h(v8u16_a, v8u16_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrarn.bu.h(
+
+  // __lsx_vssrarn_hu_w
+  // vd, vj, vk
+  // UV8HI, UV4SI, UV4SI
+  v8u16_r = __lsx_vssrarn_hu_w(v4u32_a, v4u32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrarn.hu.w(
+
+  // __lsx_vssrarn_wu_d
+  // vd, vj, vk
+  // UV4SI, UV2DI, UV2DI
+  v4u32_r = __lsx_vssrarn_wu_d(v2u64_a, v2u64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrarn.wu.d(
+
+  // __lsx_vsrln_b_h
+  // vd, vj, vk
+  // V16QI, V8HI, V8HI
+  v16i8_r = __lsx_vsrln_b_h(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsrln.b.h(
+
+  // __lsx_vsrln_h_w
+  // vd, vj, vk
+  // V8HI, V4SI, V4SI
+  v8i16_r = __lsx_vsrln_h_w(v4i32_a, v4i32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsrln.h.w(
+
+  // __lsx_vsrln_w_d
+  // vd, vj, vk
+  // V4SI, V2DI, V2DI
+  v4i32_r = __lsx_vsrln_w_d(v2i64_a, v2i64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsrln.w.d(
+
+  // __lsx_vssrln_bu_h
+  // vd, vj, vk
+  // UV16QI, UV8HI, UV8HI
+  v16u8_r = __lsx_vssrln_bu_h(v8u16_a, v8u16_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrln.bu.h(
+
+  // __lsx_vssrln_hu_w
+  // vd, vj, vk
+  // UV8HI, UV4SI, UV4SI
+  v8u16_r = __lsx_vssrln_hu_w(v4u32_a, v4u32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrln.hu.w(
+
+  // __lsx_vssrln_wu_d
+  // vd, vj, vk
+  // UV4SI, UV2DI, UV2DI
+  v4u32_r = __lsx_vssrln_wu_d(v2u64_a, v2u64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrln.wu.d(
+
+  // __lsx_vsrlrn_b_h
+  // vd, vj, vk
+  // V16QI, V8HI, V8HI
+  v16i8_r = __lsx_vsrlrn_b_h(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsrlrn.b.h(
+
+  // __lsx_vsrlrn_h_w
+  // vd, vj, vk
+  // V8HI, V4SI, V4SI
+  v8i16_r = __lsx_vsrlrn_h_w(v4i32_a, v4i32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsrlrn.h.w(
+
+  // __lsx_vsrlrn_w_d
+  // vd, vj, vk
+  // V4SI, V2DI, V2DI
+  v4i32_r = __lsx_vsrlrn_w_d(v2i64_a, v2i64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsrlrn.w.d(
+
+  // __lsx_vssrlrn_bu_h
+  // vd, vj, vk
+  // UV16QI, UV8HI, UV8HI
+  v16u8_r = __lsx_vssrlrn_bu_h(v8u16_a, v8u16_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrlrn.bu.h(
+
+  // __lsx_vssrlrn_hu_w
+  // vd, vj, vk
+  // UV8HI, UV4SI, UV4SI
+  v8u16_r = __lsx_vssrlrn_hu_w(v4u32_a, v4u32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrlrn.hu.w(
+
+  // __lsx_vssrlrn_wu_d
+  // vd, vj, vk
+  // UV4SI, UV2DI, UV2DI
+  v4u32_r = __lsx_vssrlrn_wu_d(v2u64_a, v2u64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrlrn.wu.d(
+
+  // __lsx_vfrstpi_b
+  // vd, vj, ui5
+  // V16QI, V16QI, V16QI, UQI
+  v16i8_r = __lsx_vfrstpi_b(v16i8_a, v16i8_b, ui5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vfrstpi.b(
+
+  // __lsx_vfrstpi_h
+  // vd, vj, ui5
+  // V8HI, V8HI, V8HI, UQI
+  v8i16_r = __lsx_vfrstpi_h(v8i16_a, v8i16_b, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vfrstpi.h(
+
+  // __lsx_vfrstp_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vfrstp_b(v16i8_a, v16i8_b, v16i8_c); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vfrstp.b(
+
+  // __lsx_vfrstp_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vfrstp_h(v8i16_a, v8i16_b, v8i16_c); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vfrstp.h(
+
+  // __lsx_vshuf4i_d
+  // vd, vj, ui8
+  // V2DI, V2DI, V2DI, USI
+  v2i64_r = __lsx_vshuf4i_d(v2i64_a, v2i64_b, ui8); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vshuf4i.d(
+
+  // __lsx_vbsrl_v
+  // vd, vj, ui5
+  // V16QI, V16QI, UQI
+  v16i8_r = __lsx_vbsrl_v(v16i8_a, ui5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vbsrl.v(
+
+  // __lsx_vbsll_v
+  // vd, vj, ui5
+  // V16QI, V16QI, UQI
+  v16i8_r = __lsx_vbsll_v(v16i8_a, ui5); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vbsll.v(
+
+  // __lsx_vextrins_b
+  // vd, vj, ui8
+  // V16QI, V16QI, V16QI, UQI
+  v16i8_r = __lsx_vextrins_b(v16i8_a, v16i8_b, ui8); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vextrins.b(
+
+  // __lsx_vextrins_h
+  // vd, vj, ui8
+  // V8HI, V8HI, V8HI, UQI
+  v8i16_r = __lsx_vextrins_h(v8i16_a, v8i16_b, ui8); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vextrins.h(
+
+  // __lsx_vextrins_w
+  // vd, vj, ui8
+  // V4SI, V4SI, V4SI, UQI
+  v4i32_r = __lsx_vextrins_w(v4i32_a, v4i32_b, ui8); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vextrins.w(
+
+  // __lsx_vextrins_d
+  // vd, vj, ui8
+  // V2DI, V2DI, V2DI, UQI
+  v2i64_r = __lsx_vextrins_d(v2i64_a, v2i64_b, ui8); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vextrins.d(
+
+  // __lsx_vmskltz_b
+  // vd, vj
+  // V16QI, V16QI
+  v16i8_r = __lsx_vmskltz_b(v16i8_a); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmskltz.b(
+
+  // __lsx_vmskltz_h
+  // vd, vj
+  // V8HI, V8HI
+  v8i16_r = __lsx_vmskltz_h(v8i16_a); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmskltz.h(
+
+  // __lsx_vmskltz_w
+  // vd, vj
+  // V4SI, V4SI
+  v4i32_r = __lsx_vmskltz_w(v4i32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmskltz.w(
+
+  // __lsx_vmskltz_d
+  // vd, vj
+  // V2DI, V2DI
+  v2i64_r = __lsx_vmskltz_d(v2i64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmskltz.d(
+
+  // __lsx_vsigncov_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vsigncov_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsigncov.b(
+
+  // __lsx_vsigncov_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vsigncov_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsigncov.h(
+
+  // __lsx_vsigncov_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vsigncov_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsigncov.w(
+
+  // __lsx_vsigncov_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vsigncov_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsigncov.d(
+
+  // __lsx_vfmadd_s
+  // vd, vj, vk, va
+  // V4SF, V4SF, V4SF, V4SF
+  v4f32_r = __lsx_vfmadd_s(v4f32_a, v4f32_b, v4f32_c); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfmadd.s(
+
+  // __lsx_vfmadd_d
+  // vd, vj, vk, va
+  // V2DF, V2DF, V2DF, V2DF
+  v2f64_r = __lsx_vfmadd_d(v2f64_a, v2f64_b, v2f64_c); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfmadd.d(
+
+  // __lsx_vfmsub_s
+  // vd, vj, vk, va
+  // V4SF, V4SF, V4SF, V4SF
+  v4f32_r = __lsx_vfmsub_s(v4f32_a, v4f32_b, v4f32_c); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfmsub.s(
+
+  // __lsx_vfmsub_d
+  // vd, vj, vk, va
+  // V2DF, V2DF, V2DF, V2DF
+  v2f64_r = __lsx_vfmsub_d(v2f64_a, v2f64_b, v2f64_c); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfmsub.d(
+
+  // __lsx_vfnmadd_s
+  // vd, vj, vk, va
+  // V4SF, V4SF, V4SF, V4SF
+  v4f32_r = __lsx_vfnmadd_s(v4f32_a, v4f32_b, v4f32_c); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfnmadd.s(
+
+  // __lsx_vfnmadd_d
+  // vd, vj, vk, va
+  // V2DF, V2DF, V2DF, V2DF
+  v2f64_r = __lsx_vfnmadd_d(v2f64_a, v2f64_b, v2f64_c); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfnmadd.d(
+
+  // __lsx_vfnmsub_s
+  // vd, vj, vk, va
+  // V4SF, V4SF, V4SF, V4SF
+  v4f32_r = __lsx_vfnmsub_s(v4f32_a, v4f32_b, v4f32_c); // CHECK: call <4 x float> @llvm.loongarch.lsx.vfnmsub.s(
+
+  // __lsx_vfnmsub_d
+  // vd, vj, vk, va
+  // V2DF, V2DF, V2DF, V2DF
+  v2f64_r = __lsx_vfnmsub_d(v2f64_a, v2f64_b, v2f64_c); // CHECK: call <2 x double> @llvm.loongarch.lsx.vfnmsub.d(
+
+  // __lsx_vftintrne_w_s
+  // vd, vj
+  // V4SI, V4SF
+  v4i32_r = __lsx_vftintrne_w_s(v4f32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vftintrne.w.s(
+
+  // __lsx_vftintrne_l_d
+  // vd, vj
+  // V2DI, V2DF
+  v2i64_r = __lsx_vftintrne_l_d(v2f64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintrne.l.d(
+
+  // __lsx_vftintrp_w_s
+  // vd, vj
+  // V4SI, V4SF
+  v4i32_r = __lsx_vftintrp_w_s(v4f32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vftintrp.w.s(
+
+  // __lsx_vftintrp_l_d
+  // vd, vj
+  // V2DI, V2DF
+  v2i64_r = __lsx_vftintrp_l_d(v2f64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintrp.l.d(
+
+  // __lsx_vftintrm_w_s
+  // vd, vj
+  // V4SI, V4SF
+  v4i32_r = __lsx_vftintrm_w_s(v4f32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vftintrm.w.s(
+
+  // __lsx_vftintrm_l_d
+  // vd, vj
+  // V2DI, V2DF
+  v2i64_r = __lsx_vftintrm_l_d(v2f64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintrm.l.d(
+
+  // __lsx_vftint_w_d
+  // vd, vj, vk
+  // V4SI, V2DF, V2DF
+  v4i32_r = __lsx_vftint_w_d(v2f64_a, v2f64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vftint.w.d(
+
+  // __lsx_vffint_s_l
+  // vd, vj, vk
+  // V4SF, V2DI, V2DI
+  v4f32_r = __lsx_vffint_s_l(v2i64_a, v2i64_b); // CHECK: call <4 x float> @llvm.loongarch.lsx.vffint.s.l(
+
+  // __lsx_vftintrz_w_d
+  // vd, vj, vk
+  // V4SI, V2DF, V2DF
+  v4i32_r = __lsx_vftintrz_w_d(v2f64_a, v2f64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vftintrz.w.d(
+
+  // __lsx_vftintrp_w_d
+  // vd, vj, vk
+  // V4SI, V2DF, V2DF
+  v4i32_r = __lsx_vftintrp_w_d(v2f64_a, v2f64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vftintrp.w.d(
+
+  // __lsx_vftintrm_w_d
+  // vd, vj, vk
+  // V4SI, V2DF, V2DF
+  v4i32_r = __lsx_vftintrm_w_d(v2f64_a, v2f64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vftintrm.w.d(
+
+  // __lsx_vftintrne_w_d
+  // vd, vj, vk
+  // V4SI, V2DF, V2DF
+  v4i32_r = __lsx_vftintrne_w_d(v2f64_a, v2f64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vftintrne.w.d(
+
+  // __lsx_vftintl_l_s
+  // vd, vj
+  // V2DI, V4SF
+  v2i64_r = __lsx_vftintl_l_s(v4f32_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintl.l.s(
+
+  // __lsx_vftinth_l_s
+  // vd, vj
+  // V2DI, V4SF
+  v2i64_r = __lsx_vftinth_l_s(v4f32_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftinth.l.s(
+
+  // __lsx_vffinth_d_w
+  // vd, vj
+  // V2DF, V4SI
+  v2f64_r = __lsx_vffinth_d_w(v4i32_a); // CHECK: call <2 x double> @llvm.loongarch.lsx.vffinth.d.w(
+
+  // __lsx_vffintl_d_w
+  // vd, vj
+  // V2DF, V4SI
+  v2f64_r = __lsx_vffintl_d_w(v4i32_a); // CHECK: call <2 x double> @llvm.loongarch.lsx.vffintl.d.w(
+
+  // __lsx_vftintrzl_l_s
+  // vd, vj
+  // V2DI, V4SF
+  v2i64_r = __lsx_vftintrzl_l_s(v4f32_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintrzl.l.s(
+
+  // __lsx_vftintrzh_l_s
+  // vd, vj
+  // V2DI, V4SF
+  v2i64_r = __lsx_vftintrzh_l_s(v4f32_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintrzh.l.s(
+
+  // __lsx_vftintrpl_l_s
+  // vd, vj
+  // V2DI, V4SF
+  v2i64_r = __lsx_vftintrpl_l_s(v4f32_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintrpl.l.s(
+
+  // __lsx_vftintrph_l_s
+  // vd, vj
+  // V2DI, V4SF
+  v2i64_r = __lsx_vftintrph_l_s(v4f32_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintrph.l.s(
+
+  // __lsx_vftintrml_l_s
+  // vd, vj
+  // V2DI, V4SF
+  v2i64_r = __lsx_vftintrml_l_s(v4f32_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintrml.l.s(
+
+  // __lsx_vftintrmh_l_s
+  // vd, vj
+  // V2DI, V4SF
+  v2i64_r = __lsx_vftintrmh_l_s(v4f32_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintrmh.l.s(
+
+  // __lsx_vftintrnel_l_s
+  // vd, vj
+  // V2DI, V4SF
+  v2i64_r = __lsx_vftintrnel_l_s(v4f32_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintrnel.l.s(
+
+  // __lsx_vftintrneh_l_s
+  // vd, vj
+  // V2DI, V4SF
+  v2i64_r = __lsx_vftintrneh_l_s(v4f32_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vftintrneh.l.s(
+
+  // __lsx_vfrintrne_s
+  // vd, vj
+  // V4SI, V4SF
+  v4i32_r = __lsx_vfrintrne_s(v4f32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfrintrne.s(
+
+  // __lsx_vfrintrne_d
+  // vd, vj
+  // V2DI, V2DF
+  v2i64_r = __lsx_vfrintrne_d(v2f64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfrintrne.d(
+
+  // __lsx_vfrintrz_s
+  // vd, vj
+  // V4SI, V4SF
+  v4i32_r = __lsx_vfrintrz_s(v4f32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfrintrz.s(
+
+  // __lsx_vfrintrz_d
+  // vd, vj
+  // V2DI, V2DF
+  v2i64_r = __lsx_vfrintrz_d(v2f64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfrintrz.d(
+
+  // __lsx_vfrintrp_s
+  // vd, vj
+  // V4SI, V4SF
+  v4i32_r = __lsx_vfrintrp_s(v4f32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfrintrp.s(
+
+  // __lsx_vfrintrp_d
+  // vd, vj
+  // V2DI, V2DF
+  v2i64_r = __lsx_vfrintrp_d(v2f64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfrintrp.d(
+
+  // __lsx_vfrintrm_s
+  // vd, vj
+  // V4SI, V4SF
+  v4i32_r = __lsx_vfrintrm_s(v4f32_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vfrintrm.s(
+
+  // __lsx_vfrintrm_d
+  // vd, vj
+  // V2DI, V2DF
+  v2i64_r = __lsx_vfrintrm_d(v2f64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vfrintrm.d(
+
+  // __lsx_vstelm_b
+  // vd, rj, si8, idx
+  // VOID, V16QI, CVPOINTER, SI, UQI
+  __lsx_vstelm_b(v16i8_a, &v16i8_b, 0, idx4); // CHECK: call void @llvm.loongarch.lsx.vstelm.b(
+  // __lsx_vstelm_h
+  // vd, rj, si8, idx
+  // VOID, V8HI, CVPOINTER, SI, UQI
+  __lsx_vstelm_h(v8i16_a, &v8i16_b, 0, idx3); // CHECK: call void @llvm.loongarch.lsx.vstelm.h(
+
+  // __lsx_vstelm_w
+  // vd, rj, si8, idx
+  // VOID, V4SI, CVPOINTER, SI, UQI
+  __lsx_vstelm_w(v4i32_a, &v4i32_b, 0, idx2); // CHECK: call void @llvm.loongarch.lsx.vstelm.w(
+
+  // __lsx_vstelm_d
+  // vd, rj, si8, idx
+  // VOID, V2DI, CVPOINTER, SI, UQI
+  __lsx_vstelm_d(v2i64_a, &v2i64_b, 0, idx1); // CHECK: call void @llvm.loongarch.lsx.vstelm.d(
+
+  // __lsx_vaddwev_d_w
+  // vd, vj, vk
+  // V2DI, V4SI, V4SI
+  v2i64_r = __lsx_vaddwev_d_w(v4i32_a, v4i32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vaddwev.d.w(
+
+  // __lsx_vaddwev_w_h
+  // vd, vj, vk
+  // V4SI, V8HI, V8HI
+  v4i32_r = __lsx_vaddwev_w_h(v8i16_a, v8i16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vaddwev.w.h(
+
+  // __lsx_vaddwev_h_b
+  // vd, vj, vk
+  // V8HI, V16QI, V16QI
+  v8i16_r = __lsx_vaddwev_h_b(v16i8_a, v16i8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vaddwev.h.b(
+
+  // __lsx_vaddwod_d_w
+  // vd, vj, vk
+  // V2DI, V4SI, V4SI
+  v2i64_r = __lsx_vaddwod_d_w(v4i32_a, v4i32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vaddwod.d.w(
+
+  // __lsx_vaddwod_w_h
+  // vd, vj, vk
+  // V4SI, V8HI, V8HI
+  v4i32_r = __lsx_vaddwod_w_h(v8i16_a, v8i16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vaddwod.w.h(
+
+  // __lsx_vaddwod_h_b
+  // vd, vj, vk
+  // V8HI, V16QI, V16QI
+  v8i16_r = __lsx_vaddwod_h_b(v16i8_a, v16i8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vaddwod.h.b(
+
+  // __lsx_vaddwev_d_wu
+  // vd, vj, vk
+  // V2DI, UV4SI, UV4SI
+  v2i64_r = __lsx_vaddwev_d_wu(v4u32_a, v4u32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vaddwev.d.wu(
+
+  // __lsx_vaddwev_w_hu
+  // vd, vj, vk
+  // V4SI, UV8HI, UV8HI
+  v4i32_r = __lsx_vaddwev_w_hu(v8u16_a, v8u16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vaddwev.w.hu(
+
+  // __lsx_vaddwev_h_bu
+  // vd, vj, vk
+  // V8HI, UV16QI, UV16QI
+  v8i16_r = __lsx_vaddwev_h_bu(v16u8_a, v16u8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vaddwev.h.bu(
+
+  // __lsx_vaddwod_d_wu
+  // vd, vj, vk
+  // V2DI, UV4SI, UV4SI
+  v2i64_r = __lsx_vaddwod_d_wu(v4u32_a, v4u32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vaddwod.d.wu(
+
+  // __lsx_vaddwod_w_hu
+  // vd, vj, vk
+  // V4SI, UV8HI, UV8HI
+  v4i32_r = __lsx_vaddwod_w_hu(v8u16_a, v8u16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vaddwod.w.hu(
+
+  // __lsx_vaddwod_h_bu
+  // vd, vj, vk
+  // V8HI, UV16QI, UV16QI
+  v8i16_r = __lsx_vaddwod_h_bu(v16u8_a, v16u8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vaddwod.h.bu(
+
+  // __lsx_vaddwev_d_wu_w
+  // vd, vj, vk
+  // V2DI, UV4SI, V4SI
+  v2i64_r = __lsx_vaddwev_d_wu_w(v4u32_a, v4i32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vaddwev.d.wu.w(
+
+  // __lsx_vaddwev_w_hu_h
+  // vd, vj, vk
+  // V4SI, UV8HI, V8HI
+  v4i32_r = __lsx_vaddwev_w_hu_h(v8u16_a, v8i16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vaddwev.w.hu.h(
+
+  // __lsx_vaddwev_h_bu_b
+  // vd, vj, vk
+  // V8HI, UV16QI, V16QI
+  v8i16_r = __lsx_vaddwev_h_bu_b(v16u8_a, v16i8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vaddwev.h.bu.b(
+
+  // __lsx_vaddwod_d_wu_w
+  // vd, vj, vk
+  // V2DI, UV4SI, V4SI
+  v2i64_r = __lsx_vaddwod_d_wu_w(v4u32_a, v4i32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vaddwod.d.wu.w(
+
+  // __lsx_vaddwod_w_hu_h
+  // vd, vj, vk
+  // V4SI, UV8HI, V8HI
+  v4i32_r = __lsx_vaddwod_w_hu_h(v8u16_a, v8i16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vaddwod.w.hu.h(
+
+  // __lsx_vaddwod_h_bu_b
+  // vd, vj, vk
+  // V8HI, UV16QI, V16QI
+  v8i16_r = __lsx_vaddwod_h_bu_b(v16u8_a, v16i8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vaddwod.h.bu.b(
+
+  // __lsx_vsubwev_d_w
+  // vd, vj, vk
+  // V2DI, V4SI, V4SI
+  v2i64_r = __lsx_vsubwev_d_w(v4i32_a, v4i32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsubwev.d.w(
+
+  // __lsx_vsubwev_w_h
+  // vd, vj, vk
+  // V4SI, V8HI, V8HI
+  v4i32_r = __lsx_vsubwev_w_h(v8i16_a, v8i16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsubwev.w.h(
+
+  // __lsx_vsubwev_h_b
+  // vd, vj, vk
+  // V8HI, V16QI, V16QI
+  v8i16_r = __lsx_vsubwev_h_b(v16i8_a, v16i8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsubwev.h.b(
+
+  // __lsx_vsubwod_d_w
+  // vd, vj, vk
+  // V2DI, V4SI, V4SI
+  v2i64_r = __lsx_vsubwod_d_w(v4i32_a, v4i32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsubwod.d.w(
+
+  // __lsx_vsubwod_w_h
+  // vd, vj, vk
+  // V4SI, V8HI, V8HI
+  v4i32_r = __lsx_vsubwod_w_h(v8i16_a, v8i16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsubwod.w.h(
+
+  // __lsx_vsubwod_h_b
+  // vd, vj, vk
+  // V8HI, V16QI, V16QI
+  v8i16_r = __lsx_vsubwod_h_b(v16i8_a, v16i8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsubwod.h.b(
+
+  // __lsx_vsubwev_d_wu
+  // vd, vj, vk
+  // V2DI, UV4SI, UV4SI
+  v2i64_r = __lsx_vsubwev_d_wu(v4u32_a, v4u32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsubwev.d.wu(
+
+  // __lsx_vsubwev_w_hu
+  // vd, vj, vk
+  // V4SI, UV8HI, UV8HI
+  v4i32_r = __lsx_vsubwev_w_hu(v8u16_a, v8u16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsubwev.w.hu(
+
+  // __lsx_vsubwev_h_bu
+  // vd, vj, vk
+  // V8HI, UV16QI, UV16QI
+  v8i16_r = __lsx_vsubwev_h_bu(v16u8_a, v16u8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsubwev.h.bu(
+
+  // __lsx_vsubwod_d_wu
+  // vd, vj, vk
+  // V2DI, UV4SI, UV4SI
+  v2i64_r = __lsx_vsubwod_d_wu(v4u32_a, v4u32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsubwod.d.wu(
+
+  // __lsx_vsubwod_w_hu
+  // vd, vj, vk
+  // V4SI, UV8HI, UV8HI
+  v4i32_r = __lsx_vsubwod_w_hu(v8u16_a, v8u16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsubwod.w.hu(
+
+  // __lsx_vsubwod_h_bu
+  // vd, vj, vk
+  // V8HI, UV16QI, UV16QI
+  v8i16_r = __lsx_vsubwod_h_bu(v16u8_a, v16u8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsubwod.h.bu(
+
+  // __lsx_vaddwev_q_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vaddwev_q_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vaddwev.q.d(
+
+  // __lsx_vaddwod_q_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vaddwod_q_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vaddwod.q.d(
+
+  // __lsx_vaddwev_q_du
+  // vd, vj, vk
+  // V2DI, UV2DI, UV2DI
+  v2i64_r = __lsx_vaddwev_q_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vaddwev.q.du(
+
+  // __lsx_vaddwod_q_du
+  // vd, vj, vk
+  // V2DI, UV2DI, UV2DI
+  v2i64_r = __lsx_vaddwod_q_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vaddwod.q.du(
+
+  // __lsx_vsubwev_q_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vsubwev_q_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsubwev.q.d(
+
+  // __lsx_vsubwod_q_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vsubwod_q_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsubwod.q.d(
+
+  // __lsx_vsubwev_q_du
+  // vd, vj, vk
+  // V2DI, UV2DI, UV2DI
+  v2i64_r = __lsx_vsubwev_q_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsubwev.q.du(
+
+  // __lsx_vsubwod_q_du
+  // vd, vj, vk
+  // V2DI, UV2DI, UV2DI
+  v2i64_r = __lsx_vsubwod_q_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsubwod.q.du(
+
+  // __lsx_vaddwev_q_du_d
+  // vd, vj, vk
+  // V2DI, UV2DI, V2DI
+  v2i64_r = __lsx_vaddwev_q_du_d(v2u64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vaddwev.q.du.d(
+
+  // __lsx_vaddwod_q_du_d
+  // vd, vj, vk
+  // V2DI, UV2DI, V2DI
+  v2i64_r = __lsx_vaddwod_q_du_d(v2u64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vaddwod.q.du.d(
+
+  // __lsx_vmulwev_d_w
+  // vd, vj, vk
+  // V2DI, V4SI, V4SI
+  v2i64_r = __lsx_vmulwev_d_w(v4i32_a, v4i32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmulwev.d.w(
+
+  // __lsx_vmulwev_w_h
+  // vd, vj, vk
+  // V4SI, V8HI, V8HI
+  v4i32_r = __lsx_vmulwev_w_h(v8i16_a, v8i16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmulwev.w.h(
+
+  // __lsx_vmulwev_h_b
+  // vd, vj, vk
+  // V8HI, V16QI, V16QI
+  v8i16_r = __lsx_vmulwev_h_b(v16i8_a, v16i8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmulwev.h.b(
+
+  // __lsx_vmulwod_d_w
+  // vd, vj, vk
+  // V2DI, V4SI, V4SI
+  v2i64_r = __lsx_vmulwod_d_w(v4i32_a, v4i32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmulwod.d.w(
+
+  // __lsx_vmulwod_w_h
+  // vd, vj, vk
+  // V4SI, V8HI, V8HI
+  v4i32_r = __lsx_vmulwod_w_h(v8i16_a, v8i16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmulwod.w.h(
+
+  // __lsx_vmulwod_h_b
+  // vd, vj, vk
+  // V8HI, V16QI, V16QI
+  v8i16_r = __lsx_vmulwod_h_b(v16i8_a, v16i8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmulwod.h.b(
+
+  // __lsx_vmulwev_d_wu
+  // vd, vj, vk
+  // V2DI, UV4SI, UV4SI
+  v2i64_r = __lsx_vmulwev_d_wu(v4u32_a, v4u32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmulwev.d.wu(
+
+  // __lsx_vmulwev_w_hu
+  // vd, vj, vk
+  // V4SI, UV8HI, UV8HI
+  v4i32_r = __lsx_vmulwev_w_hu(v8u16_a, v8u16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmulwev.w.hu(
+
+  // __lsx_vmulwev_h_bu
+  // vd, vj, vk
+  // V8HI, UV16QI, UV16QI
+  v8i16_r = __lsx_vmulwev_h_bu(v16u8_a, v16u8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmulwev.h.bu(
+
+  // __lsx_vmulwod_d_wu
+  // vd, vj, vk
+  // V2DI, UV4SI, UV4SI
+  v2i64_r = __lsx_vmulwod_d_wu(v4u32_a, v4u32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmulwod.d.wu(
+
+  // __lsx_vmulwod_w_hu
+  // vd, vj, vk
+  // V4SI, UV8HI, UV8HI
+  v4i32_r = __lsx_vmulwod_w_hu(v8u16_a, v8u16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmulwod.w.hu(
+
+  // __lsx_vmulwod_h_bu
+  // vd, vj, vk
+  // V8HI, UV16QI, UV16QI
+  v8i16_r = __lsx_vmulwod_h_bu(v16u8_a, v16u8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmulwod.h.bu(
+
+  // __lsx_vmulwev_d_wu_w
+  // vd, vj, vk
+  // V2DI, UV4SI, V4SI
+  v2i64_r = __lsx_vmulwev_d_wu_w(v4u32_a, v4i32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmulwev.d.wu.w(
+
+  // __lsx_vmulwev_w_hu_h
+  // vd, vj, vk
+  // V4SI, UV8HI, V8HI
+  v4i32_r = __lsx_vmulwev_w_hu_h(v8u16_a, v8i16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmulwev.w.hu.h(
+
+  // __lsx_vmulwev_h_bu_b
+  // vd, vj, vk
+  // V8HI, UV16QI, V16QI
+  v8i16_r = __lsx_vmulwev_h_bu_b(v16u8_a, v16i8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmulwev.h.bu.b(
+
+  // __lsx_vmulwod_d_wu_w
+  // vd, vj, vk
+  // V2DI, UV4SI, V4SI
+  v2i64_r = __lsx_vmulwod_d_wu_w(v4u32_a, v4i32_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmulwod.d.wu.w(
+
+  // __lsx_vmulwod_w_hu_h
+  // vd, vj, vk
+  // V4SI, UV8HI, V8HI
+  v4i32_r = __lsx_vmulwod_w_hu_h(v8u16_a, v8i16_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmulwod.w.hu.h(
+
+  // __lsx_vmulwod_h_bu_b
+  // vd, vj, vk
+  // V8HI, UV16QI, V16QI
+  v8i16_r = __lsx_vmulwod_h_bu_b(v16u8_a, v16i8_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmulwod.h.bu.b(
+
+  // __lsx_vmulwev_q_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vmulwev_q_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmulwev.q.d(
+
+  // __lsx_vmulwod_q_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vmulwod_q_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmulwod.q.d(
+
+  // __lsx_vmulwev_q_du
+  // vd, vj, vk
+  // V2DI, UV2DI, UV2DI
+  v2i64_r = __lsx_vmulwev_q_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmulwev.q.du(
+
+  // __lsx_vmulwod_q_du
+  // vd, vj, vk
+  // V2DI, UV2DI, UV2DI
+  v2i64_r = __lsx_vmulwod_q_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmulwod.q.du(
+
+  // __lsx_vmulwev_q_du_d
+  // vd, vj, vk
+  // V2DI, UV2DI, V2DI
+  v2i64_r = __lsx_vmulwev_q_du_d(v2u64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmulwev.q.du.d(
+
+  // __lsx_vmulwod_q_du_d
+  // vd, vj, vk
+  // V2DI, UV2DI, V2DI
+  v2i64_r = __lsx_vmulwod_q_du_d(v2u64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmulwod.q.du.d(
+
+  // __lsx_vhaddw_q_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vhaddw_q_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vhaddw.q.d(
+
+  // __lsx_vhaddw_qu_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vhaddw_qu_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vhaddw.qu.du(
+
+  // __lsx_vhsubw_q_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vhsubw_q_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vhsubw.q.d(
+
+  // __lsx_vhsubw_qu_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vhsubw_qu_du(v2u64_a, v2u64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vhsubw.qu.du(
+
+  // __lsx_vmaddwev_d_w
+  // vd, vj, vk
+  // V2DI, V2DI, V4SI, V4SI
+  v2i64_r = __lsx_vmaddwev_d_w(v2i64_a, v4i32_b, v4i32_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaddwev.d.w(
+
+  // __lsx_vmaddwev_w_h
+  // vd, vj, vk
+  // V4SI, V4SI, V8HI, V8HI
+  v4i32_r = __lsx_vmaddwev_w_h(v4i32_a, v8i16_b, v8i16_c); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmaddwev.w.h(
+
+  // __lsx_vmaddwev_h_b
+  // vd, vj, vk
+  // V8HI, V8HI, V16QI, V16QI
+  v8i16_r = __lsx_vmaddwev_h_b(v8i16_a, v16i8_b, v16i8_c); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmaddwev.h.b(
+
+  // __lsx_vmaddwev_d_wu
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV4SI, UV4SI
+  v2u64_r = __lsx_vmaddwev_d_wu(v2u64_a, v4u32_b, v4u32_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaddwev.d.wu(
+
+  // __lsx_vmaddwev_w_hu
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV8HI, UV8HI
+  v4u32_r = __lsx_vmaddwev_w_hu(v4u32_a, v8u16_b, v8u16_c); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmaddwev.w.hu(
+
+  // __lsx_vmaddwev_h_bu
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV16QI, UV16QI
+  v8u16_r = __lsx_vmaddwev_h_bu(v8u16_a, v16u8_b, v16u8_c); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmaddwev.h.bu(
+
+  // __lsx_vmaddwod_d_w
+  // vd, vj, vk
+  // V2DI, V2DI, V4SI, V4SI
+  v2i64_r = __lsx_vmaddwod_d_w(v2i64_a, v4i32_b, v4i32_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaddwod.d.w(
+
+  // __lsx_vmaddwod_w_h
+  // vd, vj, vk
+  // V4SI, V4SI, V8HI, V8HI
+  v4i32_r = __lsx_vmaddwod_w_h(v4i32_a, v8i16_b, v8i16_c); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmaddwod.w.h(
+
+  // __lsx_vmaddwod_h_b
+  // vd, vj, vk
+  // V8HI, V8HI, V16QI, V16QI
+  v8i16_r = __lsx_vmaddwod_h_b(v8i16_a, v16i8_b, v16i8_c); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmaddwod.h.b(
+
+  // __lsx_vmaddwod_d_wu
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV4SI, UV4SI
+  v2u64_r = __lsx_vmaddwod_d_wu(v2u64_a, v4u32_b, v4u32_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaddwod.d.wu(
+
+  // __lsx_vmaddwod_w_hu
+  // vd, vj, vk
+  // UV4SI, UV4SI, UV8HI, UV8HI
+  v4u32_r = __lsx_vmaddwod_w_hu(v4u32_a, v8u16_b, v8u16_c); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmaddwod.w.hu(
+
+  // __lsx_vmaddwod_h_bu
+  // vd, vj, vk
+  // UV8HI, UV8HI, UV16QI, UV16QI
+  v8u16_r = __lsx_vmaddwod_h_bu(v8u16_a, v16u8_b, v16u8_c); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmaddwod.h.bu(
+
+  // __lsx_vmaddwev_d_wu_w
+  // vd, vj, vk
+  // V2DI, V2DI, UV4SI, V4SI
+  v2i64_r = __lsx_vmaddwev_d_wu_w(v2i64_a, v4u32_b, v4i32_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaddwev.d.wu.w(
+
+  // __lsx_vmaddwev_w_hu_h
+  // vd, vj, vk
+  // V4SI, V4SI, UV8HI, V8HI
+  v4i32_r = __lsx_vmaddwev_w_hu_h(v4i32_a, v8u16_b, v8i16_c); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmaddwev.w.hu.h(
+
+  // __lsx_vmaddwev_h_bu_b
+  // vd, vj, vk
+  // V8HI, V8HI, UV16QI, V16QI
+  v8i16_r = __lsx_vmaddwev_h_bu_b(v8i16_a, v16u8_b, v16i8_c); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmaddwev.h.bu.b(
+
+  // __lsx_vmaddwod_d_wu_w
+  // vd, vj, vk
+  // V2DI, V2DI, UV4SI, V4SI
+  v2i64_r = __lsx_vmaddwod_d_wu_w(v2i64_a, v4u32_b, v4i32_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaddwod.d.wu.w(
+
+  // __lsx_vmaddwod_w_hu_h
+  // vd, vj, vk
+  // V4SI, V4SI, UV8HI, V8HI
+  v4i32_r = __lsx_vmaddwod_w_hu_h(v4i32_a, v8u16_b, v8i16_c); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vmaddwod.w.hu.h(
+
+  // __lsx_vmaddwod_h_bu_b
+  // vd, vj, vk
+  // V8HI, V8HI, UV16QI, V16QI
+  v8i16_r = __lsx_vmaddwod_h_bu_b(v8i16_a, v16u8_b, v16i8_c); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vmaddwod.h.bu.b(
+
+  // __lsx_vmaddwev_q_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vmaddwev_q_d(v2i64_a, v2i64_b, v2i64_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaddwev.q.d(
+
+  // __lsx_vmaddwod_q_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vmaddwod_q_d(v2i64_a, v2i64_b, v2i64_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaddwod.q.d(
+
+  // __lsx_vmaddwev_q_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vmaddwev_q_du(v2u64_a, v2u64_b, v2u64_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaddwev.q.du(
+
+  // __lsx_vmaddwod_q_du
+  // vd, vj, vk
+  // UV2DI, UV2DI, UV2DI, UV2DI
+  v2u64_r = __lsx_vmaddwod_q_du(v2u64_a, v2u64_b, v2u64_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaddwod.q.du(
+
+  // __lsx_vmaddwev_q_du_d
+  // vd, vj, vk
+  // V2DI, V2DI, UV2DI, V2DI
+  v2i64_r = __lsx_vmaddwev_q_du_d(v2i64_a, v2u64_b, v2i64_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaddwev.q.du.d(
+
+  // __lsx_vmaddwod_q_du_d
+  // vd, vj, vk
+  // V2DI, V2DI, UV2DI, V2DI
+  v2i64_r = __lsx_vmaddwod_q_du_d(v2i64_a, v2u64_b, v2i64_c); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vmaddwod.q.du.d(
+
+  // __lsx_vrotr_b
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vrotr_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vrotr.b(
+
+  // __lsx_vrotr_h
+  // vd, vj, vk
+  // V8HI, V8HI, V8HI
+  v8i16_r = __lsx_vrotr_h(v8i16_a, v8i16_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vrotr.h(
+
+  // __lsx_vrotr_w
+  // vd, vj, vk
+  // V4SI, V4SI, V4SI
+  v4i32_r = __lsx_vrotr_w(v4i32_a, v4i32_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vrotr.w(
+
+  // __lsx_vrotr_d
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vrotr_d(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vrotr.d(
+
+  // __lsx_vadd_q
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vadd_q(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vadd.q(
+
+  // __lsx_vsub_q
+  // vd, vj, vk
+  // V2DI, V2DI, V2DI
+  v2i64_r = __lsx_vsub_q(v2i64_a, v2i64_b); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsub.q(
+
+  // __lsx_vldrepl_b
+  // vd, rj, si12
+  // V16QI, CVPOINTER, SI
+  v16i8_r = __lsx_vldrepl_b(&v16i8_a, si12); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vldrepl.b(
+
+  // __lsx_vldrepl_h
+  // vd, rj, si11
+  // V8HI, CVPOINTER, SI
+  v8i16_r = __lsx_vldrepl_h(&v8i16_a, si11); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vldrepl.h(
+
+  // __lsx_vldrepl_w
+  // vd, rj, si10
+  // V4SI, CVPOINTER, SI
+  v4i32_r = __lsx_vldrepl_w(&v4i32_a, si10); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vldrepl.w(
+
+  // __lsx_vldrepl_d
+  // vd, rj, si9
+  // V2DI, CVPOINTER, SI
+  v2i64_r = __lsx_vldrepl_d(&v2i64_a, si9); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vldrepl.d(
+
+  // __lsx_vmskgez_b
+  // vd, vj
+  // V16QI, V16QI
+  v16i8_r = __lsx_vmskgez_b(v16i8_a); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmskgez.b(
+
+  // __lsx_vmsknz_b
+  // vd, vj
+  // V16QI, V16QI
+  v16i8_r = __lsx_vmsknz_b(v16i8_a); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vmsknz.b(
+
+  // __lsx_vexth_h_b
+  // vd, vj
+  // V8HI, V16QI
+  v8i16_r = __lsx_vexth_h_b(v16i8_a); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vexth.h.b(
+
+  // __lsx_vexth_w_h
+  // vd, vj
+  // V4SI, V8HI
+  v4i32_r = __lsx_vexth_w_h(v8i16_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vexth.w.h(
+
+  // __lsx_vexth_d_w
+  // vd, vj
+  // V2DI, V4SI
+  v2i64_r = __lsx_vexth_d_w(v4i32_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vexth.d.w(
+
+  // __lsx_vexth_q_d
+  // vd, vj
+  // V2DI, V2DI
+  v2i64_r = __lsx_vexth_q_d(v2i64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vexth.q.d(
+
+  // __lsx_vexth_hu_bu
+  // vd, vj
+  // UV8HI, UV16QI
+  v8u16_r = __lsx_vexth_hu_bu(v16u8_a); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vexth.hu.bu(
+
+  // __lsx_vexth_wu_hu
+  // vd, vj
+  // UV4SI, UV8HI
+  v4u32_r = __lsx_vexth_wu_hu(v8u16_a); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vexth.wu.hu(
+
+  // __lsx_vexth_du_wu
+  // vd, vj
+  // UV2DI, UV4SI
+  v2u64_r = __lsx_vexth_du_wu(v4u32_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vexth.du.wu(
+
+  // __lsx_vexth_qu_du
+  // vd, vj
+  // UV2DI, UV2DI
+  v2u64_r = __lsx_vexth_qu_du(v2u64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vexth.qu.du(
+
+  // __lsx_vrotri_b
+  // vd, vj, ui3
+  // V16QI, V16QI, UQI
+  v16i8_r = __lsx_vrotri_b(v16i8_a, ui3); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vrotri.b(
+
+  // __lsx_vrotri_h
+  // vd, vj, ui4
+  // V8HI, V8HI, UQI
+  v8i16_r = __lsx_vrotri_h(v8i16_a, ui4); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vrotri.h(
+
+  // __lsx_vrotri_w
+  // vd, vj, ui5
+  // V4SI, V4SI, UQI
+  v4i32_r = __lsx_vrotri_w(v4i32_a, ui5); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vrotri.w(
+
+  // __lsx_vrotri_d
+  // vd, vj, ui6
+  // V2DI, V2DI, UQI
+  v2i64_r = __lsx_vrotri_d(v2i64_a, ui6); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vrotri.d(
+
+  // __lsx_vextl_q_d
+  // vd, vj
+  // V2DI, V2DI
+  v2i64_r = __lsx_vextl_q_d(v2i64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vextl.q.d(
+
+  // __lsx_vsrlni_b_h
+  // vd, vj, ui4
+  // V16QI, V16QI, V16QI, USI
+  v16i8_r = __lsx_vsrlni_b_h(v16i8_a, v16i8_b, ui4); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsrlni.b.h(
+
+  // __lsx_vsrlni_h_w
+  // vd, vj, ui5
+  // V8HI, V8HI, V8HI, USI
+  v8i16_r = __lsx_vsrlni_h_w(v8i16_a, v8i16_b, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsrlni.h.w(
+
+  // __lsx_vsrlni_w_d
+  // vd, vj, ui6
+  // V4SI, V4SI, V4SI, USI
+  v4i32_r = __lsx_vsrlni_w_d(v4i32_a, v4i32_b, ui6); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsrlni.w.d(
+
+  // __lsx_vsrlni_d_q
+  // vd, vj, ui7
+  // V2DI, V2DI, V2DI, USI
+  v2i64_r = __lsx_vsrlni_d_q(v2i64_a, v2i64_b, ui7); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsrlni.d.q(
+
+  // __lsx_vssrlni_b_h
+  // vd, vj, ui4
+  // V16QI, V16QI, V16QI, USI
+  v16i8_r = __lsx_vssrlni_b_h(v16i8_a, v16i8_b, ui4); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrlni.b.h(
+
+  // __lsx_vssrlni_h_w
+  // vd, vj, ui5
+  // V8HI, V8HI, V8HI, USI
+  v8i16_r = __lsx_vssrlni_h_w(v8i16_a, v8i16_b, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrlni.h.w(
+
+  // __lsx_vssrlni_w_d
+  // vd, vj, ui6
+  // V4SI, V4SI, V4SI, USI
+  v4i32_r = __lsx_vssrlni_w_d(v4i32_a, v4i32_b, ui6); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrlni.w.d(
+
+  // __lsx_vssrlni_d_q
+  // vd, vj, ui7
+  // V2DI, V2DI, V2DI, USI
+  v2i64_r = __lsx_vssrlni_d_q(v2i64_a, v2i64_b, ui7); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vssrlni.d.q(
+
+  // __lsx_vssrlni_bu_h
+  // vd, vj, ui4
+  // UV16QI, UV16QI, V16QI, USI
+  v16u8_r = __lsx_vssrlni_bu_h(v16u8_a, v16i8_b, ui4); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrlni.bu.h(
+
+  // __lsx_vssrlni_hu_w
+  // vd, vj, ui5
+  // UV8HI, UV8HI, V8HI, USI
+  v8u16_r = __lsx_vssrlni_hu_w(v8u16_a, v8i16_b, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrlni.hu.w(
+
+  // __lsx_vssrlni_wu_d
+  // vd, vj, ui6
+  // UV4SI, UV4SI, V4SI, USI
+  v4u32_r = __lsx_vssrlni_wu_d(v4u32_a, v4i32_b, ui6); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrlni.wu.d(
+
+  // __lsx_vssrlni_du_q
+  // vd, vj, ui7
+  // UV2DI, UV2DI, V2DI, USI
+  v2u64_r = __lsx_vssrlni_du_q(v2u64_a, v2i64_b, ui7); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vssrlni.du.q(
+
+  // __lsx_vssrlrni_b_h
+  // vd, vj, ui4
+  // V16QI, V16QI, V16QI, USI
+  v16i8_r = __lsx_vssrlrni_b_h(v16i8_a, v16i8_b, ui4); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrlrni.b.h(
+
+  // __lsx_vssrlrni_h_w
+  // vd, vj, ui5
+  // V8HI, V8HI, V8HI, USI
+  v8i16_r = __lsx_vssrlrni_h_w(v8i16_a, v8i16_b, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrlrni.h.w(
+
+  // __lsx_vssrlrni_w_d
+  // vd, vj, ui6
+  // V4SI, V4SI, V4SI, USI
+  v4i32_r = __lsx_vssrlrni_w_d(v4i32_a, v4i32_b, ui6); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrlrni.w.d(
+
+  // __lsx_vssrlrni_d_q
+  // vd, vj, ui7
+  // V2DI, V2DI, V2DI, USI
+  v2i64_r = __lsx_vssrlrni_d_q(v2i64_a, v2i64_b, ui7); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vssrlrni.d.q(
+
+  // __lsx_vssrlrni_bu_h
+  // vd, vj, ui4
+  // UV16QI, UV16QI, V16QI, USI
+  v16u8_r = __lsx_vssrlrni_bu_h(v16u8_a, v16i8_b, ui4); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrlrni.bu.h(
+
+  // __lsx_vssrlrni_hu_w
+  // vd, vj, ui5
+  // UV8HI, UV8HI, V8HI, USI
+  v8u16_r = __lsx_vssrlrni_hu_w(v8u16_a, v8i16_b, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrlrni.hu.w(
+
+  // __lsx_vssrlrni_wu_d
+  // vd, vj, ui6
+  // UV4SI, UV4SI, V4SI, USI
+  v4u32_r = __lsx_vssrlrni_wu_d(v4u32_a, v4i32_b, ui6); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrlrni.wu.d(
+
+  // __lsx_vssrlrni_du_q
+  // vd, vj, ui7
+  // UV2DI, UV2DI, V2DI, USI
+  v2u64_r = __lsx_vssrlrni_du_q(v2u64_a, v2i64_b, ui7); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vssrlrni.du.q(
+
+  // __lsx_vsrani_b_h
+  // vd, vj, ui4
+  // V16QI, V16QI, V16QI, USI
+  v16i8_r = __lsx_vsrani_b_h(v16i8_a, v16i8_b, ui4); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsrani.b.h(
+
+  // __lsx_vsrani_h_w
+  // vd, vj, ui5
+  // V8HI, V8HI, V8HI, USI
+  v8i16_r = __lsx_vsrani_h_w(v8i16_a, v8i16_b, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsrani.h.w(
+
+  // __lsx_vsrani_w_d
+  // vd, vj, ui6
+  // V4SI, V4SI, V4SI, USI
+  v4i32_r = __lsx_vsrani_w_d(v4i32_a, v4i32_b, ui6); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsrani.w.d(
+
+  // __lsx_vsrani_d_q
+  // vd, vj, ui7
+  // V2DI, V2DI, V2DI, USI
+  v2i64_r = __lsx_vsrani_d_q(v2i64_a, v2i64_b, ui7); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsrani.d.q(
+
+  // __lsx_vsrarni_b_h
+  // vd, vj, ui4
+  // V16QI, V16QI, V16QI, USI
+  v16i8_r = __lsx_vsrarni_b_h(v16i8_a, v16i8_b, ui4); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vsrarni.b.h(
+
+  // __lsx_vsrarni_h_w
+  // vd, vj, ui5
+  // V8HI, V8HI, V8HI, USI
+  v8i16_r = __lsx_vsrarni_h_w(v8i16_a, v8i16_b, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vsrarni.h.w(
+
+  // __lsx_vsrarni_w_d
+  // vd, vj, ui6
+  // V4SI, V4SI, V4SI, USI
+  v4i32_r = __lsx_vsrarni_w_d(v4i32_a, v4i32_b, ui6); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vsrarni.w.d(
+
+  // __lsx_vsrarni_d_q
+  // vd, vj, ui7
+  // V2DI, V2DI, V2DI, USI
+  v2i64_r = __lsx_vsrarni_d_q(v2i64_a, v2i64_b, ui7); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vsrarni.d.q(
+
+  // __lsx_vssrani_b_h
+  // vd, vj, ui4
+  // V16QI, V16QI, V16QI, USI
+  v16i8_r = __lsx_vssrani_b_h(v16i8_a, v16i8_b, ui4); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrani.b.h(
+
+  // __lsx_vssrani_h_w
+  // vd, vj, ui5
+  // V8HI, V8HI, V8HI, USI
+  v8i16_r = __lsx_vssrani_h_w(v8i16_a, v8i16_b, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrani.h.w(
+
+  // __lsx_vssrani_w_d
+  // vd, vj, ui6
+  // V4SI, V4SI, V4SI, USI
+  v4i32_r = __lsx_vssrani_w_d(v4i32_a, v4i32_b, ui6); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrani.w.d(
+
+  // __lsx_vssrani_d_q
+  // vd, vj, ui7
+  // V2DI, V2DI, V2DI, USI
+  v2i64_r = __lsx_vssrani_d_q(v2i64_a, v2i64_b, ui7); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vssrani.d.q(
+
+  // __lsx_vssrani_bu_h
+  // vd, vj, ui4
+  // UV16QI, UV16QI, V16QI, USI
+  v16u8_r = __lsx_vssrani_bu_h(v16u8_a, v16i8_b, ui4); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrani.bu.h(
+
+  // __lsx_vssrani_hu_w
+  // vd, vj, ui5
+  // UV8HI, UV8HI, V8HI, USI
+  v8u16_r = __lsx_vssrani_hu_w(v8u16_a, v8i16_b, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrani.hu.w(
+
+  // __lsx_vssrani_wu_d
+  // vd, vj, ui6
+  // UV4SI, UV4SI, V4SI, USI
+  v4u32_r = __lsx_vssrani_wu_d(v4u32_a, v4i32_b, ui6); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrani.wu.d(
+
+  // __lsx_vssrani_du_q
+  // vd, vj, ui7
+  // UV2DI, UV2DI, V2DI, USI
+  v2u64_r = __lsx_vssrani_du_q(v2u64_a, v2i64_b, ui7); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vssrani.du.q(
+
+  // __lsx_vssrarni_b_h
+  // vd, vj, ui4
+  // V16QI, V16QI, V16QI, USI
+  v16i8_r = __lsx_vssrarni_b_h(v16i8_a, v16i8_b, ui4); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrarni.b.h(
+
+  // __lsx_vssrarni_h_w
+  // vd, vj, ui5
+  // V8HI, V8HI, V8HI, USI
+  v8i16_r = __lsx_vssrarni_h_w(v8i16_a, v8i16_b, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrarni.h.w(
+
+  // __lsx_vssrarni_w_d
+  // vd, vj, ui6
+  // V4SI, V4SI, V4SI, USI
+  v4i32_r = __lsx_vssrarni_w_d(v4i32_a, v4i32_b, ui6); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrarni.w.d(
+
+  // __lsx_vssrarni_d_q
+  // vd, vj, ui7
+  // V2DI, V2DI, V2DI, USI
+  v2i64_r = __lsx_vssrarni_d_q(v2i64_a, v2i64_b, ui7); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vssrarni.d.q(
+
+  // __lsx_vssrarni_bu_h
+  // vd, vj, ui4
+  // UV16QI, UV16QI, V16QI, USI
+  v16u8_r = __lsx_vssrarni_bu_h(v16u8_a, v16i8_b, ui4); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrarni.bu.h(
+
+  // __lsx_vssrarni_hu_w
+  // vd, vj, ui5
+  // UV8HI, UV8HI, V8HI, USI
+  v8u16_r = __lsx_vssrarni_hu_w(v8u16_a, v8i16_b, ui5); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrarni.hu.w(
+
+  // __lsx_vssrarni_wu_d
+  // vd, vj, ui6
+  // UV4SI, UV4SI, V4SI, USI
+  v4u32_r = __lsx_vssrarni_wu_d(v4u32_a, v4i32_b, ui6); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrarni.wu.d(
+
+  // __lsx_vssrarni_du_q
+  // vd, vj, ui7
+  // UV2DI, UV2DI, V2DI, USI
+  v2u64_r = __lsx_vssrarni_du_q(v2u64_a, v2i64_b, ui7); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vssrarni.du.q(
+
+  // __lsx_vpermi_w
+  // vd, vj, ui8
+  // V4SI, V4SI, V4SI, USI
+  v4i32_r = __lsx_vpermi_w(v4i32_a, v4i32_b, ui8); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vpermi.w(
+
+  // __lsx_vld
+  // vd, rj, si12
+  // V16QI, CVPOINTER, SI
+  v16i8_r = __lsx_vld(&v16i8_a, si12); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vld(
+
+  // __lsx_vst
+  // vd, rj, si12
+  // VOID, V16QI, CVPOINTER, SI
+  __lsx_vst(v16i8_a, &v16i8_b, 0); // CHECK: call void @llvm.loongarch.lsx.vst(
+
+  // __lsx_vssrlrn_b_h
+  // vd, vj, vk
+  // V16QI, V8HI, V8HI
+  v16i8_r = __lsx_vssrlrn_b_h(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrlrn.b.h(
+
+  // __lsx_vssrlrn_h_w
+  // vd, vj, vk
+  // V8HI, V4SI, V4SI
+  v8i16_r = __lsx_vssrlrn_h_w(v4i32_a, v4i32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrlrn.h.w(
+
+  // __lsx_vssrlrn_w_d
+  // vd, vj, vk
+  // V4SI, V2DI, V2DI
+  v4i32_r = __lsx_vssrlrn_w_d(v2i64_a, v2i64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrlrn.w.d(
+
+  // __lsx_vssrln_b_h
+  // vd, vj, vk
+  // V16QI, V8HI, V8HI
+  v16i8_r = __lsx_vssrln_b_h(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vssrln.b.h(
+
+  // __lsx_vssrln_h_w
+  // vd, vj, vk
+  // V8HI, V4SI, V4SI
+  v8i16_r = __lsx_vssrln_h_w(v4i32_a, v4i32_b); // CHECK: call <8 x i16> @llvm.loongarch.lsx.vssrln.h.w(
+
+  // __lsx_vssrln_w_d
+  // vd, vj, vk
+  // V4SI, V2DI, V2DI
+  v4i32_r = __lsx_vssrln_w_d(v2i64_a, v2i64_b); // CHECK: call <4 x i32> @llvm.loongarch.lsx.vssrln.w.d(
+
+  // __lsx_vorn_v
+  // vd, vj, vk
+  // V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vorn_v(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vorn.v(
+
+  // __lsx_vldi
+  // vd, i13
+  // V2DI, HI
+  v2i64_r = __lsx_vldi(i13); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vldi(
+
+  // __lsx_vshuf_b
+  // vd, vj, vk, va
+  // V16QI, V16QI, V16QI, V16QI
+  v16i8_r = __lsx_vshuf_b(v16i8_a, v16i8_b, v16i8_c); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vshuf.b(
+
+  // __lsx_vldx
+  // vd, rj, rk
+  // V16QI, CVPOINTER, DI
+  v16i8_r = __lsx_vldx(&v16i8_a, i64_d); // CHECK: call <16 x i8> @llvm.loongarch.lsx.vldx(
+
+  // __lsx_vstx
+  // vd, rj, rk
+  // VOID, V16QI, CVPOINTER, DI
+  __lsx_vstx(v16i8_a, &v16i8_b, i64_d); // CHECK: call void @llvm.loongarch.lsx.vstx(
+
+  // __lsx_vextl_qu_du
+  // vd, vj
+  // UV2DI, UV2DI
+  v2u64_r = __lsx_vextl_qu_du(v2u64_a); // CHECK: call <2 x i64> @llvm.loongarch.lsx.vextl.qu.du(
+
+  // __lsx_bnz_v
+  // rd, vj
+  // SI, UV16QI
+  i32_r = __lsx_bnz_v(v16u8_a); // CHECK: call i32 @llvm.loongarch.lsx.bnz.v(
+
+  // __lsx_bz_v
+  // rd, vj
+  // SI, UV16QI
+  i32_r = __lsx_bz_v(v16u8_a); // CHECK: call i32 @llvm.loongarch.lsx.bz.v(
+
+  // __lsx_bnz_b
+  // rd, vj
+  // SI, UV16QI
+  i32_r = __lsx_bnz_b(v16u8_a); // CHECK: call i32 @llvm.loongarch.lsx.bnz.b(
+
+  // __lsx_bnz_h
+  // rd, vj
+  // SI, UV8HI
+  i32_r = __lsx_bnz_h(v8u16_a); // CHECK: call i32 @llvm.loongarch.lsx.bnz.h(
+
+  // __lsx_bnz_w
+  // rd, vj
+  // SI, UV4SI
+  i32_r = __lsx_bnz_w(v4u32_a); // CHECK: call i32 @llvm.loongarch.lsx.bnz.w(
+
+  // __lsx_bnz_d
+  // rd, vj
+  // SI, UV2DI
+  i32_r = __lsx_bnz_d(v2u64_a); // CHECK: call i32 @llvm.loongarch.lsx.bnz.d(
+
+  // __lsx_bz_b
+  // rd, vj
+  // SI, UV16QI
+  i32_r = __lsx_bz_b(v16u8_a); // CHECK: call i32 @llvm.loongarch.lsx.bz.b(
+
+  // __lsx_bz_h
+  // rd, vj
+  // SI, UV8HI
+  i32_r = __lsx_bz_h(v8u16_a); // CHECK: call i32 @llvm.loongarch.lsx.bz.h(
+
+  // __lsx_bz_w
+  // rd, vj
+  // SI, UV4SI
+  i32_r = __lsx_bz_w(v4u32_a); // CHECK: call i32 @llvm.loongarch.lsx.bz.w(
+
+  // __lsx_bz_d
+  // rd, vj
+  // SI, UV2DI
+  i32_r = __lsx_bz_d(v2u64_a); // CHECK: call i32 @llvm.loongarch.lsx.bz.d(
+}
diff --git a/test/CodeGen/loongarch-inline-asm-modifiers.c b/test/CodeGen/loongarch-inline-asm-modifiers.c
new file mode 100644
index 00000000..08822e64
--- /dev/null
+++ b/test/CodeGen/loongarch-inline-asm-modifiers.c
@@ -0,0 +1,50 @@
+// RUN: %clang -target loongarch64-unknown-linux-gnu -S -o - -emit-llvm %s \
+// RUN: | FileCheck %s
+
+// This checks that the frontend will accept inline asm operand modifiers
+
+int printf(const char*, ...);
+
+typedef long long v2i64 __attribute__ ((vector_size(16), aligned(16)));
+typedef long long v4i64 __attribute__ ((vector_size(32), aligned(32)));
+
+// CHECK: %{{[0-9]+}} = call i32 asm "ld.w    $0,$1;\0A", "=r,*m"(i32* elementtype(i32) getelementptr inbounds ([8 x i32], [8 x i32]* @b, i64 {{[0-9]+}}, i64 {{[0-9]+}})) #2,
+// CHECK: %{{[0-9]+}} = call i32 asm "ld.w    $0,${1:D};\0A", "=r,*m"(i32* elementtype(i32) getelementptr inbounds ([8 x i32], [8 x i32]* @b, i64 {{[0-9]+}}, i64 {{[0-9]+}})) #2,
+// CHECK: %{{[0-9]+}} = call <2 x i64> asm "vldi ${0:w},1", "=f"
+// CHECK: %{{[0-9]+}} = call <4 x i64> asm "xldi ${0:u},1", "=f"
+int b[8] = {0,1,2,3,4,5,6,7};
+int  main()
+{
+  int i;
+  v2i64 v2i64_r;
+  v4i64 v4i64_r;
+
+  // The first word. Notice, no 'D'
+  {asm (
+  "ld.w    %0,%1;\n"
+  : "=r" (i)
+  : "m" (*(b+4)));}
+
+  printf("%d\n",i);
+
+  // The second word
+  {asm (
+  "ld.w    %0,%D1;\n"
+  : "=r" (i)
+  : "m" (*(b+4))
+  );}
+
+  // LSX registers
+  { asm("vldi %w0,1"
+        : "=f"(v2i64_r)); }
+
+  printf("%d\n", i);
+
+  // LASX registers
+  { asm("xldi %u0,1"
+        : "=f"(v4i64_r)); }
+
+  printf("%d\n",i);
+
+  return 1;
+}
diff --git a/test/CodeGen/loongarch-inline-asm.c b/test/CodeGen/loongarch-inline-asm.c
new file mode 100644
index 00000000..dadb7e3f
--- /dev/null
+++ b/test/CodeGen/loongarch-inline-asm.c
@@ -0,0 +1,31 @@
+// REQUIRES: loongarch-registered-target
+// RUN: %clang_cc1 -triple loongarch64-linux-gnu -emit-llvm -o - %s | FileCheck %s
+
+int data;
+
+void m () {
+  asm("ld.w $r1, %0" :: "m"(data));
+  // CHECK: call void asm sideeffect "ld.w $$r1, $0", "*m"(i32* elementtype(i32) @data)
+}
+
+void ZC () {
+  asm("ll.w $r1, %0" :: "ZC"(data));
+  // CHECK: call void asm sideeffect "ll.w $$r1, $0", "*^ZC"(i32* elementtype(i32) @data)
+}
+
+void ZB () {
+  asm("amadd_db.w $zero, $r1, %0" :: "ZB"(data));
+  // CHECK: call void asm sideeffect "amadd_db.w $$zero, $$r1, $0", "*^ZB"(i32* elementtype(i32) @data)
+}
+
+void R () {
+  asm("ld.w $r1, %0" :: "R"(data));
+  // CHECK: call void asm sideeffect "ld.w $$r1, $0", "*R"(i32* elementtype(i32) @data)
+}
+
+int *p;
+void preld () {
+  asm("preld 0, %0, 2" :: "r"(p));
+  // CHECK: %0 = load i32*, i32** @p, align 8
+  // CHECK: call void asm sideeffect "preld 0, $0, 2", "r"(i32* %0)
+}
diff --git a/test/CodeGenCXX/LoongArch/abi-lp64d-struct-inherit.cpp b/test/CodeGenCXX/LoongArch/abi-lp64d-struct-inherit.cpp
new file mode 100644
index 00000000..cbe6469d
--- /dev/null
+++ b/test/CodeGenCXX/LoongArch/abi-lp64d-struct-inherit.cpp
@@ -0,0 +1,95 @@
+// RUN: %clang_cc1 -triple loongarch64 -target-feature +f -target-feature +d -target-abi lp64d \
+// RUN:   -emit-llvm %s -o - | FileCheck %s
+
+#include <stdint.h>
+
+/// Ensure that fields inherited from a parent struct are treated in the same
+/// way as fields directly in the child for the purposes of LoongArch ABI rules.
+
+struct parent1_int32_s {
+  int32_t i1;
+};
+
+struct child1_int32_s : parent1_int32_s {
+  int32_t i2;
+};
+
+// CHECK-LABEL: define{{.*}} i64 @_Z30int32_int32_struct_inheritance14child1_int32_s(i64 %a.coerce)
+struct child1_int32_s int32_int32_struct_inheritance(struct child1_int32_s a) {
+  return a;
+}
+
+struct parent2_int32_s {
+  int32_t i1;
+};
+
+struct child2_float_s : parent2_int32_s {
+  float f1;
+};
+
+// CHECK-LABEL: define{{.*}} { i32, float } @_Z30int32_float_struct_inheritance14child2_float_s(i32 %0, float %1)
+struct child2_float_s int32_float_struct_inheritance(struct child2_float_s a) {
+  return a;
+}
+
+struct parent3_float_s {
+  float f1;
+};
+
+struct child3_int64_s : parent3_float_s {
+  int64_t i1;
+};
+
+// CHECK-LABEL: define{{.*}} { float, i64 } @_Z30float_int64_struct_inheritance14child3_int64_s(float %0, i64 %1)
+struct child3_int64_s float_int64_struct_inheritance(struct child3_int64_s a) {
+  return a;
+}
+
+struct parent4_double_s {
+  double d1;
+};
+
+struct child4_double_s : parent4_double_s {
+  double d1;
+};
+
+// CHECK-LABEL: define{{.*}} { double, double } @_Z32double_double_struct_inheritance15child4_double_s(double %0, double %1)
+struct child4_double_s double_double_struct_inheritance(struct child4_double_s a) {
+  return a;
+}
+
+/// When virtual inheritance is used, the resulting struct isn't eligible for
+/// passing in registers.
+
+struct parent5_virtual_s {
+  int32_t i1;
+};
+
+struct child5_virtual_s : virtual parent5_virtual_s {
+  float f1;
+};
+
+// CHECK-LABEL: define{{.*}} void @_ZN16child5_virtual_sC1EOS_(%struct.child5_virtual_s*{{.*}} %this, %struct.child5_virtual_s*{{.*}} dereferenceable(12) %0)
+struct child5_virtual_s int32_float_virtual_struct_inheritance(struct child5_virtual_s a) {
+  return a;
+}
+
+/// Check for correct lowering in the presence of diamoned inheritance.
+
+struct parent6_float_s {
+  float f1;
+};
+
+struct child6a_s : parent6_float_s {
+};
+
+struct child6b_s : parent6_float_s {
+};
+
+struct grandchild_6_s : child6a_s, child6b_s {
+};
+
+// CHECK-LABEL: define{{.*}} { float, float } @_Z38float_float_diamond_struct_inheritance14grandchild_6_s(float %0, float %1)
+struct grandchild_6_s float_float_diamond_struct_inheritance(struct grandchild_6_s a) {
+  return a;
+}
diff --git a/test/Driver/loongarch-abi-fpu.c b/test/Driver/loongarch-abi-fpu.c
new file mode 100644
index 00000000..180d440c
--- /dev/null
+++ b/test/Driver/loongarch-abi-fpu.c
@@ -0,0 +1,26 @@
+/// Check passing -mabi=<ABIName> and -mfpu=<FPU> options to the backend.
+
+// RUN: %clang -target loongarch64 %s -mabi=lp64s -mfpu=none -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-NF-ND %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64s -mfpu=32 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-F %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64s -mfpu=64 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-D %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64f -mfpu=none -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=ERRLP64F-WITH-FPUNONE %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64f -mfpu=32 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-F %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64f -mfpu=64 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-D %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64d -mfpu=none -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=ERRLP64D-ONLY-FPU64 %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64d -mfpu=32 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=ERRLP64D-ONLY-FPU64 %s
+// RUN: %clang -target loongarch64 %s -mabi=lp64d -mfpu=64 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-D %s
+
+// FEATURE-D: "-target-feature" "+d"
+// FEATURE-F: "-target-feature" "+f"
+// FEATURE-NF-ND: "-target-feature" "-f" "-target-feature" "-d"
+// ERRLP64D-ONLY-FPU64: error: option 'lp64d' cannot be specified without '-mfpu=64'
+// ERRLP64F-WITH-FPUNONE: error: option 'lp64f' cannot be specified with '-mfpu=none'
diff --git a/test/Driver/loongarch-alignment-feature.c b/test/Driver/loongarch-alignment-feature.c
new file mode 100644
index 00000000..2270ff53
--- /dev/null
+++ b/test/Driver/loongarch-alignment-feature.c
@@ -0,0 +1,8 @@
+// RUN: %clang -target loongarch64-unknown-linux-gnu -mno-strict-align -### %s 2> %t
+// RUN: FileCheck --check-prefix=CHECK-UNALIGNED < %t %s
+
+// RUN: %clang -target loongarch64-unknown-linux-gnu -mstrict-align -### %s 2> %t
+// RUN: FileCheck --check-prefix=CHECK-ALIGNED < %t %s
+
+// CHECK-UNALIGNED: "-target-feature" "+unaligned-access"
+// CHECK-ALIGNED: "-target-feature" "-unaligned-access"
diff --git a/test/Driver/loongarch-double-single-soft.c b/test/Driver/loongarch-double-single-soft.c
new file mode 100644
index 00000000..4b25f876
--- /dev/null
+++ b/test/Driver/loongarch-double-single-soft.c
@@ -0,0 +1,12 @@
+// Check passing -m*-float options to the backend.
+
+// RUN: %clang -target loongarch64 %s -mdouble-float -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-DOUBLE %s
+// RUN: %clang -target loongarch64 %s -msingle-float -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-SINGLE %s
+// RUN: %clang -target loongarch64 %s -msoft-float -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-SOFT %s
+
+// CHECK-DOUBLE: "-target-feature" "+d" "-target-abi" "lp64d"
+// CHECK-SINGLE: "-target-feature" "+f" "-target-abi" "lp64f"
+// CHECK-SOFT: "-target-feature" "-f" "-target-feature" "-d" "-target-abi" "lp64s"
diff --git a/test/Driver/loongarch-mabi.c b/test/Driver/loongarch-mabi.c
new file mode 100644
index 00000000..88a90408
--- /dev/null
+++ b/test/Driver/loongarch-mabi.c
@@ -0,0 +1,22 @@
+// Check passing -mabi=<ABIName> options to the backend.
+
+// check default ABI for loongarch64
+// RUN: %clang -target loongarch64 %s -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-LP64D %s
+// check -mabi=lp64d option for loongarch64
+// RUN: %clang -target loongarch64 %s -mabi=lp64d -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-LP64D %s
+// check -mabi=lp64f option for loongarch64
+// RUN: %clang -target loongarch64 %s -mabi=lp64f -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-LP64F %s
+// check -mabi=lp64s option for loongarch64
+// RUN: %clang -target loongarch64 %s -mabi=lp64s -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-LP64S %s
+// check invalid -mabi=x option for loongarch64
+// RUN: not %clang -target loongarch64 %s -mabi=x 2>&1 \
+// RUN:   | FileCheck --check-prefix=CHECK-X %s
+
+// CHECK-LP64D: "-target-abi" "lp64d"
+// CHECK-LP64F: "-target-abi" "lp64f"
+// CHECK-LP64S: "-target-abi" "lp64s"
+// CHECK-X: error: unknown target ABI 'x'
diff --git a/test/Driver/loongarch-mfpu.c b/test/Driver/loongarch-mfpu.c
new file mode 100644
index 00000000..0cf05fd3
--- /dev/null
+++ b/test/Driver/loongarch-mfpu.c
@@ -0,0 +1,21 @@
+// Check passing -mfpu=<FPU> options to the backend.
+
+// check default feature for loongarch64
+// RUN: %clang -target loongarch64 %s -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-D %s
+// check -mfpu=64 option for loongarch64
+// RUN: %clang -target loongarch64 %s -mfpu=64 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=FEATURE-D %s
+// check -mfpu=32 option for loongarch64
+// RUN: %clang -target loongarch64 %s -mfpu=32 -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=ERRLP64D-ONLY-FPU64 %s
+// check -mfpu=none option for loongarch64
+// RUN: %clang -target loongarch64 %s -mfpu=none -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=ERRLP64D-ONLY-FPU64 %s
+// check -mfpu=x option for loongarch64
+// RUN: %clang -target loongarch64 %s -mfpu=x -### 2>&1 \
+// RUN:   | FileCheck --check-prefix=INVALID-FPU %s
+
+// FEATURE-D: "-target-feature" "+d"
+// INVALID-FPU: error: invalid loongarch FPU value 'x'. Please specify FPU = 64,32 or none
+// ERRLP64D-ONLY-FPU64: error: option 'lp64d' cannot be specified without '-mfpu=64'
-- 
2.38.1

