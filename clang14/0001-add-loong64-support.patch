From e5f62e4b6e97e38353668baeac0ef7219992aa63 Mon Sep 17 00:00:00 2001
From: Xiaotian Wu <wuxiaotian@loongson.cn>
Date: Tue, 20 Dec 2022 18:53:42 +0800
Subject: [PATCH 1/2] add loong64 support

---
 bindings/python/tests/CMakeLists.txt          |    2 +-
 include/clang/Basic/BuiltinsLoongArch.def     | 1974 ++++++
 include/clang/Basic/DiagnosticDriverKinds.td  |    2 +
 include/clang/Basic/TargetBuiltins.h          |   13 +-
 include/clang/Basic/TargetCXXABI.def          |    6 +
 include/clang/Basic/TargetCXXABI.h            |    6 +
 include/clang/Driver/Options.td               |   21 +-
 include/clang/Sema/Sema.h                     |    3 +
 include/clang/module.modulemap                |    1 +
 lib/AST/ASTContext.cpp                        |    2 +
 lib/Basic/CMakeLists.txt                      |    1 +
 lib/Basic/Targets.cpp                         |   20 +
 lib/Basic/Targets/LoongArch.cpp               |  149 +
 lib/Basic/Targets/LoongArch.h                 |  352 ++
 lib/CodeGen/CodeGenFunction.cpp               |   36 +-
 lib/CodeGen/CodeGenFunction.h                 |    4 +
 lib/CodeGen/CodeGenModule.cpp                 |   17 +-
 lib/CodeGen/CodeGenModule.h                   |    5 -
 lib/CodeGen/ItaniumCXXABI.cpp                 |    3 +
 lib/CodeGen/TargetInfo.cpp                    |  555 ++
 lib/Driver/CMakeLists.txt                     |    1 +
 lib/Driver/Driver.cpp                         |   16 +
 lib/Driver/SanitizerArgs.cpp                  |   13 -
 lib/Driver/ToolChains/Arch/LoongArch.cpp      |  179 +
 lib/Driver/ToolChains/Arch/LoongArch.h        |   41 +
 lib/Driver/ToolChains/Clang.cpp               |   55 +
 lib/Driver/ToolChains/Clang.h                 |    4 +
 lib/Driver/ToolChains/CommonArgs.cpp          |   21 +
 lib/Driver/ToolChains/Gnu.cpp                 |   65 +
 lib/Driver/ToolChains/Linux.cpp               |   24 +-
 lib/Driver/ToolChains/Linux.h                 |    5 -
 lib/Driver/XRayArgs.cpp                       |    2 +
 lib/Headers/CMakeLists.txt                    |    3 +
 lib/Headers/larchintrin.h                     |  319 +
 lib/Headers/lasxintrin.h                      | 5349 +++++++++++++++++
 lib/Headers/lsxintrin.h                       | 5165 ++++++++++++++++
 lib/Sema/SemaChecking.cpp                     |  544 ++
 lib/Sema/SemaTemplateInstantiateDecl.cpp      |    5 +-
 test/CodeGen/sanitize-coverage-old-pm.c       |    4 +-
 test/CodeGen/ubsan-function.cpp               |    5 +-
 test/CodeGenCXX/catch-undef-behavior.cpp      |   37 +-
 test/CodeGenCXX/ubsan-function-noexcept.cpp   |    6 +-
 test/Driver/baremetal-sysroot.cpp             |    2 +-
 test/Driver/baremetal.cpp                     |    2 +-
 test/Driver/fsanitize.c                       |    9 +-
 test/Driver/hexagon-toolchain-linux.c         |    4 +-
 test/Driver/mips-cs.cpp                       |   48 +-
 test/Driver/stack-protector.c                 |    4 +-
 test/Preprocessor/init.c                      |   30 +
 .../InterpreterExceptionTest.cpp              |    5 +
 50 files changed, 15017 insertions(+), 122 deletions(-)
 create mode 100644 include/clang/Basic/BuiltinsLoongArch.def
 create mode 100644 lib/Basic/Targets/LoongArch.cpp
 create mode 100644 lib/Basic/Targets/LoongArch.h
 create mode 100644 lib/Driver/ToolChains/Arch/LoongArch.cpp
 create mode 100644 lib/Driver/ToolChains/Arch/LoongArch.h
 create mode 100644 lib/Headers/larchintrin.h
 create mode 100644 lib/Headers/lasxintrin.h
 create mode 100644 lib/Headers/lsxintrin.h

diff --git a/bindings/python/tests/CMakeLists.txt b/bindings/python/tests/CMakeLists.txt
index 280da9d0..9d9cb911 100644
--- a/bindings/python/tests/CMakeLists.txt
+++ b/bindings/python/tests/CMakeLists.txt
@@ -40,7 +40,7 @@ endif()
 # addressed.
 # SystemZ has broken Python/FFI interface:
 # https://reviews.llvm.org/D52840#1265716
-if(${LLVM_NATIVE_ARCH} MATCHES "^(AArch64|Hexagon|Sparc|SystemZ)$")
+if(${LLVM_NATIVE_ARCH} MATCHES "^(AArch64|Hexagon|LoongArch|Sparc|SystemZ)$")
   set(RUN_PYTHON_TESTS FALSE)
 endif()
 
diff --git a/include/clang/Basic/BuiltinsLoongArch.def b/include/clang/Basic/BuiltinsLoongArch.def
new file mode 100644
index 00000000..5606e62d
--- /dev/null
+++ b/include/clang/Basic/BuiltinsLoongArch.def
@@ -0,0 +1,1974 @@
+//===-- BuiltinsLoongArch.def - LoongArch Builtin function database --------*- C++ -*-==//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines the LoongArch-specific builtin function database. Users of
+// this file must define the BUILTIN macro to make use of this information.
+//
+//===----------------------------------------------------------------------===//
+
+// The format of this database matches clang/Basic/Builtins.def.
+
+// LoongArch LSX
+
+BUILTIN(__builtin_lsx_vclo_b, "V16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vclo_h, "V8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vclo_w, "V4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vclo_d, "V2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vflogb_s, "V4fV4f", "nc")
+BUILTIN(__builtin_lsx_vflogb_d, "V2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vpickve2gr_b, "iV16ScIUi", "nc")
+BUILTIN(__builtin_lsx_vpickve2gr_h, "iV8SsIUi", "nc")
+BUILTIN(__builtin_lsx_vpickve2gr_w, "iV4SiIUi", "nc")
+BUILTIN(__builtin_lsx_vpickve2gr_d, "LLiV2SLLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vpickve2gr_bu, "iV16UcIUi", "nc")
+BUILTIN(__builtin_lsx_vpickve2gr_hu, "iV8UsIUi", "nc")
+BUILTIN(__builtin_lsx_vpickve2gr_wu, "iV4UiIUi", "nc")
+BUILTIN(__builtin_lsx_vpickve2gr_du, "LLiV2ULLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vreplvei_b, "V16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vreplvei_h, "V8sV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vreplvei_w, "V4iV4iIUi", "nc")
+BUILTIN(__builtin_lsx_vreplvei_d, "V2LLiV2LLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vmskltz_b, "V16cV16c", "nc")
+BUILTIN(__builtin_lsx_vmskltz_h, "V8sV8s", "nc")
+BUILTIN(__builtin_lsx_vmskltz_w, "V4iV4i", "nc")
+BUILTIN(__builtin_lsx_vmskltz_d, "V2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vfmadd_s, "V4fV4fV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfmadd_d, "V2dV2dV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfmsub_s, "V4fV4fV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfmsub_d, "V2dV2dV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfnmadd_s, "V4fV4fV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfnmadd_d, "V2dV2dV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfnmsub_s, "V4fV4fV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfnmsub_d, "V2dV2dV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_caf_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_caf_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_cor_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_cor_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_cun_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_cun_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_cune_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_cune_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_cueq_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_cueq_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_ceq_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_ceq_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_cne_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_cne_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_clt_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_clt_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_cult_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_cult_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_cle_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_cle_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_cule_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_cule_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_saf_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_saf_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_sor_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_sor_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_sun_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_sun_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_sune_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_sune_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_sueq_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_sueq_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_seq_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_seq_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_sne_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_sne_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_slt_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_slt_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_sult_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_sult_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_sle_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_sle_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcmp_sule_s, "V4SiV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcmp_sule_d, "V2SLLiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vbitsel_v, "V16UcV16UcV16UcV16Uc", "nc")
+
+BUILTIN(__builtin_lsx_vshuf_b, "V16UcV16UcV16UcV16Uc", "nc")
+
+BUILTIN(__builtin_lsx_vldrepl_b, "V16cv*Ii", "nc")
+BUILTIN(__builtin_lsx_vldrepl_h, "V8sv*Ii", "nc")
+BUILTIN(__builtin_lsx_vldrepl_w, "V4iv*Ii", "nc")
+BUILTIN(__builtin_lsx_vldrepl_d, "V2LLiv*Ii", "nc")
+
+BUILTIN(__builtin_lsx_vstelm_b, "vV16Scv*IiUi", "nc")
+BUILTIN(__builtin_lsx_vstelm_h, "vV8Ssv*IiUi", "nc")
+BUILTIN(__builtin_lsx_vstelm_w, "vV4Siv*IiUi", "nc")
+BUILTIN(__builtin_lsx_vstelm_d, "vV2SLLiv*IiUi", "nc")
+
+BUILTIN(__builtin_lsx_vldx, "V16Scv*LLi", "nc")
+BUILTIN(__builtin_lsx_vstx, "vV16Scv*LLi", "nc")
+
+BUILTIN(__builtin_lsx_vaddwev_d_w, "V2LLiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vaddwev_w_h, "V4SiV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vaddwev_h_b, "V8sV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vaddwev_q_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vsubwev_d_w, "V2LLiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vsubwev_w_h, "V4SiV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vsubwev_h_b, "V8sV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vsubwev_q_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vaddwod_d_w, "V2LLiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vaddwod_w_h, "V4SiV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vaddwod_h_b, "V8sV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vaddwod_q_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vsubwod_d_w, "V2LLiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vsubwod_w_h, "V4SiV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vsubwod_h_b, "V8sV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vsubwod_q_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vaddwev_d_wu, "V2LLiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vaddwev_w_hu, "V4SiV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vaddwev_h_bu, "V8sV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vaddwev_q_du, "V2LLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vsubwev_d_wu, "V2LLiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vsubwev_w_hu, "V4SiV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vsubwev_h_bu, "V8sV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vsubwev_q_du, "V2LLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vaddwod_d_wu, "V2LLiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vaddwod_w_hu, "V4SiV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vaddwod_h_bu, "V8sV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vaddwod_q_du, "V2LLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vsubwod_d_wu, "V2LLiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vsubwod_w_hu, "V4SiV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vsubwod_h_bu, "V8sV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vsubwod_q_du, "V2LLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vaddwev_d_wu_w, "V2LLiV4UiV4Si", "nc")
+BUILTIN(__builtin_lsx_vaddwev_w_hu_h, "V4SiV8UsV8s", "nc")
+BUILTIN(__builtin_lsx_vaddwev_h_bu_b, "V8sV16UcV16c", "nc")
+BUILTIN(__builtin_lsx_vaddwev_q_du_d, "V2LLiV2ULLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vaddwod_d_wu_w, "V2LLiV4UiV4Si", "nc")
+BUILTIN(__builtin_lsx_vaddwod_w_hu_h, "V4SiV8UsV8s", "nc")
+BUILTIN(__builtin_lsx_vaddwod_h_bu_b, "V8sV16UcV16c", "nc")
+BUILTIN(__builtin_lsx_vaddwod_q_du_d, "V2LLiV2ULLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vhaddw_q_d, "V2LLiV2LLiV2LLi", "nc")
+BUILTIN(__builtin_lsx_vhsubw_q_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vhaddw_qu_du, "V2ULLiV2ULLiV2ULLi", "nc")
+BUILTIN(__builtin_lsx_vhsubw_qu_du, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vmuh_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vmuh_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vmuh_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vmuh_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vmuh_bu, "V16UcV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vmuh_hu, "V8UsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vmuh_wu, "V4UiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vmuh_du, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vmulwev_d_w, "V2LLiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmulwev_w_h, "V4SiV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vmulwev_h_b, "V8sV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vmulwev_q_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vmulwod_d_w, "V2LLiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmulwod_w_h, "V4SiV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vmulwod_h_b, "V8sV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vmulwod_q_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vmulwev_d_wu, "V2LLiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vmulwev_w_hu, "V4SiV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vmulwev_h_bu, "V8sV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vmulwev_q_du, "V2LLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vmulwod_d_wu, "V2LLiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vmulwod_w_hu, "V4SiV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vmulwod_h_bu, "V8sV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vmulwod_q_du, "V2LLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vmulwev_d_wu_w, "V2LLiV4UiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmulwev_w_hu_h, "V4SiV8UsV8s", "nc")
+BUILTIN(__builtin_lsx_vmulwev_h_bu_b, "V8sV16UcV16c", "nc")
+BUILTIN(__builtin_lsx_vmulwev_q_du_d, "V2LLiV2ULLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vmulwod_d_wu_w, "V2LLiV4UiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmulwod_w_hu_h, "V4SiV8UsV8s", "nc")
+BUILTIN(__builtin_lsx_vmulwod_h_bu_b, "V8sV16UcV16c", "nc")
+BUILTIN(__builtin_lsx_vmulwod_q_du_d, "V2LLiV2ULLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vmaddwev_d_w, "V2LLiV2LLiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmaddwev_w_h, "V4SiV4SiV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vmaddwev_h_b, "V8sV8sV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vmaddwev_q_d, "V2LLiV2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vmaddwod_d_w, "V2LLiV2LLiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmaddwod_w_h, "V4SiV4SiV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vmaddwod_h_b, "V8sV8sV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vmaddwod_q_d, "V2LLiV2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vmaddwev_d_wu, "V2ULLiV2ULLiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vmaddwev_w_hu, "V4UiV4UiV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vmaddwev_h_bu, "V8UsV8UsV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vmaddwev_q_du, "V2ULLiV2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vmaddwod_d_wu, "V2ULLiV2ULLiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vmaddwod_w_hu, "V4UiV4UiV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vmaddwod_h_bu, "V8UsV8UsV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vmaddwod_q_du, "V2ULLiV2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vmaddwev_d_wu_w, "V2LLiV2LLiV4UiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmaddwev_w_hu_h, "V4SiV4SiV8UsV8s", "nc")
+BUILTIN(__builtin_lsx_vmaddwev_h_bu_b, "V8sV8sV16UcV16c", "nc")
+BUILTIN(__builtin_lsx_vmaddwev_q_du_d, "V2LLiV2LLiV2ULLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vmaddwod_d_wu_w, "V2LLiV2LLiV4UiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmaddwod_w_hu_h, "V4SiV4SiV8UsV8s", "nc")
+BUILTIN(__builtin_lsx_vmaddwod_h_bu_b, "V8sV8sV16UcV16c", "nc")
+BUILTIN(__builtin_lsx_vmaddwod_q_du_d, "V2LLiV2LLiV2ULLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vsrln_b_h, "V16ScV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vsrln_h_w, "V8sV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vsrln_w_d, "V4SiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vsran_b_h, "V16ScV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vsran_h_w, "V8sV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vsran_w_d, "V4SiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vsrlrn_b_h, "V16ScV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vsrlrn_h_w, "V8sV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vsrlrn_w_d, "V4SiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vsrarn_b_h, "V16ScV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vsrarn_h_w, "V8sV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vsrarn_w_d, "V4SiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vssrln_b_h, "V16ScV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vssrln_h_w, "V8sV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vssrln_w_d, "V4SiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vssran_b_h, "V16ScV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vssran_h_w, "V8sV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vssran_w_d, "V4SiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vssrlrn_b_h, "V16ScV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vssrlrn_h_w, "V8sV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vssrlrn_w_d, "V4SiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vssrarn_b_h, "V16ScV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vssrarn_h_w, "V8sV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vssrarn_w_d, "V4SiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vssrln_bu_h, "V16UcV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vssrln_hu_w, "V8UsV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vssrln_wu_d, "V4UiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vssran_bu_h, "V16UcV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vssran_hu_w, "V8UsV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vssran_wu_d, "V4UiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vssrlrn_bu_h, "V16UcV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vssrlrn_hu_w, "V8UsV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vssrlrn_wu_d, "V4UiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vssrarn_bu_h, "V16UcV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vssrarn_hu_w, "V8UsV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vssrarn_wu_d, "V4UiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vandn_v, "V16UcV16UcV16Uc", "nc")
+
+BUILTIN(__builtin_lsx_vorn_v, "V16ScV16ScV16Sc", "nc")
+
+BUILTIN(__builtin_lsx_vfrstp_b, "V16ScV16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vfrstp_h, "V8SsV8SsV8SsV8Ss", "nc")
+
+BUILTIN(__builtin_lsx_vadd_q, "V2LLiV2LLiV2LLi", "nc")
+BUILTIN(__builtin_lsx_vsub_q, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vsigncov_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vsigncov_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vsigncov_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vsigncov_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vfcvt_h_s, "V8sV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfcvt_s_d, "V4fV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vftint_w_d, "V4SiV2dV2d", "nc")
+BUILTIN(__builtin_lsx_vffint_s_l, "V4fV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vftintrz_w_d, "V4SiV2dV2d", "nc")
+BUILTIN(__builtin_lsx_vftintrp_w_d, "V4SiV2dV2d", "nc")
+BUILTIN(__builtin_lsx_vftintrm_w_d, "V4SiV2dV2d", "nc")
+BUILTIN(__builtin_lsx_vftintrne_w_d, "V4SiV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vbsrl_v, "V16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vbsll_v, "V16cV16cIUi", "nc")
+
+BUILTIN(__builtin_lsx_vfrstpi_b, "V16cV16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vfrstpi_h, "V8sV8sV8sIUi", "nc")
+
+BUILTIN(__builtin_lsx_vneg_b, "V16cV16c", "nc")
+BUILTIN(__builtin_lsx_vneg_h, "V8sV8s", "nc")
+BUILTIN(__builtin_lsx_vneg_w, "V4iV4i", "nc")
+BUILTIN(__builtin_lsx_vneg_d, "V2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vmskgez_b, "V16cV16c", "nc")
+BUILTIN(__builtin_lsx_vmsknz_b, "V8sV8s", "nc")
+
+BUILTIN(__builtin_lsx_vfrintrm_s, "V4SiV4f", "nc")
+BUILTIN(__builtin_lsx_vfrintrm_d, "V2LLiV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfrintrp_s, "V4SiV4f", "nc")
+BUILTIN(__builtin_lsx_vfrintrp_d, "V2LLiV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfrintrz_s, "V4SiV4f", "nc")
+BUILTIN(__builtin_lsx_vfrintrz_d, "V2LLiV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfrintrne_s, "V4SiV4f", "nc")
+BUILTIN(__builtin_lsx_vfrintrne_d, "V2LLiV2d", "nc")
+
+BUILTIN(__builtin_lsx_vffinth_d_w, "V2dV4Si", "nc")
+BUILTIN(__builtin_lsx_vffintl_d_w, "V2dV4Si", "nc")
+
+BUILTIN(__builtin_lsx_vftintrm_w_s, "V4SiV4f", "nc")
+BUILTIN(__builtin_lsx_vftintrm_l_d, "V2LLiV2d", "nc")
+
+BUILTIN(__builtin_lsx_vftintrp_w_s, "V4SiV4f", "nc")
+BUILTIN(__builtin_lsx_vftintrp_l_d, "V2LLiV2d", "nc")
+
+BUILTIN(__builtin_lsx_vftintrz_w_s, "V4SiV4f", "nc")
+BUILTIN(__builtin_lsx_vftintrz_l_d, "V2LLiV2d", "nc")
+
+BUILTIN(__builtin_lsx_vftintrne_w_s, "V4SiV4f", "nc")
+BUILTIN(__builtin_lsx_vftintrne_l_d, "V2LLiV2d", "nc")
+
+BUILTIN(__builtin_lsx_vftinth_l_s, "V2LLiV4f", "nc")
+BUILTIN(__builtin_lsx_vftintl_l_s, "V2LLiV4f", "nc")
+
+BUILTIN(__builtin_lsx_vftintrmh_l_s, "V2LLiV4f", "nc")
+BUILTIN(__builtin_lsx_vftintrml_l_s, "V2LLiV4f", "nc")
+
+BUILTIN(__builtin_lsx_vftintrph_l_s, "V2LLiV4f", "nc")
+BUILTIN(__builtin_lsx_vftintrpl_l_s, "V2LLiV4f", "nc")
+
+BUILTIN(__builtin_lsx_vftintrzh_l_s, "V2LLiV4f", "nc")
+BUILTIN(__builtin_lsx_vftintrzl_l_s, "V2LLiV4f", "nc")
+
+BUILTIN(__builtin_lsx_vftintrneh_l_s, "V2LLiV4f", "nc")
+BUILTIN(__builtin_lsx_vftintrnel_l_s, "V2LLiV4f", "nc")
+
+BUILTIN(__builtin_lsx_vexth_d_w, "V2LLiV4Si", "nc")
+BUILTIN(__builtin_lsx_vexth_w_h, "V4SiV8s", "nc")
+BUILTIN(__builtin_lsx_vexth_h_b, "V8sV16c", "nc")
+BUILTIN(__builtin_lsx_vexth_q_d, "V2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vexth_du_wu, "V2ULLiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vexth_wu_hu, "V4UiV8Us", "nc")
+BUILTIN(__builtin_lsx_vexth_hu_bu, "V8UsV16Uc", "nc")
+BUILTIN(__builtin_lsx_vexth_qu_du, "V2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vsllwil_d_w, "V2LLiV4SiIUi", "nc")
+BUILTIN(__builtin_lsx_vsllwil_w_h, "V4SiV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vsllwil_h_b, "V8sV16cIUi", "nc")
+
+BUILTIN(__builtin_lsx_vextl_q_d, "V2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vsllwil_du_wu, "V2ULLiV4UiIUi", "nc")
+BUILTIN(__builtin_lsx_vsllwil_wu_hu, "V4UiV8UsIUi", "nc")
+BUILTIN(__builtin_lsx_vsllwil_hu_bu, "V8UsV16UcIUi", "nc")
+
+BUILTIN(__builtin_lsx_vextl_qu_du, "V2LLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vbitclri_b, "V16UcV16UcIUi", "nc")
+BUILTIN(__builtin_lsx_vbitclri_h, "V8UsV8UsIUi", "nc")
+BUILTIN(__builtin_lsx_vbitclri_w, "V4UiV4UiIUi", "nc")
+BUILTIN(__builtin_lsx_vbitclri_d, "V2ULLiV2ULLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vbitseti_b, "V16UcV16UcIUi", "nc")
+BUILTIN(__builtin_lsx_vbitseti_h, "V8UsV8UsIUi", "nc")
+BUILTIN(__builtin_lsx_vbitseti_w, "V4UiV4UiIUi", "nc")
+BUILTIN(__builtin_lsx_vbitseti_d, "V2ULLiV2ULLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vbitrevi_b, "V16UcV16UcIUi", "nc")
+BUILTIN(__builtin_lsx_vbitrevi_h, "V8UsV8UsIUi", "nc")
+BUILTIN(__builtin_lsx_vbitrevi_w, "V4UiV4UiIUi", "nc")
+BUILTIN(__builtin_lsx_vbitrevi_d, "V2ULLiV2ULLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vssrlrni_b_h, "V16cV16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vssrlrni_h_w, "V8sV8sV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vssrlrni_w_d, "V4iV4iV4iIUi", "nc")
+BUILTIN(__builtin_lsx_vssrlrni_d_q, "V2LLiV2LLiV2LLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vsrani_b_h, "V16cV16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vsrani_h_w, "V8sV8sV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vsrani_w_d, "V4iV4iV4iIUi", "nc")
+BUILTIN(__builtin_lsx_vsrani_d_q, "V2LLiV2LLiV2LLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vextrins_b, "V16cV16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vextrins_h, "V8sV8sV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vextrins_w, "V4iV4iV4iIUi", "nc")
+BUILTIN(__builtin_lsx_vextrins_d, "V2LLiV2LLiV2LLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vbitseli_b, "V16UcV16UcV16UcIUi", "nc")
+
+BUILTIN(__builtin_lsx_vandi_b, "V16UcV16UcIUi", "nc")
+
+BUILTIN(__builtin_lsx_vori_b, "V16UcV16UcIUi", "nc")
+
+BUILTIN(__builtin_lsx_vxori_b, "V16UcV16UcIUi", "nc")
+
+BUILTIN(__builtin_lsx_vnori_b, "V16UcV16UcIUi", "nc")
+
+BUILTIN(__builtin_lsx_vldi, "V2LLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vpermi_w, "V4iV4iV4iIUi", "nc")
+
+BUILTIN(__builtin_lsx_vsadd_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vsadd_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vsadd_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vsadd_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vssub_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vssub_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vssub_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vssub_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vsadd_bu, "V16UcV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vsadd_hu, "V8UsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vsadd_wu, "V4UiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vsadd_du, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vssub_bu, "V16UcV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vssub_hu, "V8UsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vssub_wu, "V4UiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vssub_du, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vhaddw_h_b, "V8SsV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vhaddw_w_h, "V4SiV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vhaddw_d_w, "V2SLLiV4SiV4Si", "nc")
+
+BUILTIN(__builtin_lsx_vhsubw_h_b, "V8SsV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vhsubw_w_h, "V4SiV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vhsubw_d_w, "V2SLLiV4SiV4Si", "nc")
+
+BUILTIN(__builtin_lsx_vhaddw_hu_bu, "V8UsV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vhaddw_wu_hu, "V4UiV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vhaddw_du_wu, "V2ULLiV4UiV4Ui", "nc")
+
+BUILTIN(__builtin_lsx_vhsubw_hu_bu, "V8UsV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vhsubw_wu_hu, "V4UiV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vhsubw_du_wu, "V2ULLiV4UiV4Ui", "nc")
+
+BUILTIN(__builtin_lsx_vadda_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vadda_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vadda_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vadda_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vabsd_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vabsd_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vabsd_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vabsd_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vabsd_bu, "V16UcV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vabsd_hu, "V8UsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vabsd_wu, "V4UiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vabsd_du, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vavg_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vavg_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vavg_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vavg_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vavg_bu, "V16UcV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vavg_hu, "V8UsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vavg_wu, "V4UiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vavg_du, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vavgr_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vavgr_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vavgr_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vavgr_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vavgr_bu, "V16UcV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vavgr_hu, "V8UsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vavgr_wu, "V4UiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vavgr_du, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vsrlr_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vsrlr_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vsrlr_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vsrlr_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vsrar_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vsrar_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vsrar_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vsrar_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vfmax_s, "V4fV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfmax_d, "V2dV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfmin_s, "V4fV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfmin_d, "V2dV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfmaxa_s, "V4fV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfmaxa_d, "V2dV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfmina_s, "V4fV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfmina_d, "V2dV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfclass_s, "V4iV4f", "nc")
+BUILTIN(__builtin_lsx_vfclass_d, "V2LLiV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfrecip_s, "V4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfrecip_d, "V2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfrsqrt_s, "V4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfrsqrt_d, "V2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfcvtl_s_h, "V4fV8s", "nc")
+BUILTIN(__builtin_lsx_vfcvtl_d_s, "V2dV4f", "nc")
+
+BUILTIN(__builtin_lsx_vfcvth_s_h, "V4fV8s", "nc")
+BUILTIN(__builtin_lsx_vfcvth_d_s, "V2dV4f", "nc")
+
+BUILTIN(__builtin_lsx_vftint_w_s, "V4SiV4f", "nc")
+BUILTIN(__builtin_lsx_vftint_l_d, "V2SLLiV2d", "nc")
+
+BUILTIN(__builtin_lsx_vftint_wu_s, "V4UiV4f", "nc")
+BUILTIN(__builtin_lsx_vftint_lu_d, "V2ULLiV2d", "nc")
+
+BUILTIN(__builtin_lsx_vsrlri_b, "V16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vsrlri_h, "V8sV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vsrlri_w, "V4iV4iIUi", "nc")
+BUILTIN(__builtin_lsx_vsrlri_d, "V2LLiV2LLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vsrari_b, "V16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vsrari_h, "V8sV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vsrari_w, "V4iV4iIUi", "nc")
+BUILTIN(__builtin_lsx_vsrari_d, "V2LLiV2LLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vsat_b, "V16ScV16ScIUi", "nc")
+BUILTIN(__builtin_lsx_vsat_h, "V8SsV8SsIUi", "nc")
+BUILTIN(__builtin_lsx_vsat_w, "V4SiV4SiIUi", "nc")
+BUILTIN(__builtin_lsx_vsat_d, "V2SLLiV2SLLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vsat_bu, "V16UcV16UcIUi", "nc")
+BUILTIN(__builtin_lsx_vsat_hu, "V8UsV8UsIUi", "nc")
+BUILTIN(__builtin_lsx_vsat_wu, "V4UiV4UiIUi", "nc")
+BUILTIN(__builtin_lsx_vsat_du, "V2ULLiV2ULLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vsrlni_b_h, "V16cV16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vsrlni_h_w, "V8sV8sV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vsrlni_w_d, "V4iV4iV4iIUi", "nc")
+BUILTIN(__builtin_lsx_vsrlni_d_q, "V2LLiV2LLiV2LLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vssrlni_b_h, "V16cV16cV16cIi", "nc")
+BUILTIN(__builtin_lsx_vssrlni_h_w, "V8sV8sV8sIi", "nc")
+BUILTIN(__builtin_lsx_vssrlni_w_d, "V4iV4iV4iIi", "nc")
+BUILTIN(__builtin_lsx_vssrlni_d_q, "V2LLiV2LLiV2LLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vssrlrni_bu_h, "V16cV16cV16cIi", "nc")
+BUILTIN(__builtin_lsx_vssrlrni_hu_w, "V8sV8sV8sIi", "nc")
+BUILTIN(__builtin_lsx_vssrlrni_wu_d, "V4iV4iV4iIi", "nc")
+BUILTIN(__builtin_lsx_vssrlrni_du_q, "V2LLiV2LLiV2LLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vsrarni_b_h, "V16cV16cV16cIi", "nc")
+BUILTIN(__builtin_lsx_vsrarni_h_w, "V8sV8sV8sIi", "nc")
+BUILTIN(__builtin_lsx_vsrarni_w_d, "V4iV4iV4iIi", "nc")
+BUILTIN(__builtin_lsx_vsrarni_d_q, "V2LLiV2LLiV2LLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vssrani_b_h, "V16cV16cV16cIi", "nc")
+BUILTIN(__builtin_lsx_vssrani_h_w, "V8sV8sV8sIi", "nc")
+BUILTIN(__builtin_lsx_vssrani_w_d, "V4iV4iV4iIi", "nc")
+BUILTIN(__builtin_lsx_vssrani_d_q, "V2LLiV2LLiV2LLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vssrani_bu_h, "V16cV16cV16cIi", "nc")
+BUILTIN(__builtin_lsx_vssrani_hu_w, "V8sV8sV8sIi", "nc")
+BUILTIN(__builtin_lsx_vssrani_wu_d, "V4iV4iV4iIi", "nc")
+BUILTIN(__builtin_lsx_vssrani_du_q, "V2LLiV2LLiV2LLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vssrarni_b_h, "V16cV16cV16cIi", "nc")
+BUILTIN(__builtin_lsx_vssrarni_h_w, "V8sV8sV8sIi", "nc")
+BUILTIN(__builtin_lsx_vssrarni_w_d, "V4iV4iV4iIi", "nc")
+BUILTIN(__builtin_lsx_vssrarni_d_q, "V2LLiV2LLiV2LLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vssrarni_bu_h, "V16cV16cV16cIi", "nc")
+BUILTIN(__builtin_lsx_vssrarni_hu_w, "V8sV8sV8sIi", "nc")
+BUILTIN(__builtin_lsx_vssrarni_wu_d, "V4iV4iV4iIi", "nc")
+BUILTIN(__builtin_lsx_vssrarni_du_q, "V2LLiV2LLiV2LLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vssrlni_bu_h, "V16cV16cV16cIi", "nc")
+BUILTIN(__builtin_lsx_vssrlni_hu_w, "V8sV8sV8sIi", "nc")
+BUILTIN(__builtin_lsx_vssrlni_wu_d, "V4iV4iV4iIi", "nc")
+BUILTIN(__builtin_lsx_vssrlni_du_q, "V2LLiV2LLiV2LLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vseq_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vseq_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vseq_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vseq_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vsle_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vsle_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vsle_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vsle_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vsle_bu, "V16ScV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vsle_hu, "V8SsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vsle_wu, "V4SiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vsle_du, "V2SLLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vslt_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vslt_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vslt_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vslt_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vslt_bu, "V16ScV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vslt_hu, "V8SsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vslt_wu, "V4SiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vslt_du, "V2SLLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vadd_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vadd_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vadd_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vadd_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vsub_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vsub_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vsub_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vsub_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vmax_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vmax_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vmax_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmax_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vmin_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vmin_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vmin_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmin_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vmax_bu, "V16UcV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vmax_hu, "V8UsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vmax_wu, "V4UiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vmax_du, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vmin_bu, "V16UcV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vmin_hu, "V8UsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vmin_wu, "V4UiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vmin_du, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vmul_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vmul_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vmul_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmul_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vmadd_b, "V16ScV16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vmadd_h, "V8SsV8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vmadd_w, "V4SiV4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmadd_d, "V2SLLiV2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vmsub_b, "V16ScV16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vmsub_h, "V8SsV8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vmsub_w, "V4SiV4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmsub_d, "V2SLLiV2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vdiv_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vdiv_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vdiv_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vdiv_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vmod_b, "V16ScV16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vmod_h, "V8SsV8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vmod_w, "V4SiV4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vmod_d, "V2SLLiV2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vdiv_bu, "V16UcV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vdiv_hu, "V8UsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vdiv_wu, "V4UiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vdiv_du, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vsll_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vsll_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vsll_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vsll_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vsrl_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vsrl_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vsrl_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vsrl_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vbitclr_b, "V16UcV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vbitclr_h, "V8UsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vbitclr_w, "V4UiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vbitclr_d, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vbitset_b, "V16UcV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vbitset_h, "V8UsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vbitset_w, "V4UiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vbitset_d, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vpackev_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vpackev_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vpackev_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vpackev_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vpackod_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vpackod_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vpackod_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vpackod_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vilvl_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vilvl_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vilvl_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vilvl_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vilvh_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vilvh_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vilvh_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vilvh_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vpickev_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vpickev_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vpickev_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vpickev_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vand_v, "V16UcV16UcV16Uc", "nc")
+
+BUILTIN(__builtin_lsx_vor_v, "V16UcV16UcV16Uc", "nc")
+
+BUILTIN(__builtin_lsx_vbitrev_b, "V16UcV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vbitrev_h, "V8UsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vbitrev_w, "V4UiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vbitrev_d, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vmod_bu, "V16UcV16UcV16Uc", "nc")
+BUILTIN(__builtin_lsx_vmod_hu, "V8UsV8UsV8Us", "nc")
+BUILTIN(__builtin_lsx_vmod_wu, "V4UiV4UiV4Ui", "nc")
+BUILTIN(__builtin_lsx_vmod_du, "V2ULLiV2ULLiV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vpickod_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vpickod_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vpickod_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vpickod_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vreplve_b, "V16cV16cUi", "nc")
+BUILTIN(__builtin_lsx_vreplve_h, "V8sV8sUi", "nc")
+BUILTIN(__builtin_lsx_vreplve_w, "V4iV4iUi", "nc")
+BUILTIN(__builtin_lsx_vreplve_d, "V2LLiV2LLiUi", "nc")
+
+BUILTIN(__builtin_lsx_vsra_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vsra_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vsra_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vsra_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vxor_v, "V16cV16cV16c", "nc")
+
+BUILTIN(__builtin_lsx_vnor_v, "V16UcV16UcV16Uc", "nc")
+
+BUILTIN(__builtin_lsx_vfadd_s, "V4fV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfadd_d, "V2dV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfsub_s, "V4fV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfsub_d, "V2dV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfmul_s, "V4fV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfmul_d, "V2dV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vshuf_h, "V8sV8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vshuf_w, "V4iV4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vshuf_d, "V2LLiV2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vseqi_b, "V16ScV16ScISi", "nc")
+BUILTIN(__builtin_lsx_vseqi_h, "V8SsV8SsISi", "nc")
+BUILTIN(__builtin_lsx_vseqi_w, "V4SiV4SiISi", "nc")
+BUILTIN(__builtin_lsx_vseqi_d, "V2SLLiV2SLLiISi", "nc")
+
+BUILTIN(__builtin_lsx_vslei_b, "V16ScV16ScISi", "nc")
+BUILTIN(__builtin_lsx_vslei_h, "V8SsV8SsISi", "nc")
+BUILTIN(__builtin_lsx_vslei_w, "V4SiV4SiISi", "nc")
+BUILTIN(__builtin_lsx_vslei_d, "V2SLLiV2SLLiISi", "nc")
+
+BUILTIN(__builtin_lsx_vslei_bu, "V16ScV16UcIUi", "nc")
+BUILTIN(__builtin_lsx_vslei_hu, "V8SsV8UsIUi", "nc")
+BUILTIN(__builtin_lsx_vslei_wu, "V4SiV4UiIUi", "nc")
+BUILTIN(__builtin_lsx_vslei_du, "V2SLLiV2ULLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vslti_b, "V16ScV16ScISi", "nc")
+BUILTIN(__builtin_lsx_vslti_h, "V8SsV8SsISi", "nc")
+BUILTIN(__builtin_lsx_vslti_w, "V4SiV4SiISi", "nc")
+BUILTIN(__builtin_lsx_vslti_d, "V2SLLiV2SLLiISi", "nc")
+
+BUILTIN(__builtin_lsx_vslti_bu, "V16ScV16UcIUi", "nc")
+BUILTIN(__builtin_lsx_vslti_hu, "V8SsV8UsIUi", "nc")
+BUILTIN(__builtin_lsx_vslti_wu, "V4SiV4UiIUi", "nc")
+BUILTIN(__builtin_lsx_vslti_du, "V2SLLiV2ULLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vaddi_bu, "V16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vaddi_hu, "V8sV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vaddi_wu, "V4iV4iIUi", "nc")
+BUILTIN(__builtin_lsx_vaddi_du, "V2LLiV2LLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vsubi_bu, "V16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vsubi_hu, "V8sV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vsubi_wu, "V4iV4iIUi", "nc")
+BUILTIN(__builtin_lsx_vsubi_du, "V2LLiV2LLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vmaxi_b, "V16ScV16ScIi", "nc")
+BUILTIN(__builtin_lsx_vmaxi_h, "V8SsV8SsIi", "nc")
+BUILTIN(__builtin_lsx_vmaxi_w, "V4SiV4SiIi", "nc")
+BUILTIN(__builtin_lsx_vmaxi_d, "V2SLLiV2SLLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vmini_b, "V16ScV16ScIi", "nc")
+BUILTIN(__builtin_lsx_vmini_h, "V8SsV8SsIi", "nc")
+BUILTIN(__builtin_lsx_vmini_w, "V4SiV4SiIi", "nc")
+BUILTIN(__builtin_lsx_vmini_d, "V2SLLiV2SLLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vmaxi_bu, "V16UcV16UcIi", "nc")
+BUILTIN(__builtin_lsx_vmaxi_hu, "V8UsV8UsIi", "nc")
+BUILTIN(__builtin_lsx_vmaxi_wu, "V4UiV4UiIi", "nc")
+BUILTIN(__builtin_lsx_vmaxi_du, "V2ULLiV2ULLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vmini_bu, "V16UcV16UcIi", "nc")
+BUILTIN(__builtin_lsx_vmini_hu, "V8UsV8UsIi", "nc")
+BUILTIN(__builtin_lsx_vmini_wu, "V4UiV4UiIi", "nc")
+BUILTIN(__builtin_lsx_vmini_du, "V2ULLiV2ULLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vclz_b, "V16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vclz_h, "V8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vclz_w, "V4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vclz_d, "V2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vpcnt_b, "V16ScV16Sc", "nc")
+BUILTIN(__builtin_lsx_vpcnt_h, "V8SsV8Ss", "nc")
+BUILTIN(__builtin_lsx_vpcnt_w, "V4SiV4Si", "nc")
+BUILTIN(__builtin_lsx_vpcnt_d, "V2SLLiV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vfsqrt_s, "V4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfsqrt_d, "V2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vfrint_s, "V4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfrint_d, "V2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vffint_s_w, "V4fV4Si", "nc")
+BUILTIN(__builtin_lsx_vffint_d_l, "V2dV2SLLi", "nc")
+
+BUILTIN(__builtin_lsx_vffint_s_wu, "V4fV4Ui", "nc")
+BUILTIN(__builtin_lsx_vffint_d_lu, "V2dV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_vftintrz_wu_s, "V4UiV4f", "nc")
+BUILTIN(__builtin_lsx_vftintrz_lu_d, "V2ULLiV2d", "nc")
+
+BUILTIN(__builtin_lsx_vreplgr2vr_b, "V16Sci", "nc")
+BUILTIN(__builtin_lsx_vreplgr2vr_h, "V8Ssi", "nc")
+BUILTIN(__builtin_lsx_vreplgr2vr_w, "V4Sii", "nc")
+BUILTIN(__builtin_lsx_vreplgr2vr_d, "V2SLLiLLi", "nc")
+
+BUILTIN(__builtin_lsx_vinsgr2vr_b, "V16ScV16SciIUi", "nc")
+BUILTIN(__builtin_lsx_vinsgr2vr_h, "V8SsV8SsiIUi", "nc")
+BUILTIN(__builtin_lsx_vinsgr2vr_w, "V4SiV4SiiIUi", "nc")
+BUILTIN(__builtin_lsx_vinsgr2vr_d, "V2SLLiV2SLLiLLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vfdiv_s, "V4fV4fV4f", "nc")
+BUILTIN(__builtin_lsx_vfdiv_d, "V2dV2dV2d", "nc")
+
+BUILTIN(__builtin_lsx_vslli_b, "V16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vslli_h, "V8sV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vslli_w, "V4iV4iIUi", "nc")
+BUILTIN(__builtin_lsx_vslli_d, "V2LLiV2LLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vsrli_b, "V16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vsrli_h, "V8sV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vsrli_w, "V4iV4iIUi", "nc")
+BUILTIN(__builtin_lsx_vsrli_d, "V2LLiV2LLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vsrai_b, "V16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vsrai_h, "V8sV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vsrai_w, "V4iV4iIUi", "nc")
+BUILTIN(__builtin_lsx_vsrai_d, "V2LLiV2LLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vshuf4i_b, "V16cV16cIUi", "nc")
+BUILTIN(__builtin_lsx_vshuf4i_h, "V8sV8sIUi", "nc")
+BUILTIN(__builtin_lsx_vshuf4i_w, "V4iV4iIUi", "nc")
+BUILTIN(__builtin_lsx_vshuf4i_d, "V2LLiV2LLiV2LLiIUi", "nc")
+
+BUILTIN(__builtin_lsx_vrotr_b, "V16cV16cV16c", "nc")
+BUILTIN(__builtin_lsx_vrotr_h, "V8sV8sV8s", "nc")
+BUILTIN(__builtin_lsx_vrotr_w, "V4iV4iV4i", "nc")
+BUILTIN(__builtin_lsx_vrotr_d, "V2LLiV2LLiV2LLi", "nc")
+
+BUILTIN(__builtin_lsx_vrotri_b, "V16cV16cIi", "nc")
+BUILTIN(__builtin_lsx_vrotri_h, "V8sV8sIi", "nc")
+BUILTIN(__builtin_lsx_vrotri_w, "V4iV4iIi", "nc")
+BUILTIN(__builtin_lsx_vrotri_d, "V2LLiV2LLiIi", "nc")
+
+BUILTIN(__builtin_lsx_vld, "V16Scv*Ii", "nc")
+
+BUILTIN(__builtin_lsx_vst, "vV16Scv*Ii", "nc")
+
+BUILTIN(__builtin_lsx_bz_v, "iV16Uc", "nc")
+
+BUILTIN(__builtin_lsx_bnz_v, "iV16Uc", "nc")
+
+BUILTIN(__builtin_lsx_bz_b, "iV16Uc", "nc")
+BUILTIN(__builtin_lsx_bz_h, "iV8Us", "nc")
+BUILTIN(__builtin_lsx_bz_w, "iV4Ui", "nc")
+BUILTIN(__builtin_lsx_bz_d, "iV2ULLi", "nc")
+
+BUILTIN(__builtin_lsx_bnz_b, "iV16Uc", "nc")
+BUILTIN(__builtin_lsx_bnz_h, "iV8Us", "nc")
+BUILTIN(__builtin_lsx_bnz_w, "iV4Ui", "nc")
+BUILTIN(__builtin_lsx_bnz_d, "iV2ULLi", "nc")
+
+//LoongArch LASX
+
+BUILTIN(__builtin_lasx_xvfmadd_s, "V8fV8fV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfmadd_d, "V4dV4dV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfmsub_s, "V8fV8fV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfmsub_d, "V4dV4dV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfnmadd_s, "V8fV8fV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfnmadd_d, "V4dV4dV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfnmsub_s, "V8fV8fV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfnmsub_d, "V4dV4dV4dV4d", "nc")
+
+
+BUILTIN(__builtin_lasx_xvsll_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvsll_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvsll_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvsll_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvslli_b, "V32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvslli_h, "V16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvslli_w, "V8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvslli_d, "V4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvsra_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvsra_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvsra_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvsra_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrai_b, "V32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrai_h, "V16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrai_w, "V8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrai_d, "V4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrar_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvsrar_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvsrar_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvsrar_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrari_b, "V32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrari_h, "V16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrari_w, "V8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrari_d, "V4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrl_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvsrl_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvsrl_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvsrl_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrli_b, "V32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrli_h, "V16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrli_w, "V8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrli_d, "V4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrlr_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvsrlr_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvsrlr_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvsrlr_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrlri_b, "V32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrlri_h, "V16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrlri_w, "V8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrlri_d, "V4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvbitclr_b, "V32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvbitclr_h, "V16UsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvbitclr_w, "V8UiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvbitclr_d, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvbitclri_b, "V32UcV32UcIUi", "nc")
+BUILTIN(__builtin_lasx_xvbitclri_h, "V16UsV16UsIUi", "nc")
+BUILTIN(__builtin_lasx_xvbitclri_w, "V8UiV8UiIUi", "nc")
+BUILTIN(__builtin_lasx_xvbitclri_d, "V4ULLiV4ULLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvbitset_b, "V32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvbitset_h, "V16UsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvbitset_w, "V8UiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvbitset_d, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvbitseti_b, "V32UcV32UcIUi", "nc")
+BUILTIN(__builtin_lasx_xvbitseti_h, "V16UsV16UsIUi", "nc")
+BUILTIN(__builtin_lasx_xvbitseti_w, "V8UiV8UiIUi", "nc")
+BUILTIN(__builtin_lasx_xvbitseti_d, "V4ULLiV4ULLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvbitrev_b, "V32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvbitrev_h, "V16UsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvbitrev_w, "V8UiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvbitrev_d, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvbitrevi_b, "V32UcV32UcIUi", "nc")
+BUILTIN(__builtin_lasx_xvbitrevi_h, "V16UsV16UsIUi", "nc")
+BUILTIN(__builtin_lasx_xvbitrevi_w, "V8UiV8UiIUi", "nc")
+BUILTIN(__builtin_lasx_xvbitrevi_d, "V4ULLiV4ULLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvadd_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvadd_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvadd_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvadd_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvaddi_bu, "V32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvaddi_hu, "V16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvaddi_wu, "V8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvaddi_du, "V4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvsub_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvsub_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvsub_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvsub_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsubi_bu, "V32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvsubi_hu, "V16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvsubi_wu, "V8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvsubi_du, "V4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvmax_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvmax_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvmax_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmax_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmaxi_b, "V32ScV32ScIi", "nc")
+BUILTIN(__builtin_lasx_xvmaxi_h, "V16SsV16SsIi", "nc")
+BUILTIN(__builtin_lasx_xvmaxi_w, "V8SiV8SiIi", "nc")
+BUILTIN(__builtin_lasx_xvmaxi_d, "V4SLLiV4SLLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xvmax_bu, "V32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvmax_hu, "V16UsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvmax_wu, "V8UiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvmax_du, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmaxi_bu, "V32UcV32UcIi", "nc")
+BUILTIN(__builtin_lasx_xvmaxi_hu, "V16UsV16UsIi", "nc")
+BUILTIN(__builtin_lasx_xvmaxi_wu, "V8UiV8UiIi", "nc")
+BUILTIN(__builtin_lasx_xvmaxi_du, "V4ULLiV4ULLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xvmin_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvmin_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvmin_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmin_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmini_b, "V32ScV32ScIi", "nc")
+BUILTIN(__builtin_lasx_xvmini_h, "V16SsV16SsIi", "nc")
+BUILTIN(__builtin_lasx_xvmini_w, "V8SiV8SiIi", "nc")
+BUILTIN(__builtin_lasx_xvmini_d, "V4SLLiV4SLLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xvmin_bu, "V32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvmin_hu, "V16UsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvmin_wu, "V8UiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvmin_du, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmini_bu, "V32UcV32UcIi", "nc")
+BUILTIN(__builtin_lasx_xvmini_hu, "V16UsV16UsIi", "nc")
+BUILTIN(__builtin_lasx_xvmini_wu, "V8UiV8UiIi", "nc")
+BUILTIN(__builtin_lasx_xvmini_du, "V4ULLiV4ULLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xvseq_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvseq_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvseq_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvseq_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvseqi_b, "V32ScV32ScISi", "nc")
+BUILTIN(__builtin_lasx_xvseqi_h, "V16SsV16SsISi", "nc")
+BUILTIN(__builtin_lasx_xvseqi_w, "V8SiV8SiISi", "nc")
+BUILTIN(__builtin_lasx_xvseqi_d, "V4SLLiV4SLLiISi", "nc")
+
+BUILTIN(__builtin_lasx_xvslt_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvslt_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvslt_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvslt_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvslti_b, "V32ScV32ScISi", "nc")
+BUILTIN(__builtin_lasx_xvslti_h, "V16SsV16SsISi", "nc")
+BUILTIN(__builtin_lasx_xvslti_w, "V8SiV8SiISi", "nc")
+BUILTIN(__builtin_lasx_xvslti_d, "V4SLLiV4SLLiISi", "nc")
+
+BUILTIN(__builtin_lasx_xvslt_bu, "V32ScV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvslt_hu, "V16SsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvslt_wu, "V8SiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvslt_du, "V4SLLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvslti_bu, "V32ScV32UcIUi", "nc")
+BUILTIN(__builtin_lasx_xvslti_hu, "V16SsV16UsIUi", "nc")
+BUILTIN(__builtin_lasx_xvslti_wu, "V8SiV8UiIUi", "nc")
+BUILTIN(__builtin_lasx_xvslti_du, "V4SLLiV4ULLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvsle_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvsle_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvsle_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvsle_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvslei_b, "V32ScV32ScISi", "nc")
+BUILTIN(__builtin_lasx_xvslei_h, "V16SsV16SsISi", "nc")
+BUILTIN(__builtin_lasx_xvslei_w, "V8SiV8SiISi", "nc")
+BUILTIN(__builtin_lasx_xvslei_d, "V4SLLiV4SLLiISi", "nc")
+
+BUILTIN(__builtin_lasx_xvsle_bu, "V32ScV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvsle_hu, "V16SsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvsle_wu, "V8SiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvsle_du, "V4SLLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvslei_bu, "V32ScV32UcIUi", "nc")
+BUILTIN(__builtin_lasx_xvslei_hu, "V16SsV16UsIUi", "nc")
+BUILTIN(__builtin_lasx_xvslei_wu, "V8SiV8UiIUi", "nc")
+BUILTIN(__builtin_lasx_xvslei_du, "V4SLLiV4ULLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvsat_b, "V32ScV32ScIUi", "nc")
+BUILTIN(__builtin_lasx_xvsat_h, "V16SsV16SsIUi", "nc")
+BUILTIN(__builtin_lasx_xvsat_w, "V8SiV8SiIUi", "nc")
+BUILTIN(__builtin_lasx_xvsat_d, "V4SLLiV4SLLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvsat_bu, "V32UcV32UcIUi", "nc")
+BUILTIN(__builtin_lasx_xvsat_hu, "V16UsV16UsIUi", "nc")
+BUILTIN(__builtin_lasx_xvsat_wu, "V8UiV8UiIUi", "nc")
+BUILTIN(__builtin_lasx_xvsat_du, "V4ULLiV4ULLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvadda_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvadda_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvadda_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvadda_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsadd_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvsadd_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvsadd_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvsadd_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsadd_bu, "V32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvsadd_hu, "V16UsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvsadd_wu, "V8UiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvsadd_du, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvavg_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvavg_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvavg_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvavg_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvavg_bu, "V32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvavg_hu, "V16UsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvavg_wu, "V8UiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvavg_du, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvavgr_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvavgr_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvavgr_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvavgr_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvavgr_bu, "V32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvavgr_hu, "V16UsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvavgr_wu, "V8UiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvavgr_du, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvssub_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvssub_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvssub_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvssub_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvssub_bu, "V32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvssub_hu, "V16UsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvssub_wu, "V8UiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvssub_du, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvabsd_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvabsd_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvabsd_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvabsd_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvabsd_bu, "V32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvabsd_hu, "V16UsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvabsd_wu, "V8UiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvabsd_du, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmul_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvmul_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvmul_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmul_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmadd_b, "V32ScV32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvmadd_h, "V16SsV16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvmadd_w, "V8SiV8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmadd_d, "V4SLLiV4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmsub_b, "V32ScV32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvmsub_h, "V16SsV16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvmsub_w, "V8SiV8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmsub_d, "V4SLLiV4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvdiv_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvdiv_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvdiv_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvdiv_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvdiv_bu, "V32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvdiv_hu, "V16UsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvdiv_wu, "V8UiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvdiv_du, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvhaddw_h_b, "V16SsV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvhaddw_w_h, "V8SiV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvhaddw_d_w, "V4SLLiV8SiV8Si", "nc")
+
+BUILTIN(__builtin_lasx_xvhaddw_hu_bu, "V16UsV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvhaddw_wu_hu, "V8UiV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvhaddw_du_wu, "V4ULLiV8UiV8Ui", "nc")
+
+BUILTIN(__builtin_lasx_xvhsubw_h_b, "V16SsV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvhsubw_w_h, "V8SiV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvhsubw_d_w, "V4SLLiV8SiV8Si", "nc")
+
+BUILTIN(__builtin_lasx_xvhsubw_hu_bu, "V16UsV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvhsubw_wu_hu, "V8UiV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvhsubw_du_wu, "V4ULLiV8UiV8Ui", "nc")
+
+BUILTIN(__builtin_lasx_xvmod_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvmod_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvmod_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmod_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmod_bu, "V32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvmod_hu, "V16UsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvmod_wu, "V8UiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvmod_du, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvrepl128vei_b, "V32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvrepl128vei_h, "V16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvrepl128vei_w, "V8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvrepl128vei_d, "V4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvpickev_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvpickev_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvpickev_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvpickev_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvpickod_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvpickod_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvpickod_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvpickod_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvilvh_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvilvh_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvilvh_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvilvh_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvilvl_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvilvl_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvilvl_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvilvl_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvpackev_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvpackev_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvpackev_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvpackev_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvpackod_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvpackod_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvpackod_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvpackod_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvshuf_b, "V32UcV32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvshuf_h, "V16sV16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvshuf_w, "V8iV8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvshuf_d, "V4LLiV4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvand_v, "V32UcV32UcV32Uc", "nc")
+
+BUILTIN(__builtin_lasx_xvandi_b, "V32UcV32UcIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvor_v, "V32UcV32UcV32Uc", "nc")
+
+BUILTIN(__builtin_lasx_xvori_b, "V32UcV32UcIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvnor_v, "V32UcV32UcV32Uc", "nc")
+
+BUILTIN(__builtin_lasx_xvnori_b, "V32UcV32UcIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvxor_v, "V32cV32cV32c", "nc")
+
+BUILTIN(__builtin_lasx_xvxori_b, "V32UcV32UcIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvbitsel_v, "V32UcV32UcV32UcV32Uc", "nc")
+
+BUILTIN(__builtin_lasx_xvbitseli_b, "V32UcV32UcV32UcIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvshuf4i_b, "V32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvshuf4i_h, "V16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvshuf4i_w, "V8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvshuf4i_d, "V4LLiV4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvreplgr2vr_b, "V32Sci", "nc")
+BUILTIN(__builtin_lasx_xvreplgr2vr_h, "V16Ssi", "nc")
+BUILTIN(__builtin_lasx_xvreplgr2vr_w, "V8Sii", "nc")
+BUILTIN(__builtin_lasx_xvreplgr2vr_d, "V4SLLiLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvpcnt_b, "V32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvpcnt_h, "V16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvpcnt_w, "V8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvpcnt_d, "V4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvclo_b, "V32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvclo_h, "V16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvclo_w, "V8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvclo_d, "V4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvclz_b, "V32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvclz_h, "V16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvclz_w, "V8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvclz_d, "V4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_caf_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_caf_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_cor_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_cor_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_cun_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_cun_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_cune_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_cune_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_cueq_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_cueq_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_ceq_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_ceq_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_cne_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_cne_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_clt_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_clt_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_cult_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_cult_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_cle_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_cle_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_cule_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_cule_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_saf_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_saf_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_sor_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_sor_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_sun_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_sun_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_sune_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_sune_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_sueq_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_sueq_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_seq_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_seq_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_sne_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_sne_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_slt_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_slt_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_sult_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_sult_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_sle_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_sle_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcmp_sule_s, "V8SiV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcmp_sule_d, "V4SLLiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfadd_s, "V8fV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfadd_d, "V4dV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfsub_s, "V8fV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfsub_d, "V4dV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfmul_s, "V8fV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfmul_d, "V4dV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfdiv_s, "V8fV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfdiv_d, "V4dV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcvt_h_s, "V16sV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfcvt_s_d, "V8fV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfmin_s, "V8fV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfmin_d, "V4dV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfmina_s, "V8fV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfmina_d, "V4dV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfmax_s, "V8fV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfmax_d, "V4dV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfmaxa_s, "V8fV8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfmaxa_d, "V4dV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfclass_s, "V8iV8f", "nc")
+BUILTIN(__builtin_lasx_xvfclass_d, "V4LLiV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfsqrt_s, "V8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfsqrt_d, "V4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfrecip_s, "V8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfrecip_d, "V4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfrint_s, "V8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfrint_d, "V4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfrsqrt_s, "V8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvfrsqrt_d, "V4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvflogb_s, "V8fV8f", "nc")
+BUILTIN(__builtin_lasx_xvflogb_d, "V4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfcvth_s_h, "V8fV16s", "nc")
+BUILTIN(__builtin_lasx_xvfcvth_d_s, "V4dV8f", "nc")
+
+BUILTIN(__builtin_lasx_xvfcvtl_s_h, "V8fV16s", "nc")
+BUILTIN(__builtin_lasx_xvfcvtl_d_s, "V4dV8f", "nc")
+
+BUILTIN(__builtin_lasx_xvftint_w_s, "V8SiV8f", "nc")
+BUILTIN(__builtin_lasx_xvftint_l_d, "V4SLLiV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvftint_wu_s, "V8UiV8f", "nc")
+BUILTIN(__builtin_lasx_xvftint_lu_d, "V4ULLiV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvftintrz_w_s, "V8SiV8f", "nc")
+BUILTIN(__builtin_lasx_xvftintrz_l_d, "V4LLiV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvftintrz_wu_s, "V8UiV8f", "nc")
+BUILTIN(__builtin_lasx_xvftintrz_lu_d, "V4ULLiV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvffint_s_w, "V8fV8Si", "nc")
+BUILTIN(__builtin_lasx_xvffint_d_l, "V4dV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvffint_s_wu, "V8fV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvffint_d_lu, "V4dV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvreplve_b, "V32cV32cUi", "nc")
+BUILTIN(__builtin_lasx_xvreplve_h, "V16sV16sUi", "nc")
+BUILTIN(__builtin_lasx_xvreplve_w, "V8iV8iUi", "nc")
+BUILTIN(__builtin_lasx_xvreplve_d, "V4LLiV4LLiUi", "nc")
+
+BUILTIN(__builtin_lasx_xvpermi_w, "V8iV8iV8iIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvandn_v, "V32UcV32UcV32Uc", "nc")
+
+BUILTIN(__builtin_lasx_xvneg_b, "V32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvneg_h, "V16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvneg_w, "V8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvneg_d, "V4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmuh_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvmuh_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvmuh_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvmuh_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmuh_bu, "V32UcV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvmuh_hu, "V16UsV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvmuh_wu, "V8UiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvmuh_du, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsllwil_d_w, "V4LLiV8SiIUi", "nc")
+BUILTIN(__builtin_lasx_xvsllwil_w_h, "V8SiV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvsllwil_h_b, "V16sV32cIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvsllwil_du_wu, "V4ULLiV8UiIUi", "nc")
+BUILTIN(__builtin_lasx_xvsllwil_wu_hu, "V8UiV16UsIUi", "nc")
+BUILTIN(__builtin_lasx_xvsllwil_hu_bu, "V16UsV32UcIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvsran_b_h, "V32ScV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvsran_h_w, "V16sV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvsran_w_d, "V8SiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvssran_b_h, "V32ScV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvssran_h_w, "V16sV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvssran_w_d, "V8SiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvssran_bu_h, "V32UcV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvssran_hu_w, "V16UsV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvssran_wu_d, "V8UiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrarn_b_h, "V32ScV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvsrarn_h_w, "V16sV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvsrarn_w_d, "V8SiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrarn_b_h, "V32ScV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvssrarn_h_w, "V16sV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvssrarn_w_d, "V8SiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrarn_bu_h, "V32UcV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvssrarn_hu_w, "V16UsV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvssrarn_wu_d, "V8UiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrln_b_h, "V32ScV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvsrln_h_w, "V16sV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvsrln_w_d, "V8SiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrln_bu_h, "V32UcV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvssrln_hu_w, "V16UsV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvssrln_wu_d, "V8UiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrlrn_b_h, "V32ScV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvsrlrn_h_w, "V16sV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvsrlrn_w_d, "V8SiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrlrn_bu_h, "V32UcV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvssrlrn_hu_w, "V16UsV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvssrlrn_wu_d, "V8UiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvfrstpi_b, "V32cV32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvfrstpi_h, "V16sV16sV16sIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvfrstp_b, "V32ScV32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvfrstp_h, "V16SsV16SsV16SsV16Ss", "nc")
+
+BUILTIN(__builtin_lasx_xvbsrl_v, "V32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvbsll_v, "V32cV32cIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvextrins_b, "V32cV32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvextrins_h, "V16sV16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvextrins_w, "V8iV8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvextrins_d, "V4LLiV4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvmskltz_b, "V32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvmskltz_h, "V16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvmskltz_w, "V8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvmskltz_d, "V4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsigncov_b, "V32ScV32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvsigncov_h, "V16SsV16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvsigncov_w, "V8SiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvsigncov_d, "V4SLLiV4SLLiV4SLLi", "nc")
+
+BUILTIN(__builtin_lasx_xvftintrne_w_s, "V8SiV8f", "nc")
+BUILTIN(__builtin_lasx_xvftintrne_l_d, "V4LLiV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvftintrp_w_s, "V8SiV8f", "nc")
+BUILTIN(__builtin_lasx_xvftintrp_l_d, "V4LLiV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvftintrm_w_s, "V8SiV8f", "nc")
+BUILTIN(__builtin_lasx_xvftintrm_l_d, "V4LLiV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvftint_w_d, "V8SiV4dV4d", "nc")
+BUILTIN(__builtin_lasx_xvffint_s_l, "V8fV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvftintrz_w_d, "V8SiV4dV4d", "nc")
+BUILTIN(__builtin_lasx_xvftintrp_w_d, "V8SiV4dV4d", "nc")
+BUILTIN(__builtin_lasx_xvftintrm_w_d, "V8SiV4dV4d", "nc")
+BUILTIN(__builtin_lasx_xvftintrne_w_d, "V8SiV4dV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvftinth_l_s, "V4LLiV8f", "nc")
+BUILTIN(__builtin_lasx_xvftintl_l_s, "V4LLiV8f", "nc")
+
+BUILTIN(__builtin_lasx_xvffinth_d_w, "V4dV8Si", "nc")
+BUILTIN(__builtin_lasx_xvffintl_d_w, "V4dV8Si", "nc")
+
+BUILTIN(__builtin_lasx_xvftintrzh_l_s, "V4LLiV8f", "nc")
+BUILTIN(__builtin_lasx_xvftintrzl_l_s, "V4LLiV8f", "nc")
+
+BUILTIN(__builtin_lasx_xvftintrph_l_s, "V4LLiV8f", "nc")
+BUILTIN(__builtin_lasx_xvftintrpl_l_s, "V4LLiV8f", "nc")
+
+BUILTIN(__builtin_lasx_xvftintrmh_l_s, "V4LLiV8f", "nc")
+BUILTIN(__builtin_lasx_xvftintrml_l_s, "V4LLiV8f", "nc")
+
+BUILTIN(__builtin_lasx_xvftintrneh_l_s, "V4LLiV8f", "nc")
+BUILTIN(__builtin_lasx_xvftintrnel_l_s, "V4LLiV8f", "nc")
+
+BUILTIN(__builtin_lasx_xvfrintrne_s, "V8SiV8f", "nc")
+BUILTIN(__builtin_lasx_xvfrintrne_d, "V4LLiV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfrintrz_s, "V8SiV8f", "nc")
+BUILTIN(__builtin_lasx_xvfrintrz_d, "V4LLiV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfrintrp_s, "V8SiV8f", "nc")
+BUILTIN(__builtin_lasx_xvfrintrp_d, "V4LLiV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvfrintrm_s, "V8SiV8f", "nc")
+BUILTIN(__builtin_lasx_xvfrintrm_d, "V4LLiV4d", "nc")
+
+BUILTIN(__builtin_lasx_xvld, "V32Scv*Ii", "nc")
+
+BUILTIN(__builtin_lasx_xvst, "vV32Scv*Ii", "nc")
+
+BUILTIN(__builtin_lasx_xvstelm_b, "vV32Scv*IiUi", "nc")
+BUILTIN(__builtin_lasx_xvstelm_h, "vV16Ssv*IiUi", "nc")
+BUILTIN(__builtin_lasx_xvstelm_w, "vV8Siv*IiUi", "nc")
+BUILTIN(__builtin_lasx_xvstelm_d, "vV4SLLiv*IiUi", "nc")
+
+BUILTIN(__builtin_lasx_xvinsve0_w, "V8iV8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvinsve0_d, "V4LLiV4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvpickve_w, "V8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvpickve_d, "V4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrlrn_b_h, "V32ScV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvssrlrn_h_w, "V16sV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvssrlrn_w_d, "V8SiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrln_b_h, "V32ScV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvssrln_h_w, "V16sV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvssrln_w_d, "V8SiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvorn_v, "V32ScV32ScV32Sc", "nc")
+
+BUILTIN(__builtin_lasx_xvldi, "V4LLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xvldx, "V32Scv*LLi", "nc")
+BUILTIN(__builtin_lasx_xvstx, "vV32Scv*LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvinsgr2vr_w, "V8SiV8SiiIUi", "nc")
+BUILTIN(__builtin_lasx_xvinsgr2vr_d, "V4SLLiV4SLLiLLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvreplve0_b, "V32ScV32Sc", "nc")
+BUILTIN(__builtin_lasx_xvreplve0_h, "V16SsV16Ss", "nc")
+BUILTIN(__builtin_lasx_xvreplve0_w, "V8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvreplve0_d, "V4SLLiV4SLLi", "nc")
+BUILTIN(__builtin_lasx_xvreplve0_q, "V32ScV32Sc", "nc")
+
+BUILTIN(__builtin_lasx_vext2xv_d_w, "V4LLiV8Si", "nc")
+BUILTIN(__builtin_lasx_vext2xv_w_h, "V8SiV16s", "nc")
+BUILTIN(__builtin_lasx_vext2xv_h_b, "V16sV32c", "nc")
+
+BUILTIN(__builtin_lasx_vext2xv_d_h, "V4LLiV16s", "nc")
+BUILTIN(__builtin_lasx_vext2xv_w_b, "V8SiV32c", "nc")
+BUILTIN(__builtin_lasx_vext2xv_d_b, "V4LLiV32c", "nc")
+
+BUILTIN(__builtin_lasx_vext2xv_du_wu, "V4LLiV8Si", "nc")
+BUILTIN(__builtin_lasx_vext2xv_wu_hu, "V8SiV16s", "nc")
+BUILTIN(__builtin_lasx_vext2xv_hu_bu, "V16sV32c", "nc")
+
+BUILTIN(__builtin_lasx_vext2xv_du_hu, "V4LLiV16s", "nc")
+BUILTIN(__builtin_lasx_vext2xv_wu_bu, "V8SiV32c", "nc")
+BUILTIN(__builtin_lasx_vext2xv_du_bu, "V4LLiV32c", "nc")
+
+BUILTIN(__builtin_lasx_xvpermi_q, "V32cV32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvpermi_d, "V4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvperm_w, "V8iV8iV8i", "nc")
+
+BUILTIN(__builtin_lasx_xvldrepl_b, "V32cv*Ii", "nc")
+BUILTIN(__builtin_lasx_xvldrepl_h, "V16sv*Ii", "nc")
+BUILTIN(__builtin_lasx_xvldrepl_w, "V8iv*Ii", "nc")
+BUILTIN(__builtin_lasx_xvldrepl_d, "V4LLiv*Ii", "nc")
+
+BUILTIN(__builtin_lasx_xvpickve2gr_w, "iV8SiIUi", "nc")
+BUILTIN(__builtin_lasx_xvpickve2gr_d, "LLiV4SLLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvpickve2gr_wu, "iV8UiIUi", "nc")
+BUILTIN(__builtin_lasx_xvpickve2gr_du, "LLiV4ULLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvaddwev_d_w, "V4LLiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvaddwev_w_h, "V8SiV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvaddwev_h_b, "V16sV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvaddwev_q_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvaddwev_d_wu, "V4LLiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvaddwev_w_hu, "V8SiV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvaddwev_h_bu, "V16sV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvaddwev_q_du, "V4LLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsubwev_d_w, "V4LLiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvsubwev_w_h, "V8SiV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvsubwev_h_b, "V16sV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvsubwev_q_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsubwev_d_wu, "V4LLiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvsubwev_w_hu, "V8SiV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvsubwev_h_bu, "V16sV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvsubwev_q_du, "V4LLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmulwev_d_w, "V4LLiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmulwev_w_h, "V8SiV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvmulwev_h_b, "V16sV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvmulwev_q_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmulwev_d_wu, "V4LLiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvmulwev_w_hu, "V8SiV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvmulwev_h_bu, "V16sV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvmulwev_q_du, "V4LLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvaddwod_d_w, "V4LLiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvaddwod_w_h, "V8SiV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvaddwod_h_b, "V16sV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvaddwod_q_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvaddwod_d_wu, "V4LLiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvaddwod_w_hu, "V8SiV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvaddwod_h_bu, "V16sV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvaddwod_q_du, "V4LLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsubwod_d_w, "V4LLiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvsubwod_w_h, "V8SiV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvsubwod_h_b, "V16sV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvsubwod_q_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvsubwod_d_wu, "V4LLiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvsubwod_w_hu, "V8SiV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvsubwod_h_bu, "V16sV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvsubwod_q_du, "V4LLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmulwod_d_w, "V4LLiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmulwod_w_h, "V8SiV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvmulwod_h_b, "V16sV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvmulwod_q_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmulwod_d_wu, "V4LLiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvmulwod_w_hu, "V8SiV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvmulwod_h_bu, "V16sV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvmulwod_q_du, "V4LLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvaddwev_d_wu_w, "V4LLiV8UiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvaddwev_w_hu_h, "V8SiV16UsV16s", "nc")
+BUILTIN(__builtin_lasx_xvaddwev_h_bu_b, "V16sV32UcV32c", "nc")
+BUILTIN(__builtin_lasx_xvaddwev_q_du_d, "V4LLiV4ULLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmulwev_d_wu_w, "V4LLiV8UiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmulwev_w_hu_h, "V8SiV16UsV16s", "nc")
+BUILTIN(__builtin_lasx_xvmulwev_h_bu_b, "V16sV32UcV32c", "nc")
+BUILTIN(__builtin_lasx_xvmulwev_q_du_d, "V4LLiV4ULLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvaddwod_d_wu_w, "V4LLiV8UiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvaddwod_w_hu_h, "V8SiV16UsV16s", "nc")
+BUILTIN(__builtin_lasx_xvaddwod_h_bu_b, "V16sV32UcV32c", "nc")
+BUILTIN(__builtin_lasx_xvaddwod_q_du_d, "V4LLiV4ULLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmulwod_d_wu_w, "V4LLiV8UiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmulwod_w_hu_h, "V8SiV16UsV16s", "nc")
+BUILTIN(__builtin_lasx_xvmulwod_h_bu_b, "V16sV32UcV32c", "nc")
+BUILTIN(__builtin_lasx_xvmulwod_q_du_d, "V4LLiV4ULLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvhaddw_q_d, "V4LLiV4LLiV4LLi", "nc")
+BUILTIN(__builtin_lasx_xvhsubw_q_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvhaddw_qu_du, "V4ULLiV4ULLiV4ULLi", "nc")
+BUILTIN(__builtin_lasx_xvhsubw_qu_du, "V4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmaddwev_d_w, "V4LLiV4LLiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmaddwev_w_h, "V8SiV8SiV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvmaddwev_h_b, "V16sV16sV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvmaddwev_q_d, "V4LLiV4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmaddwev_d_wu, "V4ULLiV4ULLiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvmaddwev_w_hu, "V8UiV8UiV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvmaddwev_h_bu, "V16UsV16UsV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvmaddwev_q_du, "V4ULLiV4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmaddwod_d_w, "V4LLiV4LLiV8SiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmaddwod_w_h, "V8SiV8SiV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvmaddwod_h_b, "V16sV16sV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvmaddwod_q_d, "V4LLiV4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmaddwod_d_wu, "V4ULLiV4ULLiV8UiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvmaddwod_w_hu, "V8UiV8UiV16UsV16Us", "nc")
+BUILTIN(__builtin_lasx_xvmaddwod_h_bu, "V16UsV16UsV32UcV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvmaddwod_q_du, "V4ULLiV4ULLiV4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmaddwev_d_wu_w, "V4LLiV4LLiV8UiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmaddwev_w_hu_h, "V8SiV8SiV16UsV16s", "nc")
+BUILTIN(__builtin_lasx_xvmaddwev_h_bu_b, "V16sV16sV32UcV32c", "nc")
+BUILTIN(__builtin_lasx_xvmaddwev_q_du_d, "V4LLiV4LLiV4ULLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmaddwod_d_wu_w, "V4LLiV4LLiV8UiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvmaddwod_w_hu_h, "V8SiV8SiV16UsV16s", "nc")
+BUILTIN(__builtin_lasx_xvmaddwod_h_bu_b, "V16sV16sV32UcV32c", "nc")
+BUILTIN(__builtin_lasx_xvmaddwod_q_du_d, "V4LLiV4LLiV4ULLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvrotr_b, "V32cV32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvrotr_h, "V16sV16sV16s", "nc")
+BUILTIN(__builtin_lasx_xvrotr_w, "V8iV8iV8i", "nc")
+BUILTIN(__builtin_lasx_xvrotr_d, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvadd_q, "V4LLiV4LLiV4LLi", "nc")
+BUILTIN(__builtin_lasx_xvsub_q, "V4LLiV4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvmskgez_b, "V32cV32c", "nc")
+BUILTIN(__builtin_lasx_xvmsknz_b, "V16sV16s", "nc")
+
+BUILTIN(__builtin_lasx_xvexth_d_w, "V4LLiV8Si", "nc")
+BUILTIN(__builtin_lasx_xvexth_w_h, "V8SiV16s", "nc")
+BUILTIN(__builtin_lasx_xvexth_h_b, "V16sV32c", "nc")
+BUILTIN(__builtin_lasx_xvexth_q_d, "V4LLiV4LLi", "nc")
+
+BUILTIN(__builtin_lasx_xvexth_du_wu, "V4ULLiV8Ui", "nc")
+BUILTIN(__builtin_lasx_xvexth_wu_hu, "V8UiV16Us", "nc")
+BUILTIN(__builtin_lasx_xvexth_hu_bu, "V16UsV32Uc", "nc")
+BUILTIN(__builtin_lasx_xvexth_qu_du, "V4ULLiV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvrotri_b, "V32cV32cIi", "nc")
+BUILTIN(__builtin_lasx_xvrotri_h, "V16sV16sIi", "nc")
+BUILTIN(__builtin_lasx_xvrotri_w, "V8iV8iIi", "nc")
+BUILTIN(__builtin_lasx_xvrotri_d, "V4LLiV4LLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrlni_b_h, "V32cV32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrlni_h_w, "V16sV16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrlni_w_d, "V8iV8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrlni_d_q, "V4LLiV4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrlrni_b_h, "V32cV32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrlrni_h_w, "V16sV16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrlrni_w_d, "V8iV8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrlrni_d_q, "V4LLiV4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrlni_b_h, "V32cV32cV32cIi", "nc")
+BUILTIN(__builtin_lasx_xvssrlni_h_w, "V16sV16sV16sIi", "nc")
+BUILTIN(__builtin_lasx_xvssrlni_w_d, "V8iV8iV8iIi", "nc")
+BUILTIN(__builtin_lasx_xvssrlni_d_q, "V4LLiV4LLiV4LLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrlni_bu_h, "V32cV32cV32cIi", "nc")
+BUILTIN(__builtin_lasx_xvssrlni_hu_w, "V16sV16sV16sIi", "nc")
+BUILTIN(__builtin_lasx_xvssrlni_wu_d, "V8iV8iV8iIi", "nc")
+BUILTIN(__builtin_lasx_xvssrlni_du_q, "V4LLiV4LLiV4LLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrlrni_b_h, "V32cV32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvssrlrni_h_w, "V16sV16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvssrlrni_w_d, "V8iV8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvssrlrni_d_q, "V4LLiV4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrlrni_bu_h, "V32cV32cV32cIi", "nc")
+BUILTIN(__builtin_lasx_xvssrlrni_hu_w, "V16sV16sV16sIi", "nc")
+BUILTIN(__builtin_lasx_xvssrlrni_wu_d, "V8iV8iV8iIi", "nc")
+BUILTIN(__builtin_lasx_xvssrlrni_du_q, "V4LLiV4LLiV4LLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrani_b_h, "V32cV32cV32cIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrani_h_w, "V16sV16sV16sIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrani_w_d, "V8iV8iV8iIUi", "nc")
+BUILTIN(__builtin_lasx_xvsrani_d_q, "V4LLiV4LLiV4LLiIUi", "nc")
+
+BUILTIN(__builtin_lasx_xvsrarni_b_h, "V32cV32cV32cIi", "nc")
+BUILTIN(__builtin_lasx_xvsrarni_h_w, "V16sV16sV16sIi", "nc")
+BUILTIN(__builtin_lasx_xvsrarni_w_d, "V8iV8iV8iIi", "nc")
+BUILTIN(__builtin_lasx_xvsrarni_d_q, "V4LLiV4LLiV4LLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrani_b_h, "V32cV32cV32cIi", "nc")
+BUILTIN(__builtin_lasx_xvssrani_h_w, "V16sV16sV16sIi", "nc")
+BUILTIN(__builtin_lasx_xvssrani_w_d, "V8iV8iV8iIi", "nc")
+BUILTIN(__builtin_lasx_xvssrani_d_q, "V4LLiV4LLiV4LLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrani_bu_h, "V32cV32cV32cIi", "nc")
+BUILTIN(__builtin_lasx_xvssrani_hu_w, "V16sV16sV16sIi", "nc")
+BUILTIN(__builtin_lasx_xvssrani_wu_d, "V8iV8iV8iIi", "nc")
+BUILTIN(__builtin_lasx_xvssrani_du_q, "V4LLiV4LLiV4LLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrarni_b_h, "V32cV32cV32cIi", "nc")
+BUILTIN(__builtin_lasx_xvssrarni_h_w, "V16sV16sV16sIi", "nc")
+BUILTIN(__builtin_lasx_xvssrarni_w_d, "V8iV8iV8iIi", "nc")
+BUILTIN(__builtin_lasx_xvssrarni_d_q, "V4LLiV4LLiV4LLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xvssrarni_bu_h, "V32cV32cV32cIi", "nc")
+BUILTIN(__builtin_lasx_xvssrarni_hu_w, "V16sV16sV16sIi", "nc")
+BUILTIN(__builtin_lasx_xvssrarni_wu_d, "V8iV8iV8iIi", "nc")
+BUILTIN(__builtin_lasx_xvssrarni_du_q, "V4LLiV4LLiV4LLiIi", "nc")
+
+BUILTIN(__builtin_lasx_xbz_v, "iV32Uc", "nc")
+
+BUILTIN(__builtin_lasx_xbnz_v, "iV32Uc", "nc")
+
+BUILTIN(__builtin_lasx_xbz_b, "iV32Uc", "nc")
+BUILTIN(__builtin_lasx_xbz_h, "iV16Us", "nc")
+BUILTIN(__builtin_lasx_xbz_w, "iV8Ui", "nc")
+BUILTIN(__builtin_lasx_xbz_d, "iV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xbnz_b, "iV32Uc", "nc")
+BUILTIN(__builtin_lasx_xbnz_h, "iV16Us", "nc")
+BUILTIN(__builtin_lasx_xbnz_w, "iV8Ui", "nc")
+BUILTIN(__builtin_lasx_xbnz_d, "iV4ULLi", "nc")
+
+BUILTIN(__builtin_lasx_xvextl_q_d, "V4LLiV4LLi", "nc")
+BUILTIN(__builtin_lasx_xvextl_qu_du, "V4LLiV4ULLi", "nc")
+
+
+// LoongArch BASE
+
+BUILTIN(__builtin_loongarch_cpucfg, "UiUi", "nc")
+BUILTIN(__builtin_loongarch_csrrd, "UiIUi", "nc")
+BUILTIN(__builtin_loongarch_dcsrrd, "ULiIULi", "nc")
+BUILTIN(__builtin_loongarch_csrwr, "UiUiIUi", "nc")
+BUILTIN(__builtin_loongarch_dcsrwr, "ULiULiIULi", "nc")
+BUILTIN(__builtin_loongarch_csrxchg, "UiUiUiIUi", "nc")
+BUILTIN(__builtin_loongarch_dcsrxchg, "ULiULiULiIULi", "nc")
+BUILTIN(__builtin_loongarch_iocsrrd_b, "UiUi", "nc")
+BUILTIN(__builtin_loongarch_iocsrrd_h, "UiUi", "nc")
+BUILTIN(__builtin_loongarch_iocsrrd_w, "UiUi", "nc")
+BUILTIN(__builtin_loongarch_iocsrrd_d, "ULiUi", "nc")
+BUILTIN(__builtin_loongarch_iocsrwr_b, "vUiUi", "nc")
+BUILTIN(__builtin_loongarch_iocsrwr_h, "vUiUi", "nc")
+BUILTIN(__builtin_loongarch_iocsrwr_w, "vUiUi", "nc")
+BUILTIN(__builtin_loongarch_iocsrwr_d, "vULiUi", "nc")
+BUILTIN(__builtin_loongarch_cacop, "viUii", "nc")
+BUILTIN(__builtin_loongarch_dcacop, "viULiLi", "nc")
+BUILTIN(__builtin_loongarch_crc_w_b_w, "iii", "nc")
+BUILTIN(__builtin_loongarch_crc_w_h_w, "iii", "nc")
+BUILTIN(__builtin_loongarch_crc_w_w_w, "iii", "nc")
+BUILTIN(__builtin_loongarch_crc_w_d_w, "iLii", "nc")
+BUILTIN(__builtin_loongarch_crcc_w_b_w, "iii", "nc")
+BUILTIN(__builtin_loongarch_crcc_w_h_w, "iii", "nc")
+BUILTIN(__builtin_loongarch_crcc_w_w_w, "iii", "nc")
+BUILTIN(__builtin_loongarch_crcc_w_d_w, "iLii", "nc")
+BUILTIN(__builtin_loongarch_tlbclr, "v", "nc")
+BUILTIN(__builtin_loongarch_tlbflush, "v", "nc")
+BUILTIN(__builtin_loongarch_tlbfill, "v", "nc")
+BUILTIN(__builtin_loongarch_tlbrd, "v", "nc")
+BUILTIN(__builtin_loongarch_tlbwr, "v", "nc")
+BUILTIN(__builtin_loongarch_tlbsrch, "v", "nc")
+BUILTIN(__builtin_loongarch_syscall, "vIULi", "nc")
+BUILTIN(__builtin_loongarch_break, "vIULi", "nc")
+BUILTIN(__builtin_loongarch_asrtle_d, "vLiLi", "nc")
+BUILTIN(__builtin_loongarch_asrtgt_d, "vLiLi", "nc")
+BUILTIN(__builtin_loongarch_dbar, "vIULi", "nc")
+BUILTIN(__builtin_loongarch_ibar, "vIULi", "nc")
+#undef BUILTIN
diff --git a/include/clang/Basic/DiagnosticDriverKinds.td b/include/clang/Basic/DiagnosticDriverKinds.td
index 3efedbe0..abcdec74 100644
--- a/include/clang/Basic/DiagnosticDriverKinds.td
+++ b/include/clang/Basic/DiagnosticDriverKinds.td
@@ -193,6 +193,8 @@ def err_drv_force_crash : Error<
   "failing because %select{environment variable 'FORCE_CLANG_DIAGNOSTICS_CRASH' is set|'-gen-reproducer' is used}0">;
 def err_drv_invalid_mfloat_abi : Error<
   "invalid float ABI '%0'">;
+def err_drv_invalid_loongarch_mfpu : Error<
+  "invalid loongarch FPU value '%0'. Please specify FPU = 64,32 or none">;
 def err_drv_invalid_mtp : Error<
   "invalid thread pointer reading mode '%0'">;
 def err_drv_missing_arg_mtp : Error<
diff --git a/include/clang/Basic/TargetBuiltins.h b/include/clang/Basic/TargetBuiltins.h
index d4ea8e98..5f3851af 100644
--- a/include/clang/Basic/TargetBuiltins.h
+++ b/include/clang/Basic/TargetBuiltins.h
@@ -145,6 +145,16 @@ namespace clang {
   };
   } // namespace RISCV
 
+  /// LoongArch builtins
+  namespace LoongArch {
+  enum {
+    LastTIBuiltin = clang::Builtin::FirstTSBuiltin-1,
+#define BUILTIN(ID, TYPE, ATTRS) BI##ID,
+#include "clang/Basic/BuiltinsLoongArch.def"
+    LastTSBuiltin
+  };
+  } // namespace LoongArch
+
   /// Flags to identify the types for overloaded Neon builtins.
   ///
   /// These must be kept in sync with the flags in utils/TableGen/NeonEmitter.h.
@@ -336,7 +346,8 @@ namespace clang {
        PPC::LastTSBuiltin, NVPTX::LastTSBuiltin, AMDGPU::LastTSBuiltin,
        X86::LastTSBuiltin, VE::LastTSBuiltin, RISCV::LastTSBuiltin,
        Hexagon::LastTSBuiltin, Mips::LastTSBuiltin, XCore::LastTSBuiltin,
-       SystemZ::LastTSBuiltin, WebAssembly::LastTSBuiltin});
+       SystemZ::LastTSBuiltin, WebAssembly::LastTSBuiltin,
+       LoongArch::LastTSBuiltin});
 
 } // end namespace clang.
 
diff --git a/include/clang/Basic/TargetCXXABI.def b/include/clang/Basic/TargetCXXABI.def
index 9501cca7..8ea4bece 100644
--- a/include/clang/Basic/TargetCXXABI.def
+++ b/include/clang/Basic/TargetCXXABI.def
@@ -88,6 +88,12 @@ ITANIUM_CXXABI(GenericAArch64, "aarch64")
 ///   - representation of member function pointers adjusted as in ARM.
 ITANIUM_CXXABI(GenericMIPS, "mips")
 
+/// The generic LoongArch ABI is a modified version of the Itanium ABI.
+///
+/// At the moment, only change from the generic ABI in this case is:
+///   - representation of member function pointers adjusted as in ARM.
+ITANIUM_CXXABI(GenericLoongArch, "loongarch")
+
 /// The WebAssembly ABI is a modified version of the Itanium ABI.
 ///
 /// The changes from the Itanium ABI are:
diff --git a/include/clang/Basic/TargetCXXABI.h b/include/clang/Basic/TargetCXXABI.h
index e727f85e..507cf580 100644
--- a/include/clang/Basic/TargetCXXABI.h
+++ b/include/clang/Basic/TargetCXXABI.h
@@ -102,6 +102,9 @@ public:
     case GenericAArch64:
       return T.isAArch64();
 
+    case GenericLoongArch:
+      return T.isLoongArch();
+
     case GenericMIPS:
       return T.isMIPS();
 
@@ -166,6 +169,7 @@ public:
     case Fuchsia:
     case GenericARM:
     case GenericAArch64:
+    case GenericLoongArch:
     case GenericMIPS:
       // TODO: ARM-style pointers to member functions put the discriminator in
       //       the this adjustment, so they don't require functions to have any
@@ -250,6 +254,7 @@ public:
     case GenericItanium:
     case iOS:   // old iOS compilers did not follow this rule
     case Microsoft:
+    case GenericLoongArch:
     case GenericMIPS:
     case XL:
       return true;
@@ -288,6 +293,7 @@ public:
     case GenericAArch64:
     case GenericARM:
     case iOS:
+    case GenericLoongArch:
     case GenericMIPS:
     case XL:
       return UseTailPaddingUnlessPOD03;
diff --git a/include/clang/Driver/Options.td b/include/clang/Driver/Options.td
index e0d21584..bbca2ae5 100644
--- a/include/clang/Driver/Options.td
+++ b/include/clang/Driver/Options.td
@@ -176,6 +176,8 @@ def m_x86_Features_Group : OptionGroup<"<x86 features group>">,
                            Group<m_Group>, Flags<[CoreOption]>, DocName<"X86">;
 def m_riscv_Features_Group : OptionGroup<"<riscv features group>">,
                              Group<m_Group>, DocName<"RISCV">;
+def m_loongarch_Features_Group : OptionGroup<"<loongarch features group>">,
+                                 Group<m_Group>, DocName<"LoongArch">;
 
 def m_libc_Group : OptionGroup<"<m libc group>">, Group<m_mips_Features_Group>,
                    Flags<[HelpHidden]>;
@@ -3315,12 +3317,15 @@ def mcmodel_EQ_medany : Flag<["-"], "mcmodel=medany">, Group<m_riscv_Features_Gr
 def menable_experimental_extensions : Flag<["-"], "menable-experimental-extensions">, Group<m_Group>,
   HelpText<"Enable use of experimental RISC-V extensions.">;
 
-def munaligned_access : Flag<["-"], "munaligned-access">, Group<m_arm_Features_Group>,
-  HelpText<"Allow memory accesses to be unaligned (AArch32/AArch64 only)">;
-def mno_unaligned_access : Flag<["-"], "mno-unaligned-access">, Group<m_arm_Features_Group>,
-  HelpText<"Force all memory accesses to be aligned (AArch32/AArch64 only)">;
+def munaligned_access : Flag<["-"], "munaligned-access">, Group<m_Group>,
+  HelpText<"Allow memory accesses to be unaligned">;
+def mno_unaligned_access : Flag<["-"], "mno-unaligned-access">, Group<m_Group>,
+  HelpText<"Force all memory accesses to be aligned">;
 def mstrict_align : Flag<["-"], "mstrict-align">, Alias<mno_unaligned_access>, Flags<[CC1Option,HelpHidden]>,
   HelpText<"Force all memory accesses to be aligned (same as mno-unaligned-access)">;
+def mno_strict_align : Flag<["-"], "mno-strict-align">, Group<m_loongarch_Features_Group>,
+  Flags<[CC1Option,HelpHidden]>, Alias<munaligned_access>,
+  HelpText<"Allow memory accesses to be unaligned (LoongArch only, same as munaligned-access)">;
 def mno_thumb : Flag<["-"], "mno-thumb">, Group<m_arm_Features_Group>;
 def mrestrict_it: Flag<["-"], "mrestrict-it">, Group<m_arm_Features_Group>,
   HelpText<"Disallow generation of deprecated IT blocks for ARMv8. It is on by default for ARMv8 Thumb mode.">;
@@ -3616,6 +3621,14 @@ def mstack_protector_guard_reg_EQ : Joined<["-"], "mstack-protector-guard-reg=">
 def mfentry : Flag<["-"], "mfentry">, HelpText<"Insert calls to fentry at function entry (x86/SystemZ only)">,
   Flags<[CC1Option]>, Group<m_Group>,
   MarshallingInfoFlag<CodeGenOpts<"CallFEntry">>;
+def mlsx : Flag<["-"], "mlsx">, Group<m_loongarch_Features_Group>,
+  HelpText<"Use LARCH Loongson LSX instructions.">;
+def mno_lsx : Flag<["-"], "mno-lsx">, Group<m_loongarch_Features_Group>,
+  HelpText<"Disable LARCH Loongson LSX instructions.">;
+def mlasx : Flag<["-"], "mlasx">, Group<m_loongarch_Features_Group>,
+  HelpText<"Enable LARCH Loongson LASX instructions.">;
+def mno_lasx : Flag<["-"], "mno-lasx">, Group<m_loongarch_Features_Group>,
+  HelpText<"Disable LARCH Loongson LASX instructions.">;
 def mnop_mcount : Flag<["-"], "mnop-mcount">, HelpText<"Generate mcount/__fentry__ calls as nops. To activate they need to be patched in.">,
   Flags<[CC1Option]>, Group<m_Group>,
   MarshallingInfoFlag<CodeGenOpts<"MNopMCount">>;
diff --git a/include/clang/Sema/Sema.h b/include/clang/Sema/Sema.h
index 4b609f4b..c6ee1053 100644
--- a/include/clang/Sema/Sema.h
+++ b/include/clang/Sema/Sema.h
@@ -12749,6 +12749,9 @@ private:
   bool CheckRISCVLMUL(CallExpr *TheCall, unsigned ArgNum);
   bool CheckRISCVBuiltinFunctionCall(const TargetInfo &TI, unsigned BuiltinID,
                                      CallExpr *TheCall);
+  bool CheckLoongArchBuiltinFunctionCall(const TargetInfo &TI,
+                                         unsigned BuiltinID,
+                                         CallExpr *TheCall);
 
   bool SemaBuiltinVAStart(unsigned BuiltinID, CallExpr *TheCall);
   bool SemaBuiltinVAStartARMMicrosoft(CallExpr *Call);
diff --git a/include/clang/module.modulemap b/include/clang/module.modulemap
index 2b73cd54..efc6aa21 100644
--- a/include/clang/module.modulemap
+++ b/include/clang/module.modulemap
@@ -42,6 +42,7 @@ module Clang_Basic {
   textual header "Basic/BuiltinsHexagon.def"
   textual header "Basic/BuiltinsHexagonDep.def"
   textual header "Basic/BuiltinsHexagonMapCustomDep.def"
+  textual header "Basic/BuiltinsLoongArch.def"
   textual header "Basic/BuiltinsMips.def"
   textual header "Basic/BuiltinsNEON.def"
   textual header "Basic/BuiltinsNVPTX.def"
diff --git a/lib/AST/ASTContext.cpp b/lib/AST/ASTContext.cpp
index e4b3827b..e56cd4ce 100644
--- a/lib/AST/ASTContext.cpp
+++ b/lib/AST/ASTContext.cpp
@@ -901,6 +901,7 @@ CXXABI *ASTContext::createCXXABI(const TargetInfo &T) {
   case TargetCXXABI::iOS:
   case TargetCXXABI::WatchOS:
   case TargetCXXABI::GenericAArch64:
+  case TargetCXXABI::GenericLoongArch:
   case TargetCXXABI::GenericMIPS:
   case TargetCXXABI::GenericItanium:
   case TargetCXXABI::WebAssembly:
@@ -11651,6 +11652,7 @@ MangleContext *ASTContext::createMangleContext(const TargetInfo *T) {
   case TargetCXXABI::GenericAArch64:
   case TargetCXXABI::GenericItanium:
   case TargetCXXABI::GenericARM:
+  case TargetCXXABI::GenericLoongArch:
   case TargetCXXABI::GenericMIPS:
   case TargetCXXABI::iOS:
   case TargetCXXABI::WebAssembly:
diff --git a/lib/Basic/CMakeLists.txt b/lib/Basic/CMakeLists.txt
index 40de9433..ac6bc570 100644
--- a/lib/Basic/CMakeLists.txt
+++ b/lib/Basic/CMakeLists.txt
@@ -78,6 +78,7 @@ add_clang_library(clangBasic
   Targets/Hexagon.cpp
   Targets/Lanai.cpp
   Targets/Le64.cpp
+  Targets/LoongArch.cpp
   Targets/M68k.cpp
   Targets/MSP430.cpp
   Targets/Mips.cpp
diff --git a/lib/Basic/Targets.cpp b/lib/Basic/Targets.cpp
index 994a491c..35f577f4 100644
--- a/lib/Basic/Targets.cpp
+++ b/lib/Basic/Targets.cpp
@@ -22,6 +22,7 @@
 #include "Targets/Hexagon.h"
 #include "Targets/Lanai.h"
 #include "Targets/Le64.h"
+#include "Targets/LoongArch.h"
 #include "Targets/M68k.h"
 #include "Targets/MSP430.h"
 #include "Targets/Mips.h"
@@ -325,6 +326,25 @@ TargetInfo *AllocateTarget(const llvm::Triple &Triple,
   case llvm::Triple::le64:
     return new Le64TargetInfo(Triple, Opts);
 
+#if 0
+  //TODO: support it in future
+  case llvm::Triple::loongarch32:
+    switch (os) {
+    case llvm::Triple::Linux:
+      return new LinuxTargetInfo<LoongArchTargetInfo>(Triple, Opts);
+    default:
+      return new LoongArchTargetInfo(Triple, Opts);
+    }
+#endif
+
+  case llvm::Triple::loongarch64:
+    switch (os) {
+    case llvm::Triple::Linux:
+      return new LinuxTargetInfo<LoongArchTargetInfo>(Triple, Opts);
+    default:
+      return new LoongArchTargetInfo(Triple, Opts);
+    }
+
   case llvm::Triple::ppc:
     if (Triple.isOSDarwin())
       return new DarwinPPC32TargetInfo(Triple, Opts);
diff --git a/lib/Basic/Targets/LoongArch.cpp b/lib/Basic/Targets/LoongArch.cpp
new file mode 100644
index 00000000..f94d9f09
--- /dev/null
+++ b/lib/Basic/Targets/LoongArch.cpp
@@ -0,0 +1,149 @@
+//===--- LoongArch.cpp - Implement LoongArch target feature support -----------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements LoongArch TargetInfo objects.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArch.h"
+#include "Targets.h"
+#include "clang/Basic/Diagnostic.h"
+#include "clang/Basic/MacroBuilder.h"
+#include "clang/Basic/TargetBuiltins.h"
+#include "llvm/ADT/StringSwitch.h"
+
+using namespace clang;
+using namespace clang::targets;
+
+const Builtin::Info LoongArchTargetInfo::BuiltinInfo[] = {
+#define BUILTIN(ID, TYPE, ATTRS)                                               \
+  {#ID, TYPE, ATTRS, nullptr, ALL_LANGUAGES, nullptr},
+#define LIBBUILTIN(ID, TYPE, ATTRS, HEADER)                                    \
+  {#ID, TYPE, ATTRS, HEADER, ALL_LANGUAGES, nullptr},
+#include "clang/Basic/BuiltinsLoongArch.def"
+};
+
+bool LoongArchTargetInfo::processorSupportsGPR64() const {
+  return llvm::StringSwitch<bool>(CPU)
+      .Cases("la464", "generic-la64", true)
+      .Default(false);
+  return false;
+}
+
+static constexpr llvm::StringLiteral ValidCPUNames[] = {
+    {"la464"}, {"generic-la64"}, {"generic-la32"}};
+
+bool LoongArchTargetInfo::isValidCPUName(StringRef Name) const {
+  return llvm::find(ValidCPUNames, Name) != std::end(ValidCPUNames);
+}
+
+void LoongArchTargetInfo::fillValidCPUList(
+    SmallVectorImpl<StringRef> &Values) const {
+  Values.append(std::begin(ValidCPUNames), std::end(ValidCPUNames));
+}
+
+void LoongArchTargetInfo::getTargetDefines(const LangOptions &Opts,
+                                      MacroBuilder &Builder) const {
+  Builder.defineMacro("__loongarch__");
+
+  if (ABI == "lp64d" || ABI == "lp64s" || ABI == "lp64f") {
+    Builder.defineMacro("__loongarch_lp64");
+    Builder.defineMacro("__loongarch64");
+    Builder.defineMacro("_ABILP64", "3");
+    Builder.defineMacro("_LOONGARCH_SIM", "_ABILP64");
+  } else
+    llvm_unreachable("Invalid ABI.");
+
+  Builder.defineMacro("__REGISTER_PREFIX__", "");
+
+  if (HasLSX)
+    Builder.defineMacro("__loongarch_sx", Twine(1));
+
+  if (HasLASX)
+    Builder.defineMacro("__loongarch_asx", Twine(1));
+
+  Builder.defineMacro("_LOONGARCH_SZPTR", Twine(getPointerWidth(0)));
+  Builder.defineMacro("_LOONGARCH_SZINT", Twine(getIntWidth()));
+  Builder.defineMacro("_LOONGARCH_SZLONG", Twine(getLongWidth()));
+
+  Builder.defineMacro("_LOONGARCH_TUNE", "\"" + CPU + "\"");
+  Builder.defineMacro("_LOONGARCH_TUNE_" + StringRef(CPU).upper());
+
+  Builder.defineMacro("_LOONGARCH_ARCH", "\"" + getTriple().getArchName() + "\"");
+  Builder.defineMacro("_LOONGARCH_ARCH_" + StringRef(getTriple().getArchName()).upper());
+
+  Builder.defineMacro("__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1");
+  Builder.defineMacro("__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2");
+  Builder.defineMacro("__GCC_HAVE_SYNC_COMPARE_AND_SWAP_4");
+
+  // 32-bit loongarch processors don't have the necessary ll.d/sc.d instructions
+  // found in 64-bit processors.
+  if (ABI == "lp64d" || ABI == "lp64s" || ABI == "lp64f")
+    Builder.defineMacro("__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8");
+
+  // Bit-width of general purpose registers.
+  Builder.defineMacro("__loongarch_grlen", Twine(getRegisterWidth()));
+
+  // Bit-width of floating-point registers. The possible values for
+  // this macro are 0, 32 and 64. 0 if there is no FPU.
+  if (HasBasicD || HasBasicF)
+    Builder.defineMacro("__loongarch_frlen", HasBasicD ? "64" : "32");
+  else
+    Builder.defineMacro("__loongarch_frlen", "0");
+
+  // FIXME: Defined if floating-point/extended ABI type is single or double.
+  if (ABI == "lp64d" || ABI == "lp64f")
+    Builder.defineMacro("__loongarch_hard_float");
+
+  // FIXME: Defined if floating-point/extended ABI type is double.
+  if (ABI == "lp64d")
+    Builder.defineMacro("__loongarch_double_float");
+
+  // FIXME: Defined if floating-point/extended ABI type is single.
+  if (ABI == "lp64f")
+    Builder.defineMacro("__loongarch_single_float");
+
+  // FIXME: Defined if floating-point/extended ABI type is soft.
+  if (ABI == "lp64s")
+    Builder.defineMacro("__loongarch_soft_float");
+}
+
+bool LoongArchTargetInfo::hasFeature(StringRef Feature) const {
+  return llvm::StringSwitch<bool>(Feature)
+      .Case("lsx", HasLSX)
+      .Case("lasx", HasLASX)
+      .Case("d", HasBasicD)
+      .Case("f", HasBasicF)
+      .Default(false);
+}
+
+ArrayRef<Builtin::Info> LoongArchTargetInfo::getTargetBuiltins() const {
+  return llvm::makeArrayRef(BuiltinInfo, clang::LoongArch::LastTSBuiltin -
+                                             Builtin::FirstTSBuiltin);
+}
+
+bool LoongArchTargetInfo::validateTarget(DiagnosticsEngine &Diags) const {
+  // 64-bit ABI's require 64-bit CPU's.
+  if (!processorSupportsGPR64() &&
+      (ABI == "lp64d" || ABI == "lp64s" || ABI == "lp64f")) {
+    Diags.Report(diag::err_target_unsupported_abi) << ABI << CPU;
+    return false;
+  }
+
+  // FIXME: It's valid to use lp64d/lp64s/lp64f on a loongarch32 triple
+  // but the backend can't handle this yet. It's better to fail here than on the
+  // backend assertion.
+  if (getTriple().isLoongArch32() &&
+      (ABI == "lp64d" || ABI == "lp64s" || ABI == "lp64f")) {
+    Diags.Report(diag::err_target_unsupported_abi_for_triple)
+        << ABI << getTriple().str();
+    return false;
+  }
+
+  return true;
+}
diff --git a/lib/Basic/Targets/LoongArch.h b/lib/Basic/Targets/LoongArch.h
new file mode 100644
index 00000000..6e854fd7
--- /dev/null
+++ b/lib/Basic/Targets/LoongArch.h
@@ -0,0 +1,352 @@
+//===--- LoongArch.h - Declare LoongArch target feature support -----------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file declares LoongArch TargetInfo objects.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CLANG_LIB_BASIC_TARGETS_LOONGARCH_H
+#define LLVM_CLANG_LIB_BASIC_TARGETS_LOONGARCH_H
+
+#include "clang/Basic/TargetInfo.h"
+#include "clang/Basic/TargetOptions.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/Support/Compiler.h"
+
+namespace clang {
+namespace targets {
+
+class LLVM_LIBRARY_VISIBILITY LoongArchTargetInfo : public TargetInfo {
+  void setDataLayout() {
+    StringRef Layout;
+
+    if (ABI == "ilp32d" || ABI == "ilp32f" || ABI == "ilp32s")
+      // TODO
+      llvm_unreachable("Unimplemented ABI");
+    else if (ABI == "lp64d" || ABI == "lp64s" || ABI == "lp64f")
+      Layout = "m:e-i8:8:32-i16:16:32-i64:64-n32:64-S128";
+    else
+      llvm_unreachable("Invalid ABI");
+
+    resetDataLayout(("e-" + Layout).str());
+  }
+
+  static const Builtin::Info BuiltinInfo[];
+  std::string CPU;
+  bool HasLSX;
+  bool HasLASX;
+  bool HasBasicF;
+  bool HasBasicD;
+
+protected:
+  std::string ABI;
+
+public:
+  LoongArchTargetInfo(const llvm::Triple &Triple, const TargetOptions &)
+      : TargetInfo(Triple), HasLSX(false), HasLASX(false), HasBasicF(false),
+        HasBasicD(false) {
+    TheCXXABI.set(TargetCXXABI::GenericLoongArch);
+
+    if (Triple.isLoongArch32())
+      // TODO
+      llvm_unreachable("Unimplemented triple");
+    else
+      setABI("lp64d");
+
+    // Currently, CPU only supports 'la464' in LA.
+    if ( ABI == "lp64d")
+      CPU = "la464";
+  }
+
+  bool processorSupportsGPR64() const;
+
+  StringRef getABI() const override { return ABI; }
+
+  bool setABI(const std::string &Name) override {
+    if (Name == "ilp32d" || Name == "ilp32f" || Name == "ilp32s") {
+      // TODO
+      llvm_unreachable("Unimplemented ABI");
+    }
+
+    if (Name == "lp64d" || Name == "lp64s" || Name == "lp64f") {
+      setLP64ABITypes();
+      ABI = Name;
+      return true;
+    }
+    return false;
+  }
+
+  void setLP64ABITypes() {
+    LongDoubleWidth = LongDoubleAlign = 128;
+    LongDoubleFormat = &llvm::APFloat::IEEEquad();
+    MaxAtomicPromoteWidth = MaxAtomicInlineWidth = 64;
+    SuitableAlign = 128;
+    Int64Type = SignedLong;
+    IntMaxType = Int64Type;
+    LongWidth = LongAlign = 64;
+    PointerWidth = PointerAlign = 64;
+    PtrDiffType = SignedLong;
+    SizeType = UnsignedLong;
+  }
+
+  bool isValidCPUName(StringRef Name) const override;
+  void fillValidCPUList(SmallVectorImpl<StringRef> &Values) const override;
+
+  bool setCPU(const std::string &Name) override {
+    CPU = Name;
+    return isValidCPUName(Name);
+  }
+
+  const std::string &getCPU() const { return CPU; }
+  bool
+  initFeatureMap(llvm::StringMap<bool> &Features, DiagnosticsEngine &Diags,
+                 StringRef CPU,
+                 const std::vector<std::string> &FeaturesVec) const override {
+#if 0
+    if (CPU.empty())
+      CPU = getCPU();
+    Features[CPU] = true;
+#else
+//    if (CPU == "la464")
+//      Features["loongarch64"] = true;
+
+//FIXME: we need this?
+//    if (CPU == "la464")
+//      Features["64bit"] = true;
+#endif
+    return TargetInfo::initFeatureMap(Features, Diags, CPU, FeaturesVec);
+  }
+
+  void getTargetDefines(const LangOptions &Opts,
+                        MacroBuilder &Builder) const override;
+
+  ArrayRef<Builtin::Info> getTargetBuiltins() const override;
+
+  bool hasFeature(StringRef Feature) const override;
+
+  bool hasBitIntType() const override { return true; }
+
+  BuiltinVaListKind getBuiltinVaListKind() const override {
+    return TargetInfo::VoidPtrBuiltinVaList;
+  }
+
+  ArrayRef<const char *> getGCCRegNames() const override {
+    static const char *const GCCRegNames[] = {
+        // CPU register names
+        // Must match second column of GCCRegAliases
+        "$r0", "$r1", "$r2", "$r3", "$r4", "$r5", "$r6", "$r7", "$r8", "$r9",
+        "$r10", "$r11", "$r12", "$r13", "$r14", "$r15", "$r16", "$r17", "$r18",
+        "$r19", "$r20", "$r21", "$r22", "$r23", "$r24", "$r25", "$r26", "$r27",
+        "$r28", "$r29", "$r30", "$r31",
+        // Floating point register names
+        "$f0", "$f1", "$f2", "$f3", "$f4", "$f5", "$f6", "$f7", "$f8", "$f9",
+        "$f10", "$f11", "$f12", "$f13", "$f14", "$f15", "$f16", "$f17", "$f18",
+        "$f19", "$f20", "$f21", "$f22", "$f23", "$f24", "$f25", "$f26", "$f27",
+        "$f28", "$f29", "$f30", "$f31",
+        // condition register names
+        "$fcc0", "$fcc1", "$fcc2", "$fcc3", "$fcc4", "$fcc5", "$fcc6", "$fcc7",
+        // LSX register names
+        "$vr0", "$vr1", "$vr2", "$vr3", "$vr4", "$vr5", "$vr6", "$vr7", "$vr8",
+        "$vr9", "$vr10", "$vr11", "$vr12", "$vr13", "$vr14", "$vr15", "$vr16",
+        "$vr17", "$vr18", "$vr19", "$vr20", "$vr21", "$vr22", "$vr23", "$vr24",
+        "$vr25", "$vr26", "$vr27", "$vr28", "$vr29", "$vr30", "$vr31",
+        // LASX register names
+        "$xr0", "$xr1", "$xr2", "$xr3", "$xr4", "$xr5", "$xr6", "$xr7", "$xr8",
+        "$xr9", "$xr10", "$xr11", "$xr12", "$xr13", "$xr14", "$xr15", "$xr16",
+        "$xr17", "$xr18", "$xr19", "$xr20", "$xr21", "$xr22", "$xr23", "$xr24",
+        "$xr25", "$xr26", "$xr27", "$xr28", "$xr29", "$xr30", "$xr31"
+
+    };
+    return llvm::makeArrayRef(GCCRegNames);
+  }
+
+  bool validateAsmConstraint(const char *&Name,
+                             TargetInfo::ConstraintInfo &Info) const override {
+    switch (*Name) {
+    default:
+      return false;
+    case 'r': // CPU registers.
+    case 'f': // floating-point registers.
+      Info.setAllowsRegister();
+      return true;
+    case 'l': // Signed 16-bit constant
+    case 'I': // Signed 12-bit constant
+    case 'K': // Unsigned 12-bit constant
+    case 'J': // Integer 0
+    case 'G': // Floating-point 0
+      return true;
+    case 'm': // Memory address with 12-bit offset
+    case 'R': // An address that can be used in a non-macro load or store
+      Info.setAllowsMemory();
+      return true;
+    case 'Z':
+      if (Name[1] == 'C'        // Memory address with 16-bit and 4 bytes aligned offset
+          || Name[1] == 'B' ) { // Memory address with 0 offset
+        Info.setAllowsMemory();
+        Name++; // Skip over 'Z'.
+        return true;
+      }
+      return false;
+    }
+  }
+
+  std::string convertConstraint(const char *&Constraint) const override {
+    std::string R;
+    switch (*Constraint) {
+    case 'Z': // Two-character constraint; add "^" hint for later parsing.
+      if (Constraint[1] == 'C' || Constraint[1] == 'B') {
+        R = std::string("^") + std::string(Constraint, 2);
+        Constraint++;
+        return R;
+      }
+      break;
+    }
+    return TargetInfo::convertConstraint(Constraint);
+  }
+
+  const char *getClobbers() const override {
+#if 0
+    // In GCC, $1 is not widely used in generated code (it's used only in a few
+    // specific situations), so there is no real need for users to add it to
+    // the clobbers list if they want to use it in their inline assembly code.
+    //
+    // In LLVM, $1 is treated as a normal GPR and is always allocatable during
+    // code generation, so using it in inline assembly without adding it to the
+    // clobbers list can cause conflicts between the inline assembly code and
+    // the surrounding generated code.
+    //
+    // Another problem is that LLVM is allowed to choose $1 for inline assembly
+    // operands, which will conflict with the ".set at" assembler option (which
+    // we use only for inline assembly, in order to maintain compatibility with
+    // GCC) and will also conflict with the user's usage of $1.
+    //
+    // The easiest way to avoid these conflicts and keep $1 as an allocatable
+    // register for generated code is to automatically clobber $1 for all inline
+    // assembly code.
+    //
+    // FIXME: We should automatically clobber $1 only for inline assembly code
+    // which actually uses it. This would allow LLVM to use $1 for inline
+    // assembly operands if the user's assembly code doesn't use it.
+    return "~{$1}";
+#endif
+    return "";
+  }
+
+  bool handleTargetFeatures(std::vector<std::string> &Features,
+                            DiagnosticsEngine &Diags) override {
+    HasBasicF = false;
+    HasBasicD = false;
+
+    for (const auto &Feature : Features) {
+      if (Feature == "+lsx")
+        HasLSX = true;
+      else if (Feature == "+lasx") {
+        HasLASX = true;
+        HasLSX = true;
+      } else if (Feature == "+f")
+        HasBasicF = true;
+      else if (Feature == "+d")
+        HasBasicD = true;
+    }
+
+    setDataLayout();
+
+    return true;
+  }
+
+  int getEHDataRegisterNumber(unsigned RegNo) const override {
+    if (RegNo == 0)
+      return 4;
+    if (RegNo == 1)
+      return 5;
+    return -1;
+  }
+
+  bool isCLZForZeroUndef() const override { return false; }
+
+  ArrayRef<TargetInfo::GCCRegAlias> getGCCRegAliases() const override {
+    static const TargetInfo::GCCRegAlias GCCRegAliases[] = {
+        {{"zero", "$zero", "r0", "$0"}, "$r0"},
+        {{"ra", "$ra", "r1", "$1"}, "$r1"},
+        {{"tp", "$tp", "r2", "$2"}, "$r2"},
+        {{"sp", "$sp", "r3", "$3"}, "$r3"},
+        {{"a0", "$a0", "r4", "$4", "v0"}, "$r4"},
+        {{"a1", "$a1", "r5", "$5", "v1"}, "$r5"},
+        {{"a2", "$a2", "r6", "$6"}, "$r6"},
+        {{"a3", "$a3", "r7", "$7"}, "$r7"},
+        {{"a4", "$a4", "r8", "$8"}, "$r8"},
+        {{"a5", "$a5", "r9", "$9"}, "$r9"},
+        {{"a6", "$a6", "r10", "$10"}, "$r10"},
+        {{"a7", "$a7", "r11", "$11"}, "$r11"},
+        {{"t0", "$t0", "r12", "$12"}, "$r12"},
+        {{"t1", "$t1", "r13", "$13"}, "$r13"},
+        {{"t2", "$t2", "r14", "$14"}, "$r14"},
+        {{"t3", "$t3", "r15", "$15"}, "$r15"},
+        {{"t4", "$t4", "r16", "$16"}, "$r16"},
+        {{"t5", "$t5", "r17", "$17"}, "$r17"},
+        {{"t6", "$t6", "r18", "$18"}, "$r18"},
+        {{"t7", "$t7", "r19", "$19"}, "$r19"},
+        {{"t8", "$t8", "r20", "$20"}, "$r20"},
+        //{{"x", "$x", "r21", "$21"}, "$r21"},
+        {{"fp", "$fp", "r22", "$22"}, "$r22"},
+        {{"s0", "$s0", "r23", "$23"}, "$r23"},
+        {{"s1", "$s1", "r24", "$24"}, "$r24"},
+        {{"s2", "$s2", "r25", "$25"}, "$r25"},
+        {{"s3", "$s3", "r26", "$26"}, "$r26"},
+        {{"s4", "$s4", "r27", "$27"}, "$r27"},
+        {{"s5", "$s5", "r28", "$28"}, "$r28"},
+        {{"s6", "$s6", "r29", "$29"}, "$r29"},
+        {{"s7", "$s7", "r30", "$30"}, "$r30"},
+        {{"s8", "$s8", "r31", "$31"}, "$r31"},
+        {{"fa0", "$fa0", "f0"}, "$f0"},
+        {{"fa1", "$fa1", "f1"}, "$f1"},
+        {{"fa2", "$fa2", "f2"}, "$f2"},
+        {{"fa3", "$fa3", "f3"}, "$f3"},
+        {{"fa4", "$fa4", "f4"}, "$f4"},
+        {{"fa5", "$fa5", "f5"}, "$f5"},
+        {{"fa6", "$fa6", "f6"}, "$f6"},
+        {{"fa7", "$fa7", "f7"}, "$f7"},
+        {{"ft0", "$ft0", "f8"}, "$f8"},
+        {{"ft1", "$ft1", "f9"}, "$f9"},
+        {{"ft2", "$ft2", "f10"}, "$f10"},
+        {{"ft3", "$ft3", "f11"}, "$f11"},
+        {{"ft4", "$ft4", "f12"}, "$f12"},
+        {{"ft5", "$ft5", "f13"}, "$f13"},
+        {{"ft6", "$ft6", "f14"}, "$f14"},
+        {{"ft7", "$ft7", "f15"}, "$f15"},
+        {{"ft8", "$ft8", "f16"}, "$f16"},
+        {{"ft9", "$ft9", "f17"}, "$f17"},
+        {{"ft10", "$ft10", "f18"}, "$f18"},
+        {{"ft11", "$ft11", "f19"}, "$f19"},
+        {{"ft12", "$ft12", "f20"}, "$f20"},
+        {{"ft13", "$ft13", "f21"}, "$f21"},
+        {{"ft14", "$ft14", "f22"}, "$f22"},
+        {{"ft15", "$ft15", "f23"}, "$f23"},
+        {{"fs0", "$fs0", "f24"}, "$f24"},
+        {{"fs1", "$fs1", "f25"}, "$f25"},
+        {{"fs2", "$fs2", "f26"}, "$f26"},
+        {{"fs3", "$fs3", "f27"}, "$f27"},
+        {{"fs4", "$fs4", "f28"}, "$f28"},
+        {{"fs5", "$fs5", "f29"}, "$f29"},
+        {{"fs6", "$fs6", "f30"}, "$f30"},
+        {{"fs7", "$fs7", "f31"}, "$f31"},
+    };
+    return llvm::makeArrayRef(GCCRegAliases);
+  }
+
+  bool hasInt128Type() const override {
+    return (ABI == "lp64d" || ABI == "lp64s" || ABI == "lp64f") ||
+           getTargetOpts().ForceEnableInt128;
+  }
+
+  bool validateTarget(DiagnosticsEngine &Diags) const override;
+};
+} // namespace targets
+} // namespace clang
+
+#endif // LLVM_CLANG_LIB_BASIC_TARGETS_LOONGARCH_H
diff --git a/lib/CodeGen/CodeGenFunction.cpp b/lib/CodeGen/CodeGenFunction.cpp
index d7393526..50e16389 100644
--- a/lib/CodeGen/CodeGenFunction.cpp
+++ b/lib/CodeGen/CodeGenFunction.cpp
@@ -560,6 +560,29 @@ bool CodeGenFunction::AlwaysEmitXRayTypedEvents() const {
               XRayInstrKind::Typed);
 }
 
+llvm::Constant *
+CodeGenFunction::EncodeAddrForUseInPrologue(llvm::Function *F,
+                                            llvm::Constant *Addr) {
+  // Addresses stored in prologue data can't require run-time fixups and must
+  // be PC-relative. Run-time fixups are undesirable because they necessitate
+  // writable text segments, which are unsafe. And absolute addresses are
+  // undesirable because they break PIE mode.
+
+  // Add a layer of indirection through a private global. Taking its address
+  // won't result in a run-time fixup, even if Addr has linkonce_odr linkage.
+  auto *GV = new llvm::GlobalVariable(CGM.getModule(), Addr->getType(),
+                                      /*isConstant=*/true,
+                                      llvm::GlobalValue::PrivateLinkage, Addr);
+
+  // Create a PC-relative address.
+  auto *GOTAsInt = llvm::ConstantExpr::getPtrToInt(GV, IntPtrTy);
+  auto *FuncAsInt = llvm::ConstantExpr::getPtrToInt(F, IntPtrTy);
+  auto *PCRelAsInt = llvm::ConstantExpr::getSub(GOTAsInt, FuncAsInt);
+  return (IntPtrTy == Int32Ty)
+             ? PCRelAsInt
+             : llvm::ConstantExpr::getTrunc(PCRelAsInt, Int32Ty);
+}
+
 llvm::Value *
 CodeGenFunction::DecodeAddrUsedInPrologue(llvm::Value *F,
                                           llvm::Value *EncodedAddr) {
@@ -903,13 +926,12 @@ void CodeGenFunction::StartFunction(GlobalDecl GD, QualType RetTy,
           FD->getType(), EST_None);
       llvm::Constant *FTRTTIConst =
           CGM.GetAddrOfRTTIDescriptor(ProtoTy, /*ForEH=*/true);
-      llvm::GlobalVariable *FTRTTIProxy =
-          CGM.GetOrCreateRTTIProxyGlobalVariable(FTRTTIConst);
-      llvm::LLVMContext &Ctx = Fn->getContext();
-      llvm::MDBuilder MDB(Ctx);
-      Fn->setMetadata(llvm::LLVMContext::MD_func_sanitize,
-                      MDB.createRTTIPointerPrologue(PrologueSig, FTRTTIProxy));
-      CGM.addCompilerUsedGlobal(FTRTTIProxy);
+      llvm::Constant *FTRTTIConstEncoded =
+          EncodeAddrForUseInPrologue(Fn, FTRTTIConst);
+      llvm::Constant *PrologueStructElems[] = {PrologueSig, FTRTTIConstEncoded};
+      llvm::Constant *PrologueStructConst =
+          llvm::ConstantStruct::getAnon(PrologueStructElems, /*Packed=*/true);
+      Fn->setPrologueData(PrologueStructConst);
     }
   }
 
diff --git a/lib/CodeGen/CodeGenFunction.h b/lib/CodeGen/CodeGenFunction.h
index 046b249b..df99cd9a 100644
--- a/lib/CodeGen/CodeGenFunction.h
+++ b/lib/CodeGen/CodeGenFunction.h
@@ -2351,6 +2351,10 @@ public:
   /// XRay typed event handling calls.
   bool AlwaysEmitXRayTypedEvents() const;
 
+  /// Encode an address into a form suitable for use in a function prologue.
+  llvm::Constant *EncodeAddrForUseInPrologue(llvm::Function *F,
+                                             llvm::Constant *Addr);
+
   /// Decode an address used in a function prologue, encoded by \c
   /// EncodeAddrForUseInPrologue.
   llvm::Value *DecodeAddrUsedInPrologue(llvm::Value *F,
diff --git a/lib/CodeGen/CodeGenModule.cpp b/lib/CodeGen/CodeGenModule.cpp
index 58eef1b0..6c95dd61 100644
--- a/lib/CodeGen/CodeGenModule.cpp
+++ b/lib/CodeGen/CodeGenModule.cpp
@@ -82,6 +82,7 @@ static CGCXXABI *createCXXABI(CodeGenModule &CGM) {
   case TargetCXXABI::GenericARM:
   case TargetCXXABI::iOS:
   case TargetCXXABI::WatchOS:
+  case TargetCXXABI::GenericLoongArch:
   case TargetCXXABI::GenericMIPS:
   case TargetCXXABI::GenericItanium:
   case TargetCXXABI::WebAssembly:
@@ -1826,22 +1827,6 @@ CodeGenModule::getMostBaseClasses(const CXXRecordDecl *RD) {
   return MostBases.takeVector();
 }
 
-llvm::GlobalVariable *
-CodeGenModule::GetOrCreateRTTIProxyGlobalVariable(llvm::Constant *Addr) {
-  auto It = RTTIProxyMap.find(Addr);
-  if (It != RTTIProxyMap.end())
-    return It->second;
-
-  auto *FTRTTIProxy = new llvm::GlobalVariable(
-      TheModule, Addr->getType(),
-      /*isConstant=*/true, llvm::GlobalValue::PrivateLinkage, Addr,
-      "__llvm_rtti_proxy");
-  FTRTTIProxy->setUnnamedAddr(llvm::GlobalValue::UnnamedAddr::Global);
-
-  RTTIProxyMap[Addr] = FTRTTIProxy;
-  return FTRTTIProxy;
-}
-
 void CodeGenModule::SetLLVMFunctionAttributesForDefinition(const Decl *D,
                                                            llvm::Function *F) {
   llvm::AttrBuilder B(F->getContext());
diff --git a/lib/CodeGen/CodeGenModule.h b/lib/CodeGen/CodeGenModule.h
index 3a9d542e..a8a63c8d 100644
--- a/lib/CodeGen/CodeGenModule.h
+++ b/lib/CodeGen/CodeGenModule.h
@@ -561,8 +561,6 @@ private:
   MetadataTypeMap VirtualMetadataIdMap;
   MetadataTypeMap GeneralizedMetadataIdMap;
 
-  llvm::DenseMap<const llvm::Constant *, llvm::GlobalVariable *> RTTIProxyMap;
-
 public:
   CodeGenModule(ASTContext &C, const HeaderSearchOptions &headersearchopts,
                 const PreprocessorOptions &ppopts,
@@ -1413,9 +1411,6 @@ public:
   std::vector<const CXXRecordDecl *>
   getMostBaseClasses(const CXXRecordDecl *RD);
 
-  llvm::GlobalVariable *
-  GetOrCreateRTTIProxyGlobalVariable(llvm::Constant *Addr);
-
   /// Get the declaration of std::terminate for the platform.
   llvm::FunctionCallee getTerminateFn();
 
diff --git a/lib/CodeGen/ItaniumCXXABI.cpp b/lib/CodeGen/ItaniumCXXABI.cpp
index 2979d92c..5ef50a16 100644
--- a/lib/CodeGen/ItaniumCXXABI.cpp
+++ b/lib/CodeGen/ItaniumCXXABI.cpp
@@ -533,6 +533,9 @@ CodeGen::CGCXXABI *CodeGen::CreateItaniumCXXABI(CodeGenModule &CGM) {
     return new ItaniumCXXABI(CGM, /*UseARMMethodPtrABI=*/true,
                              /*UseARMGuardVarABI=*/true);
 
+  case TargetCXXABI::GenericLoongArch:
+    return new ItaniumCXXABI(CGM, /*UseARMMethodPtrABI=*/true);
+
   case TargetCXXABI::GenericMIPS:
     return new ItaniumCXXABI(CGM, /*UseARMMethodPtrABI=*/true);
 
diff --git a/lib/CodeGen/TargetInfo.cpp b/lib/CodeGen/TargetInfo.cpp
index d83bc9e5..2ce2573a 100644
--- a/lib/CodeGen/TargetInfo.cpp
+++ b/lib/CodeGen/TargetInfo.cpp
@@ -11242,6 +11242,557 @@ public:
 };
 } // namespace
 
+//===----------------------------------------------------------------------===//
+// LoongArch ABI Implementation
+//===----------------------------------------------------------------------===//
+
+namespace {
+class LoongArchABIInfo : public DefaultABIInfo {
+private:
+  // Size of the integer ('r') registers in bits.
+  unsigned GRLen;
+  // Size of the floating point ('f') registers in bits. Note that the target
+  // ISA might have a wider FRLen than the selected ABI.
+  unsigned FRLen;
+  static const int NumArgGPRs = 8;
+  static const int NumArgFPRs = 8;
+  bool detectFPCCEligibleStructHelper(QualType Ty, CharUnits CurOff,
+                                      llvm::Type *&Field1Ty,
+                                      CharUnits &Field1Off,
+                                      llvm::Type *&Field2Ty,
+                                      CharUnits &Field2Off) const;
+
+public:
+  LoongArchABIInfo(CodeGen::CodeGenTypes &CGT, unsigned GRLen, unsigned FRLen)
+      : DefaultABIInfo(CGT), GRLen(GRLen), FRLen(FRLen) {}
+
+  // DefaultABIInfo's classifyReturnType and classifyArgumentType are
+  // non-virtual, but computeInfo is virtual, so we overload it.
+  void computeInfo(CGFunctionInfo &FI) const override;
+
+  ABIArgInfo classifyArgumentType(QualType Ty, bool IsFixed, int &ArgGPRsLeft,
+                                  int &ArgFPRsLeft) const;
+  ABIArgInfo classifyReturnType(QualType RetTy) const;
+
+  uint64_t MinABIStackAlignInBytes = 8;
+  uint64_t StackAlignInBytes = 16;
+  llvm::Type* HandleAggregates(QualType Ty, uint64_t TySize) const;
+  llvm::Type* getPaddingType(uint64_t Align, uint64_t Offset) const;
+  void CoerceToIntArgs(uint64_t TySize,
+                       SmallVectorImpl<llvm::Type *> &ArgList) const;
+
+  Address EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
+                    QualType Ty) const override;
+
+  ABIArgInfo extendType(QualType Ty) const;
+
+  bool detectFPCCEligibleStruct(QualType Ty, llvm::Type *&Field1Ty,
+                                CharUnits &Field1Off, llvm::Type *&Field2Ty,
+                                CharUnits &Field2Off, int &NeededArgGPRs,
+                                int &NeededArgFPRs) const;
+  ABIArgInfo coerceAndExpandFPCCEligibleStruct(llvm::Type *Field1Ty,
+                                               CharUnits Field1Off,
+                                               llvm::Type *Field2Ty,
+                                               CharUnits Field2Off) const;
+};
+} // end anonymous namespace
+
+void LoongArchABIInfo::computeInfo(CGFunctionInfo &FI) const {
+  QualType RetTy = FI.getReturnType();
+  if (!getCXXABI().classifyReturnType(FI))
+    FI.getReturnInfo() = classifyReturnType(RetTy);
+
+  // IsRetIndirect is true if classifyArgumentType indicated the value should
+  // be passed indirect or if the type size is greater than 2*grlen.
+  bool IsRetIndirect = FI.getReturnInfo().getKind() == ABIArgInfo::Indirect ||
+                       getContext().getTypeSize(RetTy) > (2 * GRLen);
+
+  // We must track the number of GPRs used in order to conform to the LoongArch
+  // ABI, as integer scalars passed in registers should have signext/zeroext
+  // when promoted, but are anyext if passed on the stack. As GPR usage is
+  // different for variadic arguments, we must also track whether we are
+  // examining a vararg or not.
+  int ArgGPRsLeft = IsRetIndirect ? NumArgGPRs - 1 : NumArgGPRs;
+  int ArgFPRsLeft = FRLen ? NumArgFPRs : 0;
+  int NumFixedArgs = FI.getNumRequiredArgs();
+
+  int ArgNum = 0;
+  for (auto &ArgInfo : FI.arguments()) {
+    bool IsFixed = ArgNum < NumFixedArgs;
+    ArgInfo.info =
+        classifyArgumentType(ArgInfo.type, IsFixed, ArgGPRsLeft, ArgFPRsLeft);
+    ArgNum++;
+  }
+}
+
+// Returns true if the struct is a potential candidate for the floating point
+// calling convention. If this function returns true, the caller is
+// responsible for checking that if there is only a single field then that
+// field is a float.
+bool LoongArchABIInfo::detectFPCCEligibleStructHelper(QualType Ty, CharUnits CurOff,
+                                                  llvm::Type *&Field1Ty,
+                                                  CharUnits &Field1Off,
+                                                  llvm::Type *&Field2Ty,
+                                                  CharUnits &Field2Off) const {
+  bool IsInt = Ty->isIntegralOrEnumerationType();
+  bool IsFloat = Ty->isRealFloatingType();
+
+  if (IsInt || IsFloat) {
+    uint64_t Size = getContext().getTypeSize(Ty);
+    if (IsInt && Size > GRLen)
+      return false;
+    // Can't be eligible if larger than the FP registers. Half precision isn't
+    // currently supported on LoongArch and the ABI hasn't been confirmed, so
+    // default to the integer ABI in that case.
+    if (IsFloat && (Size > FRLen || Size < 32))
+      return false;
+    // Can't be eligible if an integer type was already found (int+int pairs
+    // are not eligible).
+    if (IsInt && Field1Ty && Field1Ty->isIntegerTy())
+      return false;
+    if (!Field1Ty) {
+      Field1Ty = CGT.ConvertType(Ty);
+      Field1Off = CurOff;
+      return true;
+    }
+    if (!Field2Ty) {
+      Field2Ty = CGT.ConvertType(Ty);
+      Field2Off = CurOff;
+      return true;
+    }
+    return false;
+  }
+
+  if (auto CTy = Ty->getAs<ComplexType>()) {
+    if (Field1Ty)
+      return false;
+    QualType EltTy = CTy->getElementType();
+    if (getContext().getTypeSize(EltTy) > FRLen)
+      return false;
+    Field1Ty = CGT.ConvertType(EltTy);
+    Field1Off = CurOff;
+    assert(CurOff.isZero() && "Unexpected offset for first field");
+    Field2Ty = Field1Ty;
+    Field2Off = Field1Off + getContext().getTypeSizeInChars(EltTy);
+    return true;
+  }
+
+  if (const ConstantArrayType *ATy = getContext().getAsConstantArrayType(Ty)) {
+    uint64_t ArraySize = ATy->getSize().getZExtValue();
+    QualType EltTy = ATy->getElementType();
+    CharUnits EltSize = getContext().getTypeSizeInChars(EltTy);
+    for (uint64_t i = 0; i < ArraySize; ++i) {
+      bool Ret = detectFPCCEligibleStructHelper(EltTy, CurOff, Field1Ty,
+                                                Field1Off, Field2Ty, Field2Off);
+      if (!Ret)
+        return false;
+      CurOff += EltSize;
+    }
+    return true;
+  }
+
+  if (const auto *RTy = Ty->getAs<RecordType>()) {
+    // Structures with either a non-trivial destructor or a non-trivial
+    // copy constructor are not eligible for the FP calling convention.
+    if (getRecordArgABI(Ty, CGT.getCXXABI()))
+      return false;
+    if (isEmptyRecord(getContext(), Ty, true))
+      return true;
+    const RecordDecl *RD = RTy->getDecl();
+    // Unions aren't eligible unless they're empty (which is caught above).
+    if (RD->isUnion())
+      return false;
+    const ASTRecordLayout &Layout = getContext().getASTRecordLayout(RD);
+    // If this is a C++ record, check the bases first.
+    if (const CXXRecordDecl *CXXRD = dyn_cast<CXXRecordDecl>(RD)) {
+      for (const CXXBaseSpecifier &B : CXXRD->bases()) {
+        const auto *BDecl =
+            cast<CXXRecordDecl>(B.getType()->castAs<RecordType>()->getDecl());
+        CharUnits BaseOff = Layout.getBaseClassOffset(BDecl);
+        bool Ret = detectFPCCEligibleStructHelper(B.getType(), CurOff + BaseOff,
+                                                  Field1Ty, Field1Off, Field2Ty,
+                                                  Field2Off);
+        if (!Ret)
+          return false;
+      }
+    }
+    int ZeroWidthBitFieldCount = 0;
+    for (const FieldDecl *FD : RD->fields()) {
+      uint64_t FieldOffInBits = Layout.getFieldOffset(FD->getFieldIndex());
+      QualType QTy = FD->getType();
+      if (FD->isBitField()) {
+        unsigned BitWidth = FD->getBitWidthValue(getContext());
+        // Allow a bitfield with a type greater than GRLen as long as the
+        // bitwidth is GRLen or less.
+        if (getContext().getTypeSize(QTy) > GRLen && BitWidth <= GRLen)
+          QTy = getContext().getIntTypeForBitwidth(GRLen, false);
+        if (BitWidth == 0) {
+          ZeroWidthBitFieldCount++;
+          continue;
+        }
+      }
+
+      bool Ret = detectFPCCEligibleStructHelper(
+          QTy, CurOff + getContext().toCharUnitsFromBits(FieldOffInBits),
+          Field1Ty, Field1Off, Field2Ty, Field2Off);
+      if (!Ret)
+        return false;
+
+      // As a quirk of the ABI, zero-width bitfields aren't ignored for fp+fp
+      // or int+fp structs, but are ignored for a struct with an fp field and
+      // any number of zero-width bitfields.
+      if (Field2Ty && ZeroWidthBitFieldCount > 0)
+        return false;
+    }
+    return Field1Ty != nullptr;
+  }
+
+  return false;
+}
+
+// Determine if a struct is eligible for passing according to the floating
+// point calling convention (i.e., when flattened it contains a single fp
+// value, fp+fp, or int+fp of appropriate size). If so, NeededArgFPRs and
+// NeededArgGPRs are incremented appropriately.
+bool LoongArchABIInfo::detectFPCCEligibleStruct(QualType Ty, llvm::Type *&Field1Ty,
+                                            CharUnits &Field1Off,
+                                            llvm::Type *&Field2Ty,
+                                            CharUnits &Field2Off,
+                                            int &NeededArgGPRs,
+                                            int &NeededArgFPRs) const {
+  Field1Ty = nullptr;
+  Field2Ty = nullptr;
+  NeededArgGPRs = 0;
+  NeededArgFPRs = 0;
+  bool IsCandidate = detectFPCCEligibleStructHelper(
+      Ty, CharUnits::Zero(), Field1Ty, Field1Off, Field2Ty, Field2Off);
+  // Not really a candidate if we have a single int but no float.
+  if (Field1Ty && !Field2Ty && !Field1Ty->isFloatingPointTy())
+    return IsCandidate = false;
+  if (!IsCandidate)
+    return false;
+  if (Field1Ty && Field1Ty->isFloatingPointTy())
+    NeededArgFPRs++;
+  else if (Field1Ty)
+    NeededArgGPRs++;
+  if (Field2Ty && Field2Ty->isFloatingPointTy())
+    NeededArgFPRs++;
+  else if (Field2Ty)
+    NeededArgGPRs++;
+  return IsCandidate;
+}
+
+// Call getCoerceAndExpand for the two-element flattened struct described by
+// Field1Ty, Field1Off, Field2Ty, Field2Off. This method will create an
+// appropriate coerceToType and unpaddedCoerceToType.
+ABIArgInfo LoongArchABIInfo::coerceAndExpandFPCCEligibleStruct(
+    llvm::Type *Field1Ty, CharUnits Field1Off, llvm::Type *Field2Ty,
+    CharUnits Field2Off) const {
+  SmallVector<llvm::Type *, 3> CoerceElts;
+  SmallVector<llvm::Type *, 2> UnpaddedCoerceElts;
+  if (!Field1Off.isZero())
+    CoerceElts.push_back(llvm::ArrayType::get(
+        llvm::Type::getInt8Ty(getVMContext()), Field1Off.getQuantity()));
+
+  CoerceElts.push_back(Field1Ty);
+  UnpaddedCoerceElts.push_back(Field1Ty);
+
+  if (!Field2Ty) {
+    return ABIArgInfo::getCoerceAndExpand(
+        llvm::StructType::get(getVMContext(), CoerceElts, !Field1Off.isZero()),
+        UnpaddedCoerceElts[0]);
+  }
+
+  CharUnits Field2Align =
+      CharUnits::fromQuantity(getDataLayout().getABITypeAlignment(Field2Ty));
+  CharUnits Field1Size =
+      CharUnits::fromQuantity(getDataLayout().getTypeStoreSize(Field1Ty));
+  CharUnits Field2OffNoPadNoPack = Field1Size.alignTo(Field2Align);
+
+  CharUnits Padding = CharUnits::Zero();
+  if (Field2Off > Field2OffNoPadNoPack)
+    Padding = Field2Off - Field2OffNoPadNoPack;
+  else if (Field2Off != Field2Align && Field2Off > Field1Size)
+    Padding = Field2Off - Field1Size;
+
+  bool IsPacked = !Field2Off.isMultipleOf(Field2Align);
+
+  if (!Padding.isZero())
+    CoerceElts.push_back(llvm::ArrayType::get(
+        llvm::Type::getInt8Ty(getVMContext()), Padding.getQuantity()));
+
+  CoerceElts.push_back(Field2Ty);
+  UnpaddedCoerceElts.push_back(Field2Ty);
+
+  auto CoerceToType =
+      llvm::StructType::get(getVMContext(), CoerceElts, IsPacked);
+  auto UnpaddedCoerceToType =
+      llvm::StructType::get(getVMContext(), UnpaddedCoerceElts, IsPacked);
+
+  return ABIArgInfo::getCoerceAndExpand(CoerceToType, UnpaddedCoerceToType);
+}
+
+void LoongArchABIInfo::CoerceToIntArgs(
+    uint64_t TySize, SmallVectorImpl<llvm::Type *> &ArgList) const {
+  llvm::IntegerType *IntTy =
+    llvm::IntegerType::get(getVMContext(), MinABIStackAlignInBytes * 8);
+
+  // Add (TySize / MinABIStackAlignInBytes) args of IntTy.
+  for (unsigned N = TySize / (MinABIStackAlignInBytes * 8); N; --N)
+    ArgList.push_back(IntTy);
+
+  // If necessary, add one more integer type to ArgList.
+  unsigned R = TySize % (MinABIStackAlignInBytes * 8);
+
+  if (R)
+    ArgList.push_back(llvm::IntegerType::get(getVMContext(), R));
+}
+
+llvm::Type*  LoongArchABIInfo::HandleAggregates(QualType Ty, uint64_t TySize) const {
+  SmallVector<llvm::Type*, 8> ArgList, IntArgList;
+
+  if (Ty->isComplexType())
+    return CGT.ConvertType(Ty);
+
+  const RecordType *RT = Ty->getAs<RecordType>();
+
+  // Unions/vectors are passed in integer registers.
+  if (!RT || !RT->isStructureOrClassType()) {
+    CoerceToIntArgs(TySize, ArgList);
+    return llvm::StructType::get(getVMContext(), ArgList);
+  }
+
+  const RecordDecl *RD = RT->getDecl();
+  const ASTRecordLayout &Layout = getContext().getASTRecordLayout(RD);
+  assert(!(TySize % 8) && "Size of structure must be multiple of 8.");
+
+  uint64_t LastOffset = 0;
+  unsigned idx = 0;
+  llvm::IntegerType *I64 = llvm::IntegerType::get(getVMContext(), 64);
+
+  // Iterate over fields in the struct/class and check if there are any aligned
+  // double fields.
+  for (RecordDecl::field_iterator i = RD->field_begin(), e = RD->field_end();
+       i != e; ++i, ++idx) {
+    const QualType Ty = i->getType();
+    const BuiltinType *BT = Ty->getAs<BuiltinType>();
+
+    if (!BT || BT->getKind() != BuiltinType::Double)
+      continue;
+
+    uint64_t Offset = Layout.getFieldOffset(idx);
+    if (Offset % 64) // Ignore doubles that are not aligned.
+      continue;
+
+    // Add ((Offset - LastOffset) / 64) args of type i64.
+    for (unsigned j = (Offset - LastOffset) / 64; j > 0; --j)
+      ArgList.push_back(I64);
+
+    // Add double type.
+    ArgList.push_back(llvm::Type::getDoubleTy(getVMContext()));
+    LastOffset = Offset + 64;
+  }
+
+  CoerceToIntArgs(TySize - LastOffset, IntArgList);
+  ArgList.append(IntArgList.begin(), IntArgList.end());
+
+  return llvm::StructType::get(getVMContext(), ArgList);
+}
+
+llvm::Type * LoongArchABIInfo::getPaddingType(uint64_t OrigOffset,
+                                        uint64_t Offset) const {
+  if (OrigOffset + MinABIStackAlignInBytes > Offset)
+    return nullptr;
+
+  return llvm::IntegerType::get(getVMContext(), (Offset - OrigOffset) * 8);
+}
+
+ABIArgInfo LoongArchABIInfo::classifyArgumentType(QualType Ty, bool IsFixed,
+                                              int &ArgGPRsLeft,
+                                              int &ArgFPRsLeft) const {
+  assert(ArgGPRsLeft <= NumArgGPRs && "Arg GPR tracking underflow");
+  Ty = useFirstFieldIfTransparentUnion(Ty);
+
+  // Structures with either a non-trivial destructor or a non-trivial
+  // copy constructor are always passed indirectly.
+  if (CGCXXABI::RecordArgABI RAA = getRecordArgABI(Ty, getCXXABI())) {
+    if (ArgGPRsLeft)
+      ArgGPRsLeft -= 1;
+    return getNaturalAlignIndirect(Ty, /*ByVal=*/RAA ==
+                                           CGCXXABI::RAA_DirectInMemory);
+  }
+
+  // Ignore empty structs/unions.
+  if (isEmptyRecord(getContext(), Ty, true))
+    return ABIArgInfo::getIgnore();
+
+  uint64_t Size = getContext().getTypeSize(Ty);
+
+  // Pass floating point values via FPRs if possible.
+  if (IsFixed && Ty->isFloatingType() && FRLen >= Size && ArgFPRsLeft) {
+    ArgFPRsLeft--;
+    return ABIArgInfo::getDirect();
+  }
+
+  // Complex types for the hard float ABI must be passed direct rather than
+  // using CoerceAndExpand.
+  if (IsFixed && Ty->isComplexType() && FRLen && ArgFPRsLeft >= 2) {
+    QualType EltTy = Ty->getAs<ComplexType>()->getElementType();
+    if (getContext().getTypeSize(EltTy) <= FRLen) {
+      ArgFPRsLeft -= 2;
+      return ABIArgInfo::getDirect();
+    }
+  }
+
+  if (Ty->isVectorType() && (((getContext().getTypeSize(Ty) == 128) &&
+                              (getTarget().hasFeature("lsx"))) ||
+                             ((getContext().getTypeSize(Ty) == 256) &&
+                              getTarget().hasFeature("lasx"))))
+    return ABIArgInfo::getDirect();
+
+  if (IsFixed && FRLen && Ty->isStructureOrClassType()) {
+    llvm::Type *Field1Ty = nullptr;
+    llvm::Type *Field2Ty = nullptr;
+    CharUnits Field1Off = CharUnits::Zero();
+    CharUnits Field2Off = CharUnits::Zero();
+    int NeededArgGPRs;
+    int NeededArgFPRs;
+    bool IsCandidate =
+        detectFPCCEligibleStruct(Ty, Field1Ty, Field1Off, Field2Ty, Field2Off,
+                                 NeededArgGPRs, NeededArgFPRs);
+    if (IsCandidate && NeededArgGPRs <= ArgGPRsLeft &&
+        NeededArgFPRs <= ArgFPRsLeft) {
+      ArgGPRsLeft -= NeededArgGPRs;
+      ArgFPRsLeft -= NeededArgFPRs;
+      return coerceAndExpandFPCCEligibleStruct(Field1Ty, Field1Off, Field2Ty,
+                                               Field2Off);
+    }
+  } else if (Ty->isStructureOrClassType() && Size == 128 &&
+             isAggregateTypeForABI(Ty)) {
+    uint64_t Offset = 8;
+    uint64_t OrigOffset = Offset;
+    uint64_t TySize = getContext().getTypeSize(Ty);
+    uint64_t Align = getContext().getTypeAlign(Ty) / 8;
+
+    Align = std::min(std::max(Align, (uint64_t)MinABIStackAlignInBytes),
+                     (uint64_t)StackAlignInBytes);
+    unsigned CurrOffset = llvm::alignTo(Offset, Align);
+    Offset = CurrOffset + llvm::alignTo(TySize, Align * 8) / 8;
+
+    ABIArgInfo ArgInfo =
+        ABIArgInfo::getDirect(HandleAggregates(Ty, TySize), 0,
+                              getPaddingType(OrigOffset, CurrOffset));
+    ArgInfo.setInReg(true);
+    return ArgInfo;
+  }
+
+  uint64_t NeededAlign = getContext().getTypeAlign(Ty);
+  // Determine the number of GPRs needed to pass the current argument
+  // according to the ABI. 2*GRLen-aligned varargs are passed in "aligned"
+  // register pairs, so may consume 3 registers.
+  int NeededArgGPRs = 1;
+  if (!IsFixed && NeededAlign == 2 * GRLen)
+    NeededArgGPRs = 2 + (ArgGPRsLeft % 2);
+  else if (Size > GRLen && Size <= 2 * GRLen)
+    NeededArgGPRs = 2;
+
+  if (NeededArgGPRs > ArgGPRsLeft) {
+    NeededArgGPRs = ArgGPRsLeft;
+  }
+
+  ArgGPRsLeft -= NeededArgGPRs;
+
+  if (!isAggregateTypeForABI(Ty) && !Ty->isVectorType()) {
+    // Treat an enum type as its underlying type.
+    if (const EnumType *EnumTy = Ty->getAs<EnumType>())
+      Ty = EnumTy->getDecl()->getIntegerType();
+
+    // All integral types are promoted to GRLen width, unless passed on the
+    // stack.
+    if (Size < GRLen && Ty->isIntegralOrEnumerationType()) {
+      return extendType(Ty);
+    }
+
+    return ABIArgInfo::getDirect();
+  }
+
+  // Aggregates which are <= 2*GRLen will be passed in registers if possible,
+  // so coerce to integers.
+  if (Size <= 2 * GRLen) {
+    unsigned Alignment = getContext().getTypeAlign(Ty);
+
+    // Use a single GRLen int if possible, 2*GRLen if 2*GRLen alignment is
+    // required, and a 2-element GRLen array if only GRLen alignment is required.
+    if (Size <= GRLen) {
+      return ABIArgInfo::getDirect(
+          llvm::IntegerType::get(getVMContext(), GRLen));
+    } else if (Alignment == 2 * GRLen) {
+      return ABIArgInfo::getDirect(
+          llvm::IntegerType::get(getVMContext(), 2 * GRLen));
+    } else {
+      return ABIArgInfo::getDirect(llvm::ArrayType::get(
+          llvm::IntegerType::get(getVMContext(), GRLen), 2));
+    }
+  }
+  return getNaturalAlignIndirect(Ty, /*ByVal=*/false);
+}
+
+ABIArgInfo LoongArchABIInfo::classifyReturnType(QualType RetTy) const {
+  if (RetTy->isVoidType())
+    return ABIArgInfo::getIgnore();
+
+  int ArgGPRsLeft = 2;
+  int ArgFPRsLeft = FRLen ? 2 : 0;
+
+  // The rules for return and argument types are the same, so defer to
+  // classifyArgumentType.
+  return classifyArgumentType(RetTy, /*IsFixed=*/true, ArgGPRsLeft,
+                              ArgFPRsLeft);
+}
+
+Address LoongArchABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
+                                    QualType Ty) const {
+  CharUnits SlotSize = CharUnits::fromQuantity(GRLen / 8);
+
+  // Empty records are ignored for parameter passing purposes.
+  if (isEmptyRecord(getContext(), Ty, true)) {
+    Address Addr(CGF.Builder.CreateLoad(VAListAddr), SlotSize);
+    Addr = CGF.Builder.CreateElementBitCast(Addr, CGF.ConvertTypeForMem(Ty));
+    return Addr;
+  }
+
+  auto TInfo = getContext().getTypeInfoInChars(Ty);
+
+  // Arguments bigger than 2*GRlen bytes are passed indirectly.
+  bool IsIndirect = TInfo.Width > 2 * SlotSize;
+
+  return emitVoidPtrVAArg(CGF, VAListAddr, Ty, IsIndirect, TInfo,
+                          SlotSize, /*AllowHigherAlign=*/true);
+}
+
+ABIArgInfo LoongArchABIInfo::extendType(QualType Ty) const {
+  int TySize = getContext().getTypeSize(Ty);
+  // LP64 ABI requires unsigned 32 bit integers to be sign extended.
+  if (GRLen == 64 && Ty->isUnsignedIntegerOrEnumerationType() && TySize == 32)
+    return ABIArgInfo::getSignExtend(Ty);
+  return ABIArgInfo::getExtend(Ty);
+}
+
+namespace {
+class LoongArchTargetCodeGenInfo : public TargetCodeGenInfo {
+public:
+  LoongArchTargetCodeGenInfo(CodeGen::CodeGenTypes &CGT, unsigned GRLen,
+                         unsigned FRLen)
+      : TargetCodeGenInfo(std::make_unique<LoongArchABIInfo>(
+                          CGT, GRLen, FRLen)) {}
+
+  void setTargetAttributes(const Decl *D, llvm::GlobalValue *GV,
+                           CodeGen::CodeGenModule &CGM) const override {
+    return;
+  }
+};
+} // namespace
+
 //===----------------------------------------------------------------------===//
 // VE ABI Implementation.
 //
@@ -11320,6 +11871,7 @@ const TargetCodeGenInfo &CodeGenModule::getTargetCodeGenInfo() {
 
   case llvm::Triple::le32:
     return SetCGInfo(new PNaClTargetCodeGenInfo(Types));
+
   case llvm::Triple::m68k:
     return SetCGInfo(new M68kTargetCodeGenInfo(Types));
   case llvm::Triple::mips:
@@ -11437,6 +11989,9 @@ const TargetCodeGenInfo &CodeGenModule::getTargetCodeGenInfo() {
   case llvm::Triple::msp430:
     return SetCGInfo(new MSP430TargetCodeGenInfo(Types));
 
+  case llvm::Triple::loongarch64:
+    return SetCGInfo(new LoongArchTargetCodeGenInfo(Types, 64, 64));
+
   case llvm::Triple::riscv32:
   case llvm::Triple::riscv64: {
     StringRef ABIStr = getTarget().getABI();
diff --git a/lib/Driver/CMakeLists.txt b/lib/Driver/CMakeLists.txt
index 78e8fd18..90454cb1 100644
--- a/lib/Driver/CMakeLists.txt
+++ b/lib/Driver/CMakeLists.txt
@@ -26,6 +26,7 @@ add_clang_library(clangDriver
   ToolChain.cpp
   ToolChains/Arch/AArch64.cpp
   ToolChains/Arch/ARM.cpp
+  ToolChains/Arch/LoongArch.cpp
   ToolChains/Arch/M68k.cpp
   ToolChains/Arch/Mips.cpp
   ToolChains/Arch/PPC.cpp
diff --git a/lib/Driver/Driver.cpp b/lib/Driver/Driver.cpp
index 3bfddeef..0e13c31c 100644
--- a/lib/Driver/Driver.cpp
+++ b/lib/Driver/Driver.cpp
@@ -617,6 +617,22 @@ static llvm::Triple computeTargetTriple(const Driver &D,
     Target.setVendorName("intel");
   }
 
+  // If target is LoongArch adjust the target triple
+  // accordingly to provided ABI name.
+  A = Args.getLastArg(options::OPT_mabi_EQ);
+  if (A && Target.isLoongArch()) {
+    StringRef ABIName = A->getValue();
+    if (ABIName == "ilp32d" || ABIName == "ilp32f" || ABIName == "ilp32s") {
+      // TODO
+      llvm_unreachable("Unimplemented ABI");
+    } else if (ABIName == "lp64d") {
+      Target = Target.get64BitArchVariant();
+      if (Target.getEnvironment() == llvm::Triple::GNU ||
+          Target.getEnvironment() == llvm::Triple::GNUABILPX32)
+        Target.setEnvironment(llvm::Triple::GNUABI64);
+    }
+  }
+
   // If target is MIPS adjust the target triple
   // accordingly to provided ABI name.
   A = Args.getLastArg(options::OPT_mabi_EQ);
diff --git a/lib/Driver/SanitizerArgs.cpp b/lib/Driver/SanitizerArgs.cpp
index 96cef9eb..403fac76 100644
--- a/lib/Driver/SanitizerArgs.cpp
+++ b/lib/Driver/SanitizerArgs.cpp
@@ -367,19 +367,6 @@ SanitizerArgs::SanitizerArgs(const ToolChain &TC,
         Add &= ~NotAllowedWithMinimalRuntime;
       }
 
-      if (llvm::opt::Arg *A = Args.getLastArg(options::OPT_mcmodel_EQ)) {
-        StringRef CM = A->getValue();
-        if (CM != "small" &&
-            (Add & SanitizerKind::Function & ~DiagnosedKinds)) {
-          if (DiagnoseErrors)
-            D.Diag(diag::err_drv_argument_only_allowed_with)
-                << "-fsanitize=function"
-                << "-mcmodel=small";
-          Add &= ~SanitizerKind::Function;
-          DiagnosedKinds |= SanitizerKind::Function;
-        }
-      }
-
       // FIXME: Make CFI on member function calls compatible with cross-DSO CFI.
       // There are currently two problems:
       // - Virtual function call checks need to pass a pointer to the function
diff --git a/lib/Driver/ToolChains/Arch/LoongArch.cpp b/lib/Driver/ToolChains/Arch/LoongArch.cpp
new file mode 100644
index 00000000..0dcec221
--- /dev/null
+++ b/lib/Driver/ToolChains/Arch/LoongArch.cpp
@@ -0,0 +1,179 @@
+//===--- LoongArch.cpp - Tools Implementations -----------------------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArch.h"
+#include "ToolChains/CommonArgs.h"
+#include "clang/Driver/Driver.h"
+#include "clang/Driver/DriverDiagnostic.h"
+#include "clang/Driver/Options.h"
+#include "llvm/ADT/StringSwitch.h"
+#include "llvm/Option/ArgList.h"
+
+using namespace clang::driver;
+using namespace clang::driver::tools;
+using namespace clang;
+using namespace llvm::opt;
+
+// Get CPU and ABI names. They are not independent
+// so we have to calculate them together.
+void loongarch::getLoongArchCPUAndABI(const ArgList &Args, const llvm::Triple &Triple,
+                            StringRef &CPUName, StringRef &ABIName) {
+  const char *DefLoongArch32CPU = "generic-la32";
+  const char *DefLoongArch64CPU = "la464";
+
+  if (Arg *A = Args.getLastArg(clang::driver::options::OPT_march_EQ,
+                               options::OPT_mcpu_EQ))
+    CPUName = A->getValue();
+
+  if (Arg *A = Args.getLastArg(options::OPT_mabi_EQ))
+    ABIName = A->getValue();
+
+  // Setup default CPU and ABI names.
+  if (CPUName.empty() && ABIName.empty()) {
+    switch (Triple.getArch()) {
+    default:
+      llvm_unreachable("Unexpected triple arch name");
+    case llvm::Triple::loongarch32:
+      CPUName = DefLoongArch32CPU;
+      break;
+    case llvm::Triple::loongarch64:
+      CPUName = DefLoongArch64CPU;
+      break;
+    }
+  }
+
+  if (ABIName.empty() && (Triple.getEnvironment() == llvm::Triple::GNUABILPX32))
+    ABIName = "lpx32";
+
+  if (ABIName.empty()) {
+    ABIName = llvm::StringSwitch<const char *>(CPUName)
+                  .Case("generic-la32", "ilp32d")
+                  .Cases("la464", "generic-la64", "lp64d")
+                  .Default(Triple.isLoongArch32() ? "ilp32d" : "lp64d");
+  }
+
+  if (CPUName.empty()) {
+    // Deduce CPU name from ABI name.
+    CPUName = llvm::StringSwitch<const char *>(ABIName)
+                  .Cases("lp64d", "lp64f", "lp64s", DefLoongArch64CPU)
+                  .Default("");
+  }
+
+  if (Arg *A = Args.getLastArg(options::OPT_msingle_float,
+                               options::OPT_mdouble_float,
+                               options::OPT_msoft_float)) {
+    if (A->getOption().matches(options::OPT_msingle_float))
+      ABIName = "lp64f";
+    else if (A->getOption().matches(options::OPT_mdouble_float))
+      ABIName = "lp64d";
+    else
+      ABIName = "lp64s";
+  }
+
+  // FIXME: Warn on inconsistent use of -march and -mabi.
+}
+
+std::string loongarch::getLoongArchABILibSuffix(const ArgList &Args,
+                                      const llvm::Triple &Triple) {
+  StringRef CPUName, ABIName;
+  tools::loongarch::getLoongArchCPUAndABI(Args, Triple, CPUName, ABIName);
+  return llvm::StringSwitch<std::string>(ABIName)
+      .Cases("ilp32d", "ilp32f", "ilp32s", "32")
+      .Cases("lp64d", "lp64f", "lp64s", "64");
+}
+
+void loongarch::getLoongArchTargetFeatures(const Driver &D, const llvm::Triple &Triple,
+                                 const ArgList &Args,
+                                 std::vector<StringRef> &Features) {
+  StringRef CPUName;
+  StringRef ABIName;
+  StringRef FPUValue;
+  getLoongArchCPUAndABI(Args, Triple, CPUName, ABIName);
+
+  bool NonPIC = false;
+
+  Arg *LastPICArg = Args.getLastArg(options::OPT_fPIC, options::OPT_fno_PIC,
+                                    options::OPT_fpic, options::OPT_fno_pic,
+                                    options::OPT_fPIE, options::OPT_fno_PIE,
+                                    options::OPT_fpie, options::OPT_fno_pie);
+  if (LastPICArg) {
+    Option O = LastPICArg->getOption();
+    NonPIC =
+        (O.matches(options::OPT_fno_PIC) || O.matches(options::OPT_fno_pic) ||
+         O.matches(options::OPT_fno_PIE) || O.matches(options::OPT_fno_pie));
+  }
+
+  if (NonPIC) {
+    NonPIC = false;
+  }
+
+  AddTargetFeature(Args, Features, options::OPT_mlsx, options::OPT_mno_lsx,
+                   "lsx");
+  AddTargetFeature(Args, Features, options::OPT_mlasx, options::OPT_mno_lasx,
+                   "lasx");
+
+  AddTargetFeature(Args, Features, options::OPT_munaligned_access,
+                   options::OPT_mno_unaligned_access, "unaligned-access");
+  if (Arg *A = Args.getLastArg(options::OPT_mfpu_EQ))
+    FPUValue = A->getValue();
+
+  if (Arg *A = Args.getLastArg(options::OPT_msingle_float,
+                               options::OPT_mdouble_float,
+                               options::OPT_msoft_float)) {
+    if (A->getOption().matches(options::OPT_msingle_float))
+      FPUValue = "32";
+    else if (A->getOption().matches(options::OPT_mdouble_float))
+      FPUValue = "64";
+    else
+      FPUValue = "none";
+  }
+
+  // Setup feature.
+  if (FPUValue.empty())
+    Features.push_back("+d");
+  else {
+    if (FPUValue == "64")
+      Features.push_back("+d");
+    else if (FPUValue == "32")
+      Features.push_back("+f");
+    else if (FPUValue == "none") {
+      Features.push_back("-f");
+      Features.push_back("-d");
+    } else
+      D.Diag(clang::diag::err_drv_invalid_loongarch_mfpu)
+          << FPUValue;
+  }
+
+  // lp64f ABI and -mfpu=none are incompatible.
+  if (hasLoongArchAbiArg(Args, "lp64f") && hasLoongArchFpuArg(Args, "none")) {
+    D.Diag(clang::diag::err_opt_not_valid_with_opt) << "lp64f"
+                                                    << "-mfpu=none";
+  }
+
+  // Also lp64d ABI is only compatible with -mfpu=64.
+  if ((hasLoongArchAbiArg(Args, "lp64d") || ABIName == "lp64d") &&
+      (hasLoongArchFpuArg(Args, "none") || hasLoongArchFpuArg(Args, "32"))) {
+    D.Diag(clang::diag::err_opt_not_valid_without_opt) << "lp64d"
+                                                       << "-mfpu=64";
+  }
+}
+
+bool loongarch::hasLoongArchAbiArg(const ArgList &Args, const char *Value) {
+  Arg *A = Args.getLastArg(options::OPT_mabi_EQ);
+  return A && (A->getValue() == StringRef(Value));
+}
+
+bool loongarch::isUCLibc(const ArgList &Args) {
+  Arg *A = Args.getLastArg(options::OPT_m_libc_Group);
+  return A && A->getOption().matches(options::OPT_muclibc);
+}
+
+bool loongarch::hasLoongArchFpuArg(const ArgList &Args, const char *Value) {
+  Arg *A = Args.getLastArg(options::OPT_mfpu_EQ);
+  return A && (A->getValue() == StringRef(Value));
+}
diff --git a/lib/Driver/ToolChains/Arch/LoongArch.h b/lib/Driver/ToolChains/Arch/LoongArch.h
new file mode 100644
index 00000000..5c581ff6
--- /dev/null
+++ b/lib/Driver/ToolChains/Arch/LoongArch.h
@@ -0,0 +1,41 @@
+//===--- LoongArch.h - LoongArch-specific Tool Helpers ----------------------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CLANG_LIB_DRIVER_TOOLCHAINS_ARCH_LOONGARCH_H
+#define LLVM_CLANG_LIB_DRIVER_TOOLCHAINS_ARCH_LOONGARCH_H
+
+#include "clang/Driver/Driver.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/Option/Option.h"
+#include <string>
+#include <vector>
+
+namespace clang {
+namespace driver {
+namespace tools {
+
+namespace loongarch {
+void getLoongArchCPUAndABI(const llvm::opt::ArgList &Args,
+                      const llvm::Triple &Triple, StringRef &CPUName,
+                      StringRef &ABIName);
+void getLoongArchTargetFeatures(const Driver &D, const llvm::Triple &Triple,
+                           const llvm::opt::ArgList &Args,
+                           std::vector<StringRef> &Features);
+std::string getLoongArchABILibSuffix(const llvm::opt::ArgList &Args,
+                                const llvm::Triple &Triple);
+bool hasLoongArchAbiArg(const llvm::opt::ArgList &Args, const char *Value);
+bool hasLoongArchFpuArg(const llvm::opt::ArgList &Args, const char *Value);
+bool isUCLibc(const llvm::opt::ArgList &Args);
+
+} // end namespace loongarch
+} // end namespace target
+} // end namespace driver
+} // end namespace clang
+
+#endif // LLVM_CLANG_LIB_DRIVER_TOOLCHAINS_ARCH_LOONGARCH_H
diff --git a/lib/Driver/ToolChains/Clang.cpp b/lib/Driver/ToolChains/Clang.cpp
index f2f18e90..fdfde359 100644
--- a/lib/Driver/ToolChains/Clang.cpp
+++ b/lib/Driver/ToolChains/Clang.cpp
@@ -10,6 +10,7 @@
 #include "AMDGPU.h"
 #include "Arch/AArch64.h"
 #include "Arch/ARM.h"
+#include "Arch/LoongArch.h"
 #include "Arch/M68k.h"
 #include "Arch/Mips.h"
 #include "Arch/PPC.h"
@@ -331,6 +332,11 @@ static void getTargetFeatures(const Driver &D, const llvm::Triple &Triple,
     arm::getARMTargetFeatures(D, Triple, Args, CmdArgs, Features, ForAS);
     break;
 
+  case llvm::Triple::loongarch32:
+  case llvm::Triple::loongarch64:
+    loongarch::getLoongArchTargetFeatures(D, Triple, Args, Features);
+    break;
+
   case llvm::Triple::ppc:
   case llvm::Triple::ppcle:
   case llvm::Triple::ppc64:
@@ -535,6 +541,8 @@ static bool useFramePointerForTargetByDefault(const ArgList &Args,
     // XCore never wants frame pointers, regardless of OS.
     // WebAssembly never wants frame pointers.
     return false;
+  case llvm::Triple::loongarch64:
+  case llvm::Triple::loongarch32:
   case llvm::Triple::ppc:
   case llvm::Triple::ppcle:
   case llvm::Triple::ppc64:
@@ -1751,6 +1759,11 @@ void Clang::RenderTargetOptions(const llvm::Triple &EffectiveTriple,
     CmdArgs.push_back("-fallow-half-arguments-and-returns");
     break;
 
+  case llvm::Triple::loongarch32:
+  case llvm::Triple::loongarch64:
+    AddLoongArchTargetArgs(Args, CmdArgs);
+    break;
+
   case llvm::Triple::mips:
   case llvm::Triple::mipsel:
   case llvm::Triple::mips64:
@@ -1895,6 +1908,32 @@ void Clang::AddAArch64TargetArgs(const ArgList &Args,
   }
 }
 
+void Clang::AddLoongArchTargetArgs(const ArgList &Args,
+                                   ArgStringList &CmdArgs) const {
+  const Driver &D = getToolChain().getDriver();
+  StringRef CPUName;
+  StringRef ABIName;
+  const llvm::Triple &Triple = getToolChain().getTriple();
+  loongarch::getLoongArchCPUAndABI(Args, Triple, CPUName, ABIName);
+
+  CmdArgs.push_back("-target-abi");
+  CmdArgs.push_back(ABIName.data());
+
+  if (Arg *A = Args.getLastArg(options::OPT_mcheck_zero_division,
+                               options::OPT_mno_check_zero_division)) {
+    if (A->getOption().matches(options::OPT_mno_check_zero_division)) {
+      CmdArgs.push_back("-mllvm");
+      CmdArgs.push_back("-mnocheck-zero-division");
+    }
+  }
+
+  llvm::Reloc::Model RelocationModel;
+  unsigned PICLevel;
+  bool IsPIE;
+  std::tie(RelocationModel, PICLevel, IsPIE) =
+      ParsePICArgs(getToolChain(), Args);
+}
+
 void Clang::AddMIPSTargetArgs(const ArgList &Args,
                               ArgStringList &CmdArgs) const {
   const Driver &D = getToolChain().getDriver();
@@ -7627,6 +7666,17 @@ const char *Clang::getDependencyFileName(const ArgList &Args,
 
 // Begin ClangAs
 
+void ClangAs::AddLoongArchTargetArgs(const ArgList &Args,
+                                     ArgStringList &CmdArgs) const {
+  StringRef CPUName;
+  StringRef ABIName;
+  const llvm::Triple &Triple = getToolChain().getTriple();
+  loongarch::getLoongArchCPUAndABI(Args, Triple, CPUName, ABIName);
+
+  CmdArgs.push_back("-target-abi");
+  CmdArgs.push_back(ABIName.data());
+}
+
 void ClangAs::AddMIPSTargetArgs(const ArgList &Args,
                                 ArgStringList &CmdArgs) const {
   StringRef CPUName;
@@ -7816,6 +7866,11 @@ void ClangAs::ConstructJob(Compilation &C, const JobAction &JA,
   default:
     break;
 
+  case llvm::Triple::loongarch32:
+  case llvm::Triple::loongarch64:
+    AddLoongArchTargetArgs(Args, CmdArgs);
+    break;
+
   case llvm::Triple::mips:
   case llvm::Triple::mipsel:
   case llvm::Triple::mips64:
diff --git a/lib/Driver/ToolChains/Clang.h b/lib/Driver/ToolChains/Clang.h
index 79407c98..ba59f751 100644
--- a/lib/Driver/ToolChains/Clang.h
+++ b/lib/Driver/ToolChains/Clang.h
@@ -57,6 +57,8 @@ private:
                         bool KernelOrKext) const;
   void AddARM64TargetArgs(const llvm::opt::ArgList &Args,
                           llvm::opt::ArgStringList &CmdArgs) const;
+  void AddLoongArchTargetArgs(const llvm::opt::ArgList &Args,
+                              llvm::opt::ArgStringList &CmdArgs) const;
   void AddMIPSTargetArgs(const llvm::opt::ArgList &Args,
                          llvm::opt::ArgStringList &CmdArgs) const;
   void AddPPCTargetArgs(const llvm::opt::ArgList &Args,
@@ -123,6 +125,8 @@ class LLVM_LIBRARY_VISIBILITY ClangAs : public Tool {
 public:
   ClangAs(const ToolChain &TC)
       : Tool("clang::as", "clang integrated assembler", TC) {}
+  void AddLoongArchTargetArgs(const llvm::opt::ArgList &Args,
+                              llvm::opt::ArgStringList &CmdArgs) const;
   void AddMIPSTargetArgs(const llvm::opt::ArgList &Args,
                          llvm::opt::ArgStringList &CmdArgs) const;
   void AddX86TargetArgs(const llvm::opt::ArgList &Args,
diff --git a/lib/Driver/ToolChains/CommonArgs.cpp b/lib/Driver/ToolChains/CommonArgs.cpp
index 8f9244ca..a8bcf851 100644
--- a/lib/Driver/ToolChains/CommonArgs.cpp
+++ b/lib/Driver/ToolChains/CommonArgs.cpp
@@ -9,6 +9,7 @@
 #include "CommonArgs.h"
 #include "Arch/AArch64.h"
 #include "Arch/ARM.h"
+#include "Arch/LoongArch.h"
 #include "Arch/M68k.h"
 #include "Arch/Mips.h"
 #include "Arch/PPC.h"
@@ -383,6 +384,14 @@ std::string tools::getCPUName(const Driver &D, const ArgList &Args,
       return A->getValue();
     return "";
 
+  case llvm::Triple::loongarch32:
+  case llvm::Triple::loongarch64: {
+    StringRef CPUName;
+    StringRef ABIName;
+    loongarch::getLoongArchCPUAndABI(Args, T, CPUName, ABIName);
+    return std::string(CPUName);
+  }
+
   case llvm::Triple::m68k:
     return m68k::getM68kTargetCPU(Args);
 
@@ -1321,6 +1330,18 @@ tools::ParsePICArgs(const ToolChain &ToolChain, const ArgList &Args) {
   if ((ROPI || RWPI) && (PIC || PIE))
     ToolChain.getDriver().Diag(diag::err_drv_ropi_rwpi_incompatible_with_pic);
 
+  if (Triple.isLoongArch()) {
+    StringRef CPUName;
+    StringRef ABIName;
+    loongarch::getLoongArchCPUAndABI(Args, Triple, CPUName, ABIName);
+    // When targeting the LP64D ABI, PIC is the default.
+    if (ABIName == "lp64d")
+      PIC = true;
+    // Unlike other architectures, LoongArch, even with -fPIC/-mxgot/multigot,
+    // does not use PIC level 2 for historical reasons.
+    IsPICLevelTwo = false;
+  }
+
   if (Triple.isMIPS()) {
     StringRef CPUName;
     StringRef ABIName;
diff --git a/lib/Driver/ToolChains/Gnu.cpp b/lib/Driver/ToolChains/Gnu.cpp
index 7a9570a6..c0aa8fef 100644
--- a/lib/Driver/ToolChains/Gnu.cpp
+++ b/lib/Driver/ToolChains/Gnu.cpp
@@ -8,6 +8,7 @@
 
 #include "Gnu.h"
 #include "Arch/ARM.h"
+#include "Arch/LoongArch.h"
 #include "Arch/Mips.h"
 #include "Arch/PPC.h"
 #include "Arch/RISCV.h"
@@ -254,6 +255,10 @@ static const char *getLDMOption(const llvm::Triple &T, const ArgList &Args) {
   case llvm::Triple::armeb:
   case llvm::Triple::thumbeb:
     return isArmBigEndian(T, Args) ? "armelfb_linux_eabi" : "armelf_linux_eabi";
+  case llvm::Triple::loongarch32:
+    return "elf32loongarch";
+  case llvm::Triple::loongarch64:
+    return "elf64loongarch";
   case llvm::Triple::m68k:
     return "m68kelf";
   case llvm::Triple::ppc:
@@ -822,6 +827,55 @@ void tools::gnutools::Assembler::ConstructJob(Compilation &C,
 
     break;
   }
+  case llvm::Triple::loongarch32:
+  case llvm::Triple::loongarch64: {
+    StringRef CPUName;
+    StringRef ABIName;
+    loongarch::getLoongArchCPUAndABI(Args, getToolChain().getTriple(), CPUName, ABIName);
+
+    //FIXME: Currently gnu as doesn't support -march
+    //CmdArgs.push_back("-march=loongarch");
+    //CmdArgs.push_back(CPUName.data());
+
+    CmdArgs.push_back("-mabi=lp64d");
+
+    // -mno-shared should be emitted unless -fpic, -fpie, -fPIC, -fPIE,
+    // or -mshared (not implemented) is in effect.
+    if (RelocationModel == llvm::Reloc::Static)
+      CmdArgs.push_back("-mno-shared");
+
+    break;
+
+    // Add the last -mfp32/-mfp64.
+    if (Arg *A = Args.getLastArg(options::OPT_mfp32,
+                                 options::OPT_mfp64)) {
+      A->claim();
+      A->render(Args, CmdArgs);
+    }
+
+    if (Arg *A = Args.getLastArg(options::OPT_mlsx, options::OPT_mno_lsx)) {
+      // Do not use AddLastArg because not all versions of LoongArch assembler
+      // support -mlsx / -mno-lsx options.
+      if (A->getOption().matches(options::OPT_mlsx))
+        CmdArgs.push_back(Args.MakeArgString("-mlsx"));
+    }
+
+    if (Arg *A = Args.getLastArg(options::OPT_mlasx, options::OPT_mno_lasx)) {
+      // Do not use AddLastArg because not all versions of LoongArch assembler
+      // support -mlasx / -mno-lasx options.
+      if (A->getOption().matches(options::OPT_mlasx))
+        CmdArgs.push_back(Args.MakeArgString("-mlasx"));
+    }
+
+    Args.AddLastArg(CmdArgs, options::OPT_mhard_float,
+                    options::OPT_msoft_float);
+
+    Args.AddLastArg(CmdArgs, options::OPT_mdouble_float,
+                    options::OPT_msingle_float);
+
+    AddAssemblerKPIC(getToolChain(), Args, CmdArgs);
+    break;
+  }
   case llvm::Triple::mips:
   case llvm::Triple::mipsel:
   case llvm::Triple::mips64:
@@ -2185,6 +2239,10 @@ void Generic_GCC::GCCInstallationDetector::AddDefaultGCCPrefixes(
       "s390x-linux-gnu", "s390x-unknown-linux-gnu", "s390x-ibm-linux-gnu",
       "s390x-suse-linux", "s390x-redhat-linux"};
 
+  static const char *const LoongArch64LibDirs[] = {"/lib64", "/lib"};
+  static const char *const LoongArch64Triples[] = {
+      "loongarch64-linux-gnu", "loongarch64-unknown-linux-gnu",
+      "loongarch64-loongson-linux-gnu"};
 
   using std::begin;
   using std::end;
@@ -2353,6 +2411,10 @@ void Generic_GCC::GCCInstallationDetector::AddDefaultGCCPrefixes(
       BiarchTripleAliases.append(begin(X32Triples), end(X32Triples));
     }
     break;
+  case llvm::Triple::loongarch64:
+    LibDirs.append(begin(LoongArch64LibDirs), end(LoongArch64LibDirs));
+    TripleAliases.append(begin(LoongArch64Triples), end(LoongArch64Triples));
+    break;
   case llvm::Triple::m68k:
     LibDirs.append(begin(M68kLibDirs), end(M68kLibDirs));
     TripleAliases.append(begin(M68kTriples), end(M68kTriples));
@@ -2708,6 +2770,7 @@ bool Generic_GCC::isPICDefault() const {
   switch (getArch()) {
   case llvm::Triple::x86_64:
     return getTriple().isOSWindows();
+  case llvm::Triple::loongarch64:
   case llvm::Triple::mips64:
   case llvm::Triple::mips64el:
     return true;
@@ -2750,6 +2813,8 @@ bool Generic_GCC::IsIntegratedAssemblerDefault() const {
   case llvm::Triple::mips64el:
   case llvm::Triple::msp430:
   case llvm::Triple::m68k:
+  case llvm::Triple::loongarch32:
+  case llvm::Triple::loongarch64:
     return true;
   case llvm::Triple::sparc:
   case llvm::Triple::sparcel:
diff --git a/lib/Driver/ToolChains/Linux.cpp b/lib/Driver/ToolChains/Linux.cpp
index 83cb4115..d001dcd0 100644
--- a/lib/Driver/ToolChains/Linux.cpp
+++ b/lib/Driver/ToolChains/Linux.cpp
@@ -8,6 +8,7 @@
 
 #include "Linux.h"
 #include "Arch/ARM.h"
+#include "Arch/LoongArch.h"
 #include "Arch/Mips.h"
 #include "Arch/PPC.h"
 #include "Arch/RISCV.h"
@@ -85,6 +86,11 @@ std::string Linux::getMultiarchTriple(const Driver &D,
   case llvm::Triple::aarch64_be:
     return "aarch64_be-linux-gnu";
 
+  case llvm::Triple::loongarch32:
+    return "loongarch32-linux-gnu";
+  case llvm::Triple::loongarch64:
+    return "loongarch64-linux-gnu";
+
   case llvm::Triple::m68k:
     return "m68k-linux-gnu";
 
@@ -452,6 +458,14 @@ std::string Linux::getDynamicLinker(const ArgList &Args) const {
     Loader = HF ? "ld-linux-armhf.so.3" : "ld-linux.so.3";
     break;
   }
+  case llvm::Triple::loongarch32:
+  case llvm::Triple::loongarch64: {
+    StringRef CPUName, ABIName;
+    tools::loongarch::getLoongArchCPUAndABI(Args, Triple, CPUName, ABIName);
+    LibDir = "lib" + tools::loongarch::getLoongArchABILibSuffix(Args, Triple);
+    Loader = ("ld-linux-loongarch-" + ABIName + ".so.1").str();
+    break;
+  }
   case llvm::Triple::m68k:
     LibDir = "lib";
     Loader = "ld.so.1";
@@ -702,6 +716,7 @@ SanitizerMask Linux::getSupportedSanitizers() const {
   const bool IsRISCV64 = getTriple().getArch() == llvm::Triple::riscv64;
   const bool IsSystemZ = getTriple().getArch() == llvm::Triple::systemz;
   const bool IsHexagon = getTriple().getArch() == llvm::Triple::hexagon;
+  const bool IsLoongArch64 = getTriple().getArch() == llvm::Triple::loongarch64;
   SanitizerMask Res = ToolChain::getSupportedSanitizers();
   Res |= SanitizerKind::Address;
   Res |= SanitizerKind::PointerCompare;
@@ -712,19 +727,20 @@ SanitizerMask Linux::getSupportedSanitizers() const {
   Res |= SanitizerKind::Memory;
   Res |= SanitizerKind::Vptr;
   Res |= SanitizerKind::SafeStack;
-  if (IsX86_64 || IsMIPS64 || IsAArch64)
+  if (IsX86_64 || IsMIPS64 || IsAArch64 || IsLoongArch64)
     Res |= SanitizerKind::DataFlow;
   if (IsX86_64 || IsMIPS64 || IsAArch64 || IsX86 || IsArmArch || IsPowerPC64 ||
-      IsRISCV64 || IsSystemZ || IsHexagon)
+      IsRISCV64 || IsSystemZ || IsHexagon || IsLoongArch64)
     Res |= SanitizerKind::Leak;
-  if (IsX86_64 || IsMIPS64 || IsAArch64 || IsPowerPC64 || IsSystemZ)
+  if (IsX86_64 || IsMIPS64 || IsAArch64 || IsPowerPC64 || IsSystemZ ||
+      IsLoongArch64)
     Res |= SanitizerKind::Thread;
   if (IsX86_64)
     Res |= SanitizerKind::KernelMemory;
   if (IsX86 || IsX86_64)
     Res |= SanitizerKind::Function;
   if (IsX86_64 || IsMIPS64 || IsAArch64 || IsX86 || IsMIPS || IsArmArch ||
-      IsPowerPC64 || IsHexagon)
+      IsPowerPC64 || IsHexagon || IsLoongArch64)
     Res |= SanitizerKind::Scudo;
   if (IsX86_64 || IsAArch64) {
     Res |= SanitizerKind::HWAddress;
diff --git a/lib/Driver/ToolChains/Linux.h b/lib/Driver/ToolChains/Linux.h
index 3c4546cb..a5648d79 100644
--- a/lib/Driver/ToolChains/Linux.h
+++ b/lib/Driver/ToolChains/Linux.h
@@ -10,7 +10,6 @@
 #define LLVM_CLANG_LIB_DRIVER_TOOLCHAINS_LINUX_H
 
 #include "Gnu.h"
-#include "clang/Basic/LangOptions.h"
 #include "clang/Driver/ToolChain.h"
 
 namespace clang {
@@ -47,10 +46,6 @@ public:
   IsAArch64OutlineAtomicsDefault(const llvm::opt::ArgList &Args) const override;
   bool isPIEDefault(const llvm::opt::ArgList &Args) const override;
   bool IsMathErrnoDefault() const override;
-  LangOptions::StackProtectorMode
-  GetDefaultStackProtectorLevel(bool KernelOrKext) const override {
-    return LangOptions::SSPStrong;
-  }
   SanitizerMask getSupportedSanitizers() const override;
   void addProfileRTLibs(const llvm::opt::ArgList &Args,
                         llvm::opt::ArgStringList &CmdArgs) const override;
diff --git a/lib/Driver/XRayArgs.cpp b/lib/Driver/XRayArgs.cpp
index 63b57517..4e3ae3f2 100644
--- a/lib/Driver/XRayArgs.cpp
+++ b/lib/Driver/XRayArgs.cpp
@@ -42,6 +42,8 @@ XRayArgs::XRayArgs(const ToolChain &TC, const ArgList &Args) {
     case llvm::Triple::aarch64:
     case llvm::Triple::hexagon:
     case llvm::Triple::ppc64le:
+    case llvm::Triple::loongarch32:
+    case llvm::Triple::loongarch64:
     case llvm::Triple::mips:
     case llvm::Triple::mipsel:
     case llvm::Triple::mips64:
diff --git a/lib/Headers/CMakeLists.txt b/lib/Headers/CMakeLists.txt
index 07898898..5ae7dbab 100644
--- a/lib/Headers/CMakeLists.txt
+++ b/lib/Headers/CMakeLists.txt
@@ -85,6 +85,7 @@ set(files
   invpcidintrin.h
   iso646.h
   keylockerintrin.h
+  larchintrin.h
   limits.h
   lwpintrin.h
   lzcntintrin.h
@@ -94,6 +95,8 @@ set(files
   module.modulemap
   movdirintrin.h
   msa.h
+  lsxintrin.h
+  lasxintrin.h
   mwaitxintrin.h
   nmmintrin.h
   opencl-c.h
diff --git a/lib/Headers/larchintrin.h b/lib/Headers/larchintrin.h
new file mode 100644
index 00000000..7e99f19a
--- /dev/null
+++ b/lib/Headers/larchintrin.h
@@ -0,0 +1,319 @@
+//===----------- larchintrin.h - LoongArch BASE intrinsics ------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the LoongArch Base intrinsics
+//
+//===----------------------------------------------------------------------===//
+#ifndef __LOONGARCH_BASE_H
+#define __LOONGARCH_BASE_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+typedef struct drdtime{
+	unsigned long dvalue;
+	unsigned long dtimeid;
+} __drdtime_t;
+
+typedef struct rdtime{
+	unsigned int value;
+	unsigned int timeid;
+} __rdtime_t;
+
+/* Assembly instruction format:          rd, csr_num */
+/* Data types in instruction templates:  unsigned int, uimm14_32 */
+#define __csrrd(/*uimm14_32*/ _1)	((unsigned int)__builtin_loongarch_csrrd(_1))
+
+/* Assembly instruction format:          rd, csr_num */
+/* Data types in instruction templates:  unsigned int, uimm14_32 */
+#define __csrwr(/*unsigned int*/ _1, /*uimm14_32*/ _2)	((unsigned int)__builtin_loongarch_csrwr((unsigned int)(_1), (_2)))
+
+/* Assembly instruction format:          rd, rj, csr_num */
+/* Data types in instruction templates:  unsigned int, unsigned int, uimm14_32 */
+#define __csrxchg(/*unsigned int*/ _1, /*unsigned int*/ _2, /*uimm14_32*/ _3) ((unsigned int)__builtin_loongarch_csrxchg((unsigned int)(_1), (unsigned int)(_2), (_3)))
+
+/* Assembly instruction format:          rd, csr_num */
+/* Data types in instruction templates:  unsigned long int, uimm14 */
+#define __dcsrrd(/*uimm14*/ _1)	((unsigned long int)__builtin_loongarch_dcsrrd(_1))
+
+/* Assembly instruction format:          rd, csr_num */
+/* Data types in instruction templates:  unsigned long int, uimm14 */
+#define __dcsrwr(/*unsigned long int*/ _1, /*uimm14*/ _2)	((unsigned long int)__builtin_loongarch_dcsrwr((unsigned long int)(_1), (_2)))
+
+/* Assembly instruction format:          rd, rj, csr_num */
+/* Data types in instruction templates:  unsigned long int, unsigned long int, uimm14 */
+#define __dcsrxchg(/*unsigned long int*/ _1, /*unsigned long int*/ _2, /*uimm14*/ _3) ((unsigned long int)__builtin_loongarch_dcsrxchg((unsigned long int)(_1), (unsigned long int)(_2), (_3)))
+
+/* Assembly instruction format:          rd, rj */
+/* Data types in instruction templates:  unsigned char, unsigned int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+unsigned char __iocsrrd_b(unsigned int _1)
+{
+	return (unsigned char)__builtin_loongarch_iocsrrd_b((unsigned int)_1);
+}
+
+/* Assembly instruction format:          rd, rj */
+/* Data types in instruction templates:  unsigned short, unsigned int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+unsigned short __iocsrrd_h(unsigned int _1)
+{
+	return (unsigned short)__builtin_loongarch_iocsrrd_h((unsigned int)_1);
+}
+
+/* Assembly instruction format:          rd, rj */
+/* Data types in instruction templates:  unsigned int, unsigned int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+unsigned int __iocsrrd_w(unsigned int _1)
+{
+	return (unsigned int)__builtin_loongarch_iocsrrd_w((unsigned int)_1);
+}
+
+/* Assembly instruction format:          rd, rj */
+/* Data types in instruction templates: unsigned long int, unsigned int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+unsigned long int __iocsrrd_d(unsigned int _1)
+{
+	return (unsigned long int)__builtin_loongarch_iocsrrd_d((unsigned int)_1);
+}
+
+/* Assembly instruction format:          rd, rj */
+/* Data types in instruction templates:  unsigned char, unsigned int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+void __iocsrwr_b(unsigned char _1, unsigned int _2)
+{
+	return (void)__builtin_loongarch_iocsrwr_b((unsigned char)_1, (unsigned int)_2);
+}
+
+/* Assembly instruction format:          rd, rj */
+/* Data types in instruction templates:  unsigned short, unsigned int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+void __iocsrwr_h(unsigned short _1, unsigned int _2)
+{
+	return (void)__builtin_loongarch_iocsrwr_h((unsigned short)_1, (unsigned int)_2);
+}
+
+/* Assembly instruction format:          rd, rj */
+/* Data types in instruction templates:  unsigned int, unsigned int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+void __iocsrwr_w(unsigned int _1, unsigned int _2)
+{
+	return (void)__builtin_loongarch_iocsrwr_w((unsigned int)_1, (unsigned int)_2);
+}
+
+/* Assembly instruction format:          rd, rj */
+/* Data types in instruction templates: unsigned long int, unsigned int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+void __iocsrwr_d(unsigned long int _1, unsigned int _2)
+{
+	return (void)__builtin_loongarch_iocsrwr_d((unsigned long int)_1, (unsigned int)_2);
+}
+
+/* Assembly instruction format:          op, rj, si12 */
+/* Data types in instruction templates: uimm5, unsigned int, simm12 */
+#define __cacop(/*uimm5*/ _1, /*unsigned int*/ _2, /*simm12*/ _3) ((void)__builtin_loongarch_cacop((_1), (unsigned int)(_2), (_3)))
+
+/* Assembly instruction format:          op, rj, si12 */
+/* Data types in instruction templates: uimm5, unsigned long int, simm12 */
+#define __dcacop(/*uimm5*/ _1, /*unsigned long int*/ _2, /*simm12*/ _3)	((void)__builtin_loongarch_dcacop((_1), (unsigned long int)(_2), (_3)))
+
+#define __rdtime_d	__builtin_loongarch_rdtime_d
+#define __rdtimel_w	__builtin_loongarch_rdtimel_w
+#define __rdtimeh_w	__builtin_loongarch_rdtimeh_w
+
+extern __inline __drdtime_t __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+__builtin_loongarch_rdtime_d (void)
+{
+  __drdtime_t drdtime;
+  __asm__ volatile (
+    "rdtime.d\t%[val],%[tid]\n\t"
+    : [val]"=&r"(drdtime.dvalue),[tid]"=&r"(drdtime.dtimeid)
+    :
+  );
+  return drdtime;
+}
+
+extern __inline __rdtime_t __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+__builtin_loongarch_rdtimeh_w (void)
+{
+  __rdtime_t rdtime;
+  __asm__ volatile (
+    "rdtimeh.w\t%[val],%[tid]\n\t"
+    : [val]"=&r"(rdtime.value),[tid]"=&r"(rdtime.timeid)
+    :
+  );
+  return rdtime;
+}
+
+extern __inline __rdtime_t __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+__builtin_loongarch_rdtimel_w (void)
+{
+  __rdtime_t rdtime;
+  __asm__ volatile (
+    "rdtimel.w\t%[val],%[tid]\n\t"
+    : [val]"=&r"(rdtime.value),[tid]"=&r"(rdtime.timeid)
+    :
+  );
+  return rdtime;
+}
+
+/* Assembly instruction format:          rd, rj, rk */
+/* Data types in instruction templates:  int, char, int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+int __crc_w_b_w(char _1, int _2)
+{
+	return (int)__builtin_loongarch_crc_w_b_w((char)_1, (int)_2);
+}
+
+/* Assembly instruction format:          rd, rj, rk */
+/* Data types in instruction templates:  int, short, int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+int __crc_w_h_w(short _1, int _2)
+{
+	return (int)__builtin_loongarch_crc_w_h_w((short)_1, (int)_2);
+}
+
+/* Assembly instruction format:          rd, rj, rk */
+/* Data types in instruction templates:  int, int, int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+int __crc_w_w_w(int _1, int _2)
+{
+	return (int)__builtin_loongarch_crc_w_w_w((int)_1, (int)_2);
+}
+
+/* Assembly instruction format:          rd, rj, rk */
+/* Data types in instruction templates: int, long int, int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+int __crc_w_d_w(long int _1, int _2)
+{
+	return (int)__builtin_loongarch_crc_w_d_w((long int)_1, (int)_2);
+}
+
+/* Assembly instruction format:          rd, rj, rk */
+/* Data types in instruction templates:  int, char, int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+int __crcc_w_b_w(char _1, int _2)
+{
+	return (int)__builtin_loongarch_crcc_w_b_w((char)_1, (int)_2);
+}
+
+/* Assembly instruction format:          rd, rj, rk */
+/* Data types in instruction templates:  int, short, int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+int __crcc_w_h_w(short _1, int _2)
+{
+	return (int)__builtin_loongarch_crcc_w_h_w((short)_1, (int)_2);
+}
+
+/* Assembly instruction format:          rd, rj, rk */
+/* Data types in instruction templates:  int, int, int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+int __crcc_w_w_w(int _1, int _2)
+{
+	return (int)__builtin_loongarch_crcc_w_w_w((int)_1, (int)_2);
+}
+
+/* Assembly instruction format:          rd, rj, rk */
+/* Data types in instruction templates: int, long int, int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+int __crcc_w_d_w(long int _1, int _2)
+{
+	return (int)__builtin_loongarch_crcc_w_d_w((long int)_1, (int)_2);
+}
+
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+void __tlbclr()
+{
+	return (void)__builtin_loongarch_tlbclr();
+}
+
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+void __tlbflush()
+{
+	return (void)__builtin_loongarch_tlbflush();
+}
+
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+void __tlbfill()
+{
+	return (void)__builtin_loongarch_tlbfill();
+}
+
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+void __tlbrd()
+{
+	return (void)__builtin_loongarch_tlbrd();
+}
+
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+void __tlbwr()
+{
+	return (void)__builtin_loongarch_tlbwr();
+}
+
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+void __tlbsrch()
+{
+	return (void)__builtin_loongarch_tlbsrch();
+}
+
+/* Assembly instruction format:          code */
+/* Data types in instruction templates:  uimm15 */
+#define __syscall(/*uimm15*/ _1)	((void)__builtin_loongarch_syscall(_1))
+
+/* Assembly instruction format:          code */
+/* Data types in instruction templates:  uimm15 */
+#define __break(/*uimm15*/ _1)	((void)__builtin_loongarch_break(_1))
+
+/* Assembly instruction format:          hint */
+/* Data types in instruction templates:  uimm15 */
+#define __dbar(/*uimm15*/ _1)	((void)__builtin_loongarch_dbar(_1))
+
+/* Assembly instruction format:          hint */
+/* Data types in instruction templates:  uimm15 */
+#define __ibar(/*uimm15*/ _1)	((void)__builtin_loongarch_ibar(_1))
+
+/* Assembly instruction format:          rj, rk */
+/* Data types in instruction templates:  long int, long int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+void __asrtle_d(long int _1, long int _2)
+{
+	return (void)__builtin_loongarch_asrtle_d((long int)_1, (long int)_2);
+}
+
+/* Assembly instruction format:          rj, rk */
+/* Data types in instruction templates:  long int, long int */
+extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+void __asrtgt_d(long int _1, long int _2)
+{
+	return (void)__builtin_loongarch_asrtgt_d((long int)_1, (long int)_2);
+}
+
+#define __movfcsr2gr(uimm5) \
+({ \
+  unsigned int rd; \
+  __asm__ volatile ( \
+    "movfcsr2gr %0, $fcsr" #uimm5 \
+    : "=&r"(rd) \
+    : \
+  ); rd; \
+})
+
+#define __movgr2fcsr(uimm5, rj) \
+{ \
+  __asm__ volatile ( \
+    "movgr2fcsr $fcsr" #uimm5 ", %0" \
+    : \
+    : "r" (rj) \
+  ); \
+}
+
+#ifdef __cplusplus
+}
+#endif
+#endif /* __LOONGARCH_BASE_H */
diff --git a/lib/Headers/lasxintrin.h b/lib/Headers/lasxintrin.h
new file mode 100644
index 00000000..48a0a176
--- /dev/null
+++ b/lib/Headers/lasxintrin.h
@@ -0,0 +1,5349 @@
+//===----------- lasxintrin.h - LoongArch LASX intrinsics
+//------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the LoongArch LASX intrinsics.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef _GCC_LOONGSON_ASXINTRIN_H
+#define _GCC_LOONGSON_ASXINTRIN_H 1
+
+#if defined(__loongarch_asx)
+
+typedef signed char v32i8 __attribute__((vector_size(32), aligned(32)));
+typedef signed char v32i8_b __attribute__((vector_size(32), aligned(1)));
+typedef unsigned char v32u8 __attribute__((vector_size(32), aligned(32)));
+typedef unsigned char v32u8_b __attribute__((vector_size(32), aligned(1)));
+typedef short v16i16 __attribute__((vector_size(32), aligned(32)));
+typedef short v16i16_h __attribute__((vector_size(32), aligned(2)));
+typedef unsigned short v16u16 __attribute__((vector_size(32), aligned(32)));
+typedef unsigned short v16u16_h __attribute__((vector_size(32), aligned(2)));
+typedef int v8i32 __attribute__((vector_size(32), aligned(32)));
+typedef int v8i32_w __attribute__((vector_size(32), aligned(4)));
+typedef unsigned int v8u32 __attribute__((vector_size(32), aligned(32)));
+typedef unsigned int v8u32_w __attribute__((vector_size(32), aligned(4)));
+typedef long long v4i64 __attribute__((vector_size(32), aligned(32)));
+typedef long long v4i64_d __attribute__((vector_size(32), aligned(8)));
+typedef unsigned long long v4u64 __attribute__((vector_size(32), aligned(32)));
+typedef unsigned long long v4u64_d __attribute__((vector_size(32), aligned(8)));
+typedef float v8f32 __attribute__((vector_size(32), aligned(32)));
+typedef float v8f32_w __attribute__((vector_size(32), aligned(4)));
+typedef double v4f64 __attribute__((vector_size(32), aligned(32)));
+typedef double v4f64_d __attribute__((vector_size(32), aligned(8)));
+
+typedef double v4f64 __attribute__((vector_size(32), aligned(32)));
+typedef double v4f64_d __attribute__((vector_size(32), aligned(8)));
+
+typedef float __m256 __attribute__((__vector_size__(32), __may_alias__));
+typedef long long __m256i __attribute__((__vector_size__(32), __may_alias__));
+typedef double __m256d __attribute__((__vector_size__(32), __may_alias__));
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsll_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsll_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsll_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsll_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsll_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsll_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsll_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsll_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  V32QI, V32QI, UQI */
+#define __lasx_xvslli_b(/*__m256i*/ _1, /*ui3*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvslli_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V16HI, V16HI, UQI */
+#define __lasx_xvslli_h(/*__m256i*/ _1, /*ui4*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvslli_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V8SI, V8SI, UQI */
+#define __lasx_xvslli_w(/*__m256i*/ _1, /*ui5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvslli_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V4DI, V4DI, UQI */
+#define __lasx_xvslli_d(/*__m256i*/ _1, /*ui6*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvslli_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsra_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsra_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsra_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsra_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsra_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsra_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsra_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsra_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  V32QI, V32QI, UQI */
+#define __lasx_xvsrai_b(/*__m256i*/ _1, /*ui3*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvsrai_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V16HI, V16HI, UQI */
+#define __lasx_xvsrai_h(/*__m256i*/ _1, /*ui4*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvsrai_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V8SI, V8SI, UQI */
+#define __lasx_xvsrai_w(/*__m256i*/ _1, /*ui5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvsrai_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V4DI, V4DI, UQI */
+#define __lasx_xvsrai_d(/*__m256i*/ _1, /*ui6*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvsrai_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrar_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrar_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrar_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrar_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrar_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrar_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrar_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrar_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  V32QI, V32QI, UQI */
+#define __lasx_xvsrari_b(/*__m256i*/ _1, /*ui3*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvsrari_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V16HI, V16HI, UQI */
+#define __lasx_xvsrari_h(/*__m256i*/ _1, /*ui4*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvsrari_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V8SI, V8SI, UQI */
+#define __lasx_xvsrari_w(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvsrari_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V4DI, V4DI, UQI */
+#define __lasx_xvsrari_d(/*__m256i*/ _1, /*ui6*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvsrari_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrl_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrl_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrl_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrl_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrl_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrl_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrl_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrl_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  V32QI, V32QI, UQI */
+#define __lasx_xvsrli_b(/*__m256i*/ _1, /*ui3*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvsrli_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V16HI, V16HI, UQI */
+#define __lasx_xvsrli_h(/*__m256i*/ _1, /*ui4*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvsrli_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V8SI, V8SI, UQI */
+#define __lasx_xvsrli_w(/*__m256i*/ _1, /*ui5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvsrli_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V4DI, V4DI, UQI */
+#define __lasx_xvsrli_d(/*__m256i*/ _1, /*ui6*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvsrli_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrlr_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrlr_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrlr_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrlr_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrlr_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrlr_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrlr_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrlr_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  V32QI, V32QI, UQI */
+#define __lasx_xvsrlri_b(/*__m256i*/ _1, /*ui3*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvsrlri_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V16HI, V16HI, UQI */
+#define __lasx_xvsrlri_h(/*__m256i*/ _1, /*ui4*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvsrlri_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V8SI, V8SI, UQI */
+#define __lasx_xvsrlri_w(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvsrlri_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V4DI, V4DI, UQI */
+#define __lasx_xvsrlri_d(/*__m256i*/ _1, /*ui6*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvsrlri_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvbitclr_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvbitclr_b((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvbitclr_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvbitclr_h((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvbitclr_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvbitclr_w((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvbitclr_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvbitclr_d((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  UV32QI, UV32QI, UQI */
+#define __lasx_xvbitclri_b(/*__m256i*/ _1, /*ui3*/ _2)                         \
+  ((__m256i)__builtin_lasx_xvbitclri_b((v32u8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  UV16HI, UV16HI, UQI */
+#define __lasx_xvbitclri_h(/*__m256i*/ _1, /*ui4*/ _2)                         \
+  ((__m256i)__builtin_lasx_xvbitclri_h((v16u16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV8SI, UV8SI, UQI */
+#define __lasx_xvbitclri_w(/*__m256i*/ _1, /*ui5*/ _2)                         \
+  ((__m256i)__builtin_lasx_xvbitclri_w((v8u32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  UV4DI, UV4DI, UQI */
+#define __lasx_xvbitclri_d(/*__m256i*/ _1, /*ui6*/ _2)                         \
+  ((__m256i)__builtin_lasx_xvbitclri_d((v4u64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvbitset_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvbitset_b((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvbitset_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvbitset_h((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvbitset_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvbitset_w((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvbitset_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvbitset_d((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  UV32QI, UV32QI, UQI */
+#define __lasx_xvbitseti_b(/*__m256i*/ _1, /*ui3*/ _2)                         \
+  ((__m256i)__builtin_lasx_xvbitseti_b((v32u8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  UV16HI, UV16HI, UQI */
+#define __lasx_xvbitseti_h(/*__m256i*/ _1, /*ui4*/ _2)                         \
+  ((__m256i)__builtin_lasx_xvbitseti_h((v16u16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV8SI, UV8SI, UQI */
+#define __lasx_xvbitseti_w(/*__m256i*/ _1, /*ui5*/ _2)                         \
+  ((__m256i)__builtin_lasx_xvbitseti_w((v8u32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  UV4DI, UV4DI, UQI */
+#define __lasx_xvbitseti_d(/*__m256i*/ _1, /*ui6*/ _2)                         \
+  ((__m256i)__builtin_lasx_xvbitseti_d((v4u64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvbitrev_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvbitrev_b((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvbitrev_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvbitrev_h((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvbitrev_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvbitrev_w((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvbitrev_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvbitrev_d((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  UV32QI, UV32QI, UQI */
+#define __lasx_xvbitrevi_b(/*__m256i*/ _1, /*ui3*/ _2)                         \
+  ((__m256i)__builtin_lasx_xvbitrevi_b((v32u8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  UV16HI, UV16HI, UQI */
+#define __lasx_xvbitrevi_h(/*__m256i*/ _1, /*ui4*/ _2)                         \
+  ((__m256i)__builtin_lasx_xvbitrevi_h((v16u16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV8SI, UV8SI, UQI */
+#define __lasx_xvbitrevi_w(/*__m256i*/ _1, /*ui5*/ _2)                         \
+  ((__m256i)__builtin_lasx_xvbitrevi_w((v8u32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  UV4DI, UV4DI, UQI */
+#define __lasx_xvbitrevi_d(/*__m256i*/ _1, /*ui6*/ _2)                         \
+  ((__m256i)__builtin_lasx_xvbitrevi_d((v4u64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvadd_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvadd_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvadd_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvadd_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvadd_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvadd_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvadd_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvadd_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V32QI, V32QI, UQI */
+#define __lasx_xvaddi_bu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvaddi_bu((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V16HI, V16HI, UQI */
+#define __lasx_xvaddi_hu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvaddi_hu((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V8SI, V8SI, UQI */
+#define __lasx_xvaddi_wu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvaddi_wu((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V4DI, V4DI, UQI */
+#define __lasx_xvaddi_du(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvaddi_du((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsub_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsub_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsub_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsub_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsub_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsub_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsub_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsub_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V32QI, V32QI, UQI */
+#define __lasx_xvsubi_bu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvsubi_bu((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V16HI, V16HI, UQI */
+#define __lasx_xvsubi_hu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvsubi_hu((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V8SI, V8SI, UQI */
+#define __lasx_xvsubi_wu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvsubi_wu((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V4DI, V4DI, UQI */
+#define __lasx_xvsubi_du(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvsubi_du((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmax_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmax_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmax_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmax_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmax_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmax_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmax_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmax_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V32QI, V32QI, QI */
+#define __lasx_xvmaxi_b(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvmaxi_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V16HI, V16HI, QI */
+#define __lasx_xvmaxi_h(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvmaxi_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V8SI, V8SI, QI */
+#define __lasx_xvmaxi_w(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvmaxi_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V4DI, V4DI, QI */
+#define __lasx_xvmaxi_d(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvmaxi_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmax_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmax_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmax_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmax_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmax_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmax_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmax_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmax_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV32QI, UV32QI, UQI */
+#define __lasx_xvmaxi_bu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvmaxi_bu((v32u8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV16HI, UV16HI, UQI */
+#define __lasx_xvmaxi_hu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvmaxi_hu((v16u16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV8SI, UV8SI, UQI */
+#define __lasx_xvmaxi_wu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvmaxi_wu((v8u32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV4DI, UV4DI, UQI */
+#define __lasx_xvmaxi_du(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvmaxi_du((v4u64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmin_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmin_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmin_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmin_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmin_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmin_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmin_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmin_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V32QI, V32QI, QI */
+#define __lasx_xvmini_b(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvmini_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V16HI, V16HI, QI */
+#define __lasx_xvmini_h(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvmini_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V8SI, V8SI, QI */
+#define __lasx_xvmini_w(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvmini_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V4DI, V4DI, QI */
+#define __lasx_xvmini_d(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvmini_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmin_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmin_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmin_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmin_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmin_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmin_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmin_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmin_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV32QI, UV32QI, UQI */
+#define __lasx_xvmini_bu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvmini_bu((v32u8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV16HI, UV16HI, UQI */
+#define __lasx_xvmini_hu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvmini_hu((v16u16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV8SI, UV8SI, UQI */
+#define __lasx_xvmini_wu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvmini_wu((v8u32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV4DI, UV4DI, UQI */
+#define __lasx_xvmini_du(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvmini_du((v4u64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvseq_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvseq_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvseq_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvseq_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvseq_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvseq_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvseq_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvseq_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V32QI, V32QI, QI */
+#define __lasx_xvseqi_b(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvseqi_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V16HI, V16HI, QI */
+#define __lasx_xvseqi_h(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvseqi_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V8SI, V8SI, QI */
+#define __lasx_xvseqi_w(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvseqi_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V4DI, V4DI, QI */
+#define __lasx_xvseqi_d(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvseqi_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvslt_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvslt_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvslt_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvslt_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvslt_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvslt_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvslt_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvslt_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V32QI, V32QI, QI */
+#define __lasx_xvslti_b(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvslti_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V16HI, V16HI, QI */
+#define __lasx_xvslti_h(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvslti_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V8SI, V8SI, QI */
+#define __lasx_xvslti_w(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvslti_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V4DI, V4DI, QI */
+#define __lasx_xvslti_d(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvslti_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvslt_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvslt_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvslt_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvslt_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvslt_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvslt_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvslt_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvslt_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V32QI, UV32QI, UQI */
+#define __lasx_xvslti_bu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvslti_bu((v32u8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V16HI, UV16HI, UQI */
+#define __lasx_xvslti_hu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvslti_hu((v16u16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V8SI, UV8SI, UQI */
+#define __lasx_xvslti_wu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvslti_wu((v8u32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V4DI, UV4DI, UQI */
+#define __lasx_xvslti_du(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvslti_du((v4u64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsle_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsle_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsle_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsle_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsle_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsle_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsle_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsle_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V32QI, V32QI, QI */
+#define __lasx_xvslei_b(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvslei_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V16HI, V16HI, QI */
+#define __lasx_xvslei_h(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvslei_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V8SI, V8SI, QI */
+#define __lasx_xvslei_w(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvslei_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, si5 */
+/* Data types in instruction templates:  V4DI, V4DI, QI */
+#define __lasx_xvslei_d(/*__m256i*/ _1, /*si5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvslei_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsle_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsle_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsle_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsle_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsle_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsle_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsle_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsle_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V32QI, UV32QI, UQI */
+#define __lasx_xvslei_bu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvslei_bu((v32u8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V16HI, UV16HI, UQI */
+#define __lasx_xvslei_hu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvslei_hu((v16u16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V8SI, UV8SI, UQI */
+#define __lasx_xvslei_wu(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvslei_wu((v8u32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V4DI, UV4DI, UQI */
+#define __lasx_xvslei_du(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvslei_du((v4u64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  V32QI, V32QI, UQI */
+#define __lasx_xvsat_b(/*__m256i*/ _1, /*ui3*/ _2)                             \
+  ((__m256i)__builtin_lasx_xvsat_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V16HI, V16HI, UQI */
+#define __lasx_xvsat_h(/*__m256i*/ _1, /*ui4*/ _2)                             \
+  ((__m256i)__builtin_lasx_xvsat_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V8SI, V8SI, UQI */
+#define __lasx_xvsat_w(/*__m256i*/ _1, /*ui5*/ _2)                             \
+  ((__m256i)__builtin_lasx_xvsat_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V4DI, V4DI, UQI */
+#define __lasx_xvsat_d(/*__m256i*/ _1, /*ui6*/ _2)                             \
+  ((__m256i)__builtin_lasx_xvsat_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  UV32QI, UV32QI, UQI */
+#define __lasx_xvsat_bu(/*__m256i*/ _1, /*ui3*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvsat_bu((v32u8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  UV16HI, UV16HI, UQI */
+#define __lasx_xvsat_hu(/*__m256i*/ _1, /*ui4*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvsat_hu((v16u16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV8SI, UV8SI, UQI */
+#define __lasx_xvsat_wu(/*__m256i*/ _1, /*ui5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvsat_wu((v8u32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  UV4DI, UV4DI, UQI */
+#define __lasx_xvsat_du(/*__m256i*/ _1, /*ui6*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvsat_du((v4u64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvadda_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvadda_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvadda_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvadda_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvadda_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvadda_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvadda_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvadda_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsadd_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsadd_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsadd_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsadd_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsadd_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsadd_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsadd_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsadd_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsadd_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsadd_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsadd_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsadd_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsadd_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsadd_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsadd_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsadd_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavg_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavg_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavg_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavg_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavg_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavg_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavg_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavg_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavg_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavg_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavg_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavg_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavg_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavg_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavg_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavg_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavgr_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavgr_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavgr_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavgr_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavgr_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavgr_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavgr_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavgr_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavgr_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavgr_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavgr_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavgr_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavgr_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavgr_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvavgr_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvavgr_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssub_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssub_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssub_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssub_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssub_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssub_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssub_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssub_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssub_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssub_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssub_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssub_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssub_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssub_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssub_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssub_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvabsd_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvabsd_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvabsd_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvabsd_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvabsd_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvabsd_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvabsd_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvabsd_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvabsd_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvabsd_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvabsd_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvabsd_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvabsd_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvabsd_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvabsd_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvabsd_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmul_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmul_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmul_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmul_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmul_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmul_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmul_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmul_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmadd_b(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmadd_b((v32i8)_1, (v32i8)_2, (v32i8)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmadd_h(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmadd_h((v16i16)_1, (v16i16)_2, (v16i16)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmadd_w(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmadd_w((v8i32)_1, (v8i32)_2, (v8i32)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmadd_d(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmadd_d((v4i64)_1, (v4i64)_2, (v4i64)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmsub_b(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmsub_b((v32i8)_1, (v32i8)_2, (v32i8)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmsub_h(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmsub_h((v16i16)_1, (v16i16)_2, (v16i16)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmsub_w(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmsub_w((v8i32)_1, (v8i32)_2, (v8i32)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmsub_d(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmsub_d((v4i64)_1, (v4i64)_2, (v4i64)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvdiv_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvdiv_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvdiv_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvdiv_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvdiv_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvdiv_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvdiv_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvdiv_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvdiv_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvdiv_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvdiv_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvdiv_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvdiv_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvdiv_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvdiv_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvdiv_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhaddw_h_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhaddw_h_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhaddw_w_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhaddw_w_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhaddw_d_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhaddw_d_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhaddw_hu_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhaddw_hu_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhaddw_wu_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhaddw_wu_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhaddw_du_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhaddw_du_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhsubw_h_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhsubw_h_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhsubw_w_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhsubw_w_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhsubw_d_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhsubw_d_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhsubw_hu_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhsubw_hu_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhsubw_wu_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhsubw_wu_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhsubw_du_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhsubw_du_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmod_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmod_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmod_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmod_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmod_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmod_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmod_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmod_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmod_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmod_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmod_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmod_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmod_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmod_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmod_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmod_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V32QI, V32QI, UQI */
+#define __lasx_xvrepl128vei_b(/*__m256i*/ _1, /*ui4*/ _2)                      \
+  ((__m256i)__builtin_lasx_xvrepl128vei_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  V16HI, V16HI, UQI */
+#define __lasx_xvrepl128vei_h(/*__m256i*/ _1, /*ui3*/ _2)                      \
+  ((__m256i)__builtin_lasx_xvrepl128vei_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui2 */
+/* Data types in instruction templates:  V8SI, V8SI, UQI */
+#define __lasx_xvrepl128vei_w(/*__m256i*/ _1, /*ui2*/ _2)                      \
+  ((__m256i)__builtin_lasx_xvrepl128vei_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui1 */
+/* Data types in instruction templates:  V4DI, V4DI, UQI */
+#define __lasx_xvrepl128vei_d(/*__m256i*/ _1, /*ui1*/ _2)                      \
+  ((__m256i)__builtin_lasx_xvrepl128vei_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpickev_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpickev_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpickev_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpickev_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpickev_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpickev_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpickev_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpickev_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpickod_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpickod_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpickod_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpickod_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpickod_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpickod_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpickod_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpickod_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvilvh_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvilvh_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvilvh_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvilvh_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvilvh_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvilvh_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvilvh_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvilvh_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvilvl_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvilvl_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvilvl_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvilvl_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvilvl_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvilvl_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvilvl_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvilvl_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpackev_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpackev_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpackev_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpackev_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpackev_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpackev_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpackev_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpackev_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpackod_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpackod_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpackod_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpackod_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpackod_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpackod_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpackod_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvpackod_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk, xa */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvshuf_b(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvshuf_b((v32i8)_1, (v32i8)_2, (v32i8)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvshuf_h(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvshuf_h((v16i16)_1, (v16i16)_2, (v16i16)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvshuf_w(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvshuf_w((v8i32)_1, (v8i32)_2, (v8i32)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvshuf_d(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvshuf_d((v4i64)_1, (v4i64)_2, (v4i64)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvand_v(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvand_v((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  UV32QI, UV32QI, UQI */
+#define __lasx_xvandi_b(/*__m256i*/ _1, /*ui8*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvandi_b((v32u8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvor_v(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvor_v((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  UV32QI, UV32QI, UQI */
+#define __lasx_xvori_b(/*__m256i*/ _1, /*ui8*/ _2)                             \
+  ((__m256i)__builtin_lasx_xvori_b((v32u8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvnor_v(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvnor_v((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  UV32QI, UV32QI, UQI */
+#define __lasx_xvnori_b(/*__m256i*/ _1, /*ui8*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvnori_b((v32u8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvxor_v(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvxor_v((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  UV32QI, UV32QI, UQI */
+#define __lasx_xvxori_b(/*__m256i*/ _1, /*ui8*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvxori_b((v32u8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk, xa */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvbitsel_v(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvbitsel_v((v32u8)_1, (v32u8)_2, (v32u8)_3);
+}
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI, UQI */
+#define __lasx_xvbitseli_b(/*__m256i*/ _1, /*__m256i*/ _2, /*ui8*/ _3)         \
+  ((__m256i)__builtin_lasx_xvbitseli_b((v32u8)(_1), (v32u8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  V32QI, V32QI, USI */
+#define __lasx_xvshuf4i_b(/*__m256i*/ _1, /*ui8*/ _2)                          \
+  ((__m256i)__builtin_lasx_xvshuf4i_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  V16HI, V16HI, USI */
+#define __lasx_xvshuf4i_h(/*__m256i*/ _1, /*ui8*/ _2)                          \
+  ((__m256i)__builtin_lasx_xvshuf4i_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  V8SI, V8SI, USI */
+#define __lasx_xvshuf4i_w(/*__m256i*/ _1, /*ui8*/ _2)                          \
+  ((__m256i)__builtin_lasx_xvshuf4i_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, rj */
+/* Data types in instruction templates:  V32QI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvreplgr2vr_b(int _1) {
+  return (__m256i)__builtin_lasx_xvreplgr2vr_b((int)_1);
+}
+
+/* Assembly instruction format:          xd, rj */
+/* Data types in instruction templates:  V16HI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvreplgr2vr_h(int _1) {
+  return (__m256i)__builtin_lasx_xvreplgr2vr_h((int)_1);
+}
+
+/* Assembly instruction format:          xd, rj */
+/* Data types in instruction templates:  V8SI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvreplgr2vr_w(int _1) {
+  return (__m256i)__builtin_lasx_xvreplgr2vr_w((int)_1);
+}
+
+/* Assembly instruction format:          xd, rj */
+/* Data types in instruction templates:  V4DI, DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvreplgr2vr_d(long int _1) {
+  return (__m256i)__builtin_lasx_xvreplgr2vr_d((long int)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpcnt_b(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvpcnt_b((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpcnt_h(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvpcnt_h((v16i16)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpcnt_w(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvpcnt_w((v8i32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvpcnt_d(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvpcnt_d((v4i64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvclo_b(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvclo_b((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvclo_h(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvclo_h((v16i16)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvclo_w(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvclo_w((v8i32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvclo_d(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvclo_d((v4i64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvclz_b(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvclz_b((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvclz_h(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvclz_h((v16i16)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvclz_w(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvclz_w((v8i32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvclz_d(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvclz_d((v4i64)_1);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_caf_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_caf_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_caf_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_caf_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cor_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cor_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cor_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cor_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cun_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cun_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cun_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cun_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cune_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cune_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cune_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cune_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cueq_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cueq_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cueq_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cueq_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_ceq_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_ceq_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_ceq_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_ceq_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cne_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cne_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cne_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cne_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_clt_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_clt_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_clt_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_clt_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cult_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cult_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cult_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cult_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cle_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cle_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cle_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cle_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cule_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cule_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_cule_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_cule_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_saf_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_saf_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_saf_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_saf_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sor_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sor_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sor_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sor_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sun_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sun_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sun_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sun_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sune_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sune_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sune_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sune_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sueq_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sueq_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sueq_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sueq_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_seq_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_seq_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_seq_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_seq_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sne_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sne_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sne_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sne_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_slt_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_slt_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_slt_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_slt_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sult_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sult_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sult_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sult_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sle_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sle_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sle_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sle_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sule_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sule_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcmp_sule_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvfcmp_sule_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SF, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfadd_s(__m256 _1, __m256 _2) {
+  return (__m256)__builtin_lasx_xvfadd_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DF, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfadd_d(__m256d _1, __m256d _2) {
+  return (__m256d)__builtin_lasx_xvfadd_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SF, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfsub_s(__m256 _1, __m256 _2) {
+  return (__m256)__builtin_lasx_xvfsub_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DF, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfsub_d(__m256d _1, __m256d _2) {
+  return (__m256d)__builtin_lasx_xvfsub_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SF, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfmul_s(__m256 _1, __m256 _2) {
+  return (__m256)__builtin_lasx_xvfmul_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DF, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfmul_d(__m256d _1, __m256d _2) {
+  return (__m256d)__builtin_lasx_xvfmul_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SF, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfdiv_s(__m256 _1, __m256 _2) {
+  return (__m256)__builtin_lasx_xvfdiv_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DF, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfdiv_d(__m256d _1, __m256d _2) {
+  return (__m256d)__builtin_lasx_xvfdiv_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfcvt_h_s(__m256 _1, __m256 _2) {
+  return (__m256i)__builtin_lasx_xvfcvt_h_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SF, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfcvt_s_d(__m256d _1, __m256d _2) {
+  return (__m256)__builtin_lasx_xvfcvt_s_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SF, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfmin_s(__m256 _1, __m256 _2) {
+  return (__m256)__builtin_lasx_xvfmin_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DF, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfmin_d(__m256d _1, __m256d _2) {
+  return (__m256d)__builtin_lasx_xvfmin_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SF, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfmina_s(__m256 _1, __m256 _2) {
+  return (__m256)__builtin_lasx_xvfmina_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DF, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfmina_d(__m256d _1, __m256d _2) {
+  return (__m256d)__builtin_lasx_xvfmina_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SF, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfmax_s(__m256 _1, __m256 _2) {
+  return (__m256)__builtin_lasx_xvfmax_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DF, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfmax_d(__m256d _1, __m256d _2) {
+  return (__m256d)__builtin_lasx_xvfmax_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SF, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfmaxa_s(__m256 _1, __m256 _2) {
+  return (__m256)__builtin_lasx_xvfmaxa_s((v8f32)_1, (v8f32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DF, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfmaxa_d(__m256d _1, __m256d _2) {
+  return (__m256d)__builtin_lasx_xvfmaxa_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfclass_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvfclass_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfclass_d(__m256d _1) {
+  return (__m256i)__builtin_lasx_xvfclass_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfsqrt_s(__m256 _1) {
+  return (__m256)__builtin_lasx_xvfsqrt_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfsqrt_d(__m256d _1) {
+  return (__m256d)__builtin_lasx_xvfsqrt_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfrecip_s(__m256 _1) {
+  return (__m256)__builtin_lasx_xvfrecip_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfrecip_d(__m256d _1) {
+  return (__m256d)__builtin_lasx_xvfrecip_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfrint_s(__m256 _1) {
+  return (__m256)__builtin_lasx_xvfrint_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfrint_d(__m256d _1) {
+  return (__m256d)__builtin_lasx_xvfrint_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfrsqrt_s(__m256 _1) {
+  return (__m256)__builtin_lasx_xvfrsqrt_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfrsqrt_d(__m256d _1) {
+  return (__m256d)__builtin_lasx_xvfrsqrt_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvflogb_s(__m256 _1) {
+  return (__m256)__builtin_lasx_xvflogb_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvflogb_d(__m256d _1) {
+  return (__m256d)__builtin_lasx_xvflogb_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SF, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfcvth_s_h(__m256i _1) {
+  return (__m256)__builtin_lasx_xvfcvth_s_h((v16i16)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfcvth_d_s(__m256 _1) {
+  return (__m256d)__builtin_lasx_xvfcvth_d_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SF, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfcvtl_s_h(__m256i _1) {
+  return (__m256)__builtin_lasx_xvfcvtl_s_h((v16i16)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfcvtl_d_s(__m256 _1) {
+  return (__m256d)__builtin_lasx_xvfcvtl_d_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftint_w_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftint_w_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftint_l_d(__m256d _1) {
+  return (__m256i)__builtin_lasx_xvftint_l_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  UV8SI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftint_wu_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftint_wu_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  UV4DI, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftint_lu_d(__m256d _1) {
+  return (__m256i)__builtin_lasx_xvftint_lu_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrz_w_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintrz_w_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrz_l_d(__m256d _1) {
+  return (__m256i)__builtin_lasx_xvftintrz_l_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  UV8SI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrz_wu_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintrz_wu_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  UV4DI, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrz_lu_d(__m256d _1) {
+  return (__m256i)__builtin_lasx_xvftintrz_lu_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SF, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvffint_s_w(__m256i _1) {
+  return (__m256)__builtin_lasx_xvffint_s_w((v8i32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DF, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvffint_d_l(__m256i _1) {
+  return (__m256d)__builtin_lasx_xvffint_d_l((v4i64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SF, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvffint_s_wu(__m256i _1) {
+  return (__m256)__builtin_lasx_xvffint_s_wu((v8u32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DF, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvffint_d_lu(__m256i _1) {
+  return (__m256d)__builtin_lasx_xvffint_d_lu((v4u64)_1);
+}
+
+/* Assembly instruction format:          xd, xj, rk */
+/* Data types in instruction templates:  V32QI, V32QI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvreplve_b(__m256i _1, int _2) {
+  return (__m256i)__builtin_lasx_xvreplve_b((v32i8)_1, (int)_2);
+}
+
+/* Assembly instruction format:          xd, xj, rk */
+/* Data types in instruction templates:  V16HI, V16HI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvreplve_h(__m256i _1, int _2) {
+  return (__m256i)__builtin_lasx_xvreplve_h((v16i16)_1, (int)_2);
+}
+
+/* Assembly instruction format:          xd, xj, rk */
+/* Data types in instruction templates:  V8SI, V8SI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvreplve_w(__m256i _1, int _2) {
+  return (__m256i)__builtin_lasx_xvreplve_w((v8i32)_1, (int)_2);
+}
+
+/* Assembly instruction format:          xd, xj, rk */
+/* Data types in instruction templates:  V4DI, V4DI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvreplve_d(__m256i _1, int _2) {
+  return (__m256i)__builtin_lasx_xvreplve_d((v4i64)_1, (int)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, USI */
+#define __lasx_xvpermi_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui8*/ _3)           \
+  ((__m256i)__builtin_lasx_xvpermi_w((v8i32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvandn_v(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvandn_v((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvneg_b(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvneg_b((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvneg_h(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvneg_h((v16i16)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvneg_w(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvneg_w((v8i32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvneg_d(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvneg_d((v4i64)_1);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmuh_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmuh_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmuh_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmuh_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmuh_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmuh_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmuh_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmuh_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmuh_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmuh_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmuh_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmuh_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmuh_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmuh_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmuh_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmuh_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  V16HI, V32QI, UQI */
+#define __lasx_xvsllwil_h_b(/*__m256i*/ _1, /*ui3*/ _2)                        \
+  ((__m256i)__builtin_lasx_xvsllwil_h_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V8SI, V16HI, UQI */
+#define __lasx_xvsllwil_w_h(/*__m256i*/ _1, /*ui4*/ _2)                        \
+  ((__m256i)__builtin_lasx_xvsllwil_w_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V4DI, V8SI, UQI */
+#define __lasx_xvsllwil_d_w(/*__m256i*/ _1, /*ui5*/ _2)                        \
+  ((__m256i)__builtin_lasx_xvsllwil_d_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  UV16HI, UV32QI, UQI */
+#define __lasx_xvsllwil_hu_bu(/*__m256i*/ _1, /*ui3*/ _2)                      \
+  ((__m256i)__builtin_lasx_xvsllwil_hu_bu((v32u8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  UV8SI, UV16HI, UQI */
+#define __lasx_xvsllwil_wu_hu(/*__m256i*/ _1, /*ui4*/ _2)                      \
+  ((__m256i)__builtin_lasx_xvsllwil_wu_hu((v16u16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV4DI, UV8SI, UQI */
+#define __lasx_xvsllwil_du_wu(/*__m256i*/ _1, /*ui5*/ _2)                      \
+  ((__m256i)__builtin_lasx_xvsllwil_du_wu((v8u32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsran_b_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsran_b_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsran_h_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsran_h_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsran_w_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsran_w_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssran_b_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssran_b_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssran_h_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssran_h_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssran_w_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssran_w_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssran_bu_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssran_bu_h((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssran_hu_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssran_hu_w((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssran_wu_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssran_wu_d((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrarn_b_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrarn_b_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrarn_h_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrarn_h_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrarn_w_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrarn_w_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrarn_b_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrarn_b_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrarn_h_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrarn_h_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrarn_w_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrarn_w_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrarn_bu_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrarn_bu_h((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrarn_hu_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrarn_hu_w((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrarn_wu_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrarn_wu_d((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrln_b_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrln_b_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrln_h_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrln_h_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrln_w_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrln_w_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrln_bu_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrln_bu_h((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrln_hu_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrln_hu_w((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrln_wu_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrln_wu_d((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrlrn_b_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrlrn_b_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrlrn_h_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrlrn_h_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsrlrn_w_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsrlrn_w_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV32QI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrlrn_bu_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrlrn_bu_h((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrlrn_hu_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrlrn_hu_w((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrlrn_wu_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrlrn_wu_d((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, UQI */
+#define __lasx_xvfrstpi_b(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)          \
+  ((__m256i)__builtin_lasx_xvfrstpi_b((v32i8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, UQI */
+#define __lasx_xvfrstpi_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)          \
+  ((__m256i)__builtin_lasx_xvfrstpi_h((v16i16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfrstp_b(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvfrstp_b((v32i8)_1, (v32i8)_2, (v32i8)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfrstp_h(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvfrstp_h((v16i16)_1, (v16i16)_2, (v16i16)_3);
+}
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, USI */
+#define __lasx_xvshuf4i_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui8*/ _3)          \
+  ((__m256i)__builtin_lasx_xvshuf4i_d((v4i64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V32QI, V32QI, UQI */
+#define __lasx_xvbsrl_v(/*__m256i*/ _1, /*ui5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvbsrl_v((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V32QI, V32QI, UQI */
+#define __lasx_xvbsll_v(/*__m256i*/ _1, /*ui5*/ _2)                            \
+  ((__m256i)__builtin_lasx_xvbsll_v((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, UQI */
+#define __lasx_xvextrins_b(/*__m256i*/ _1, /*__m256i*/ _2, /*ui8*/ _3)         \
+  ((__m256i)__builtin_lasx_xvextrins_b((v32i8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, UQI */
+#define __lasx_xvextrins_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui8*/ _3)         \
+  ((__m256i)__builtin_lasx_xvextrins_h((v16i16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, UQI */
+#define __lasx_xvextrins_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui8*/ _3)         \
+  ((__m256i)__builtin_lasx_xvextrins_w((v8i32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, UQI */
+#define __lasx_xvextrins_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui8*/ _3)         \
+  ((__m256i)__builtin_lasx_xvextrins_d((v4i64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmskltz_b(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvmskltz_b((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmskltz_h(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvmskltz_h((v16i16)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmskltz_w(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvmskltz_w((v8i32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmskltz_d(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvmskltz_d((v4i64)_1);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsigncov_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsigncov_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsigncov_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsigncov_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsigncov_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsigncov_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsigncov_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsigncov_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk, xa */
+/* Data types in instruction templates:  V8SF, V8SF, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfmadd_s(__m256 _1, __m256 _2, __m256 _3) {
+  return (__m256)__builtin_lasx_xvfmadd_s((v8f32)_1, (v8f32)_2, (v8f32)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk, xa */
+/* Data types in instruction templates:  V4DF, V4DF, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfmadd_d(__m256d _1, __m256d _2, __m256d _3) {
+  return (__m256d)__builtin_lasx_xvfmadd_d((v4f64)_1, (v4f64)_2, (v4f64)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk, xa */
+/* Data types in instruction templates:  V8SF, V8SF, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfmsub_s(__m256 _1, __m256 _2, __m256 _3) {
+  return (__m256)__builtin_lasx_xvfmsub_s((v8f32)_1, (v8f32)_2, (v8f32)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk, xa */
+/* Data types in instruction templates:  V4DF, V4DF, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfmsub_d(__m256d _1, __m256d _2, __m256d _3) {
+  return (__m256d)__builtin_lasx_xvfmsub_d((v4f64)_1, (v4f64)_2, (v4f64)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk, xa */
+/* Data types in instruction templates:  V8SF, V8SF, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfnmadd_s(__m256 _1, __m256 _2, __m256 _3) {
+  return (__m256)__builtin_lasx_xvfnmadd_s((v8f32)_1, (v8f32)_2, (v8f32)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk, xa */
+/* Data types in instruction templates:  V4DF, V4DF, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfnmadd_d(__m256d _1, __m256d _2, __m256d _3) {
+  return (__m256d)__builtin_lasx_xvfnmadd_d((v4f64)_1, (v4f64)_2, (v4f64)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk, xa */
+/* Data types in instruction templates:  V8SF, V8SF, V8SF, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvfnmsub_s(__m256 _1, __m256 _2, __m256 _3) {
+  return (__m256)__builtin_lasx_xvfnmsub_s((v8f32)_1, (v8f32)_2, (v8f32)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk, xa */
+/* Data types in instruction templates:  V4DF, V4DF, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvfnmsub_d(__m256d _1, __m256d _2, __m256d _3) {
+  return (__m256d)__builtin_lasx_xvfnmsub_d((v4f64)_1, (v4f64)_2, (v4f64)_3);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrne_w_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintrne_w_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrne_l_d(__m256d _1) {
+  return (__m256i)__builtin_lasx_xvftintrne_l_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrp_w_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintrp_w_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrp_l_d(__m256d _1) {
+  return (__m256i)__builtin_lasx_xvftintrp_l_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrm_w_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintrm_w_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrm_l_d(__m256d _1) {
+  return (__m256i)__builtin_lasx_xvftintrm_l_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftint_w_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvftint_w_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SF, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256
+    __lasx_xvffint_s_l(__m256i _1, __m256i _2) {
+  return (__m256)__builtin_lasx_xvffint_s_l((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrz_w_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvftintrz_w_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrp_w_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvftintrp_w_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrm_w_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvftintrm_w_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V4DF, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrne_w_d(__m256d _1, __m256d _2) {
+  return (__m256i)__builtin_lasx_xvftintrne_w_d((v4f64)_1, (v4f64)_2);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftinth_l_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftinth_l_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintl_l_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintl_l_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DF, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvffinth_d_w(__m256i _1) {
+  return (__m256d)__builtin_lasx_xvffinth_d_w((v8i32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DF, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256d
+    __lasx_xvffintl_d_w(__m256i _1) {
+  return (__m256d)__builtin_lasx_xvffintl_d_w((v8i32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrzh_l_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintrzh_l_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrzl_l_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintrzl_l_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrph_l_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintrph_l_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrpl_l_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintrpl_l_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrmh_l_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintrmh_l_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrml_l_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintrml_l_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrneh_l_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintrneh_l_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvftintrnel_l_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvftintrnel_l_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfrintrne_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvfrintrne_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfrintrne_d(__m256d _1) {
+  return (__m256i)__builtin_lasx_xvfrintrne_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfrintrz_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvfrintrz_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfrintrz_d(__m256d _1) {
+  return (__m256i)__builtin_lasx_xvfrintrz_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfrintrp_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvfrintrp_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfrintrp_d(__m256d _1) {
+  return (__m256i)__builtin_lasx_xvfrintrp_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfrintrm_s(__m256 _1) {
+  return (__m256i)__builtin_lasx_xvfrintrm_s((v8f32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvfrintrm_d(__m256d _1) {
+  return (__m256i)__builtin_lasx_xvfrintrm_d((v4f64)_1);
+}
+
+/* Assembly instruction format:          xd, rj, si12 */
+/* Data types in instruction templates:  V32QI, CVPOINTER, SI */
+#define __lasx_xvld(/*void **/ _1, /*si12*/ _2)                                \
+  ((__m256i)__builtin_lasx_xvld((void *)(_1), (_2)))
+
+/* Assembly instruction format:          xd, rj, si12 */
+/* Data types in instruction templates:  VOID, V32QI, CVPOINTER, SI */
+#define __lasx_xvst(/*__m256i*/ _1, /*void **/ _2, /*si12*/ _3)                \
+  ((void)__builtin_lasx_xvst((v32i8)(_1), (void *)(_2), (_3)))
+
+/* Assembly instruction format:          xd, rj, si8, idx */
+/* Data types in instruction templates:  VOID, V32QI, CVPOINTER, SI, UQI */
+#define __lasx_xvstelm_b(/*__m256i*/ _1, /*void **/ _2, /*si8*/ _3,            \
+                         /*idx*/ _4)                                           \
+  ((void)__builtin_lasx_xvstelm_b((v32i8)(_1), (void *)(_2), (_3), (_4)))
+
+/* Assembly instruction format:          xd, rj, si8, idx */
+/* Data types in instruction templates:  VOID, V16HI, CVPOINTER, SI, UQI */
+#define __lasx_xvstelm_h(/*__m256i*/ _1, /*void **/ _2, /*si8*/ _3,            \
+                         /*idx*/ _4)                                           \
+  ((void)__builtin_lasx_xvstelm_h((v16i16)(_1), (void *)(_2), (_3), (_4)))
+
+/* Assembly instruction format:          xd, rj, si8, idx */
+/* Data types in instruction templates:  VOID, V8SI, CVPOINTER, SI, UQI */
+#define __lasx_xvstelm_w(/*__m256i*/ _1, /*void **/ _2, /*si8*/ _3,            \
+                         /*idx*/ _4)                                           \
+  ((void)__builtin_lasx_xvstelm_w((v8i32)(_1), (void *)(_2), (_3), (_4)))
+
+/* Assembly instruction format:          xd, rj, si8, idx */
+/* Data types in instruction templates:  VOID, V4DI, CVPOINTER, SI, UQI */
+#define __lasx_xvstelm_d(/*__m256i*/ _1, /*void **/ _2, /*si8*/ _3,            \
+                         /*idx*/ _4)                                           \
+  ((void)__builtin_lasx_xvstelm_d((v4i64)(_1), (void *)(_2), (_3), (_4)))
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, UQI */
+#define __lasx_xvinsve0_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui3*/ _3)          \
+  ((__m256i)__builtin_lasx_xvinsve0_w((v8i32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui2 */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, UQI */
+#define __lasx_xvinsve0_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui2*/ _3)          \
+  ((__m256i)__builtin_lasx_xvinsve0_d((v4i64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  V8SI, V8SI, UQI */
+#define __lasx_xvpickve_w(/*__m256i*/ _1, /*ui3*/ _2)                          \
+  ((__m256i)__builtin_lasx_xvpickve_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui2 */
+/* Data types in instruction templates:  V4DI, V4DI, UQI */
+#define __lasx_xvpickve_d(/*__m256i*/ _1, /*ui2*/ _2)                          \
+  ((__m256i)__builtin_lasx_xvpickve_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrlrn_b_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrlrn_b_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrlrn_h_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrlrn_h_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrlrn_w_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrlrn_w_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrln_b_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrln_b_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrln_h_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrln_h_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvssrln_w_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvssrln_w_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvorn_v(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvorn_v((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, i13 */
+/* Data types in instruction templates:  V4DI, HI */
+#define __lasx_xvldi(/*i13*/ _1) ((__m256i)__builtin_lasx_xvldi((_1)))
+
+/* Assembly instruction format:          xd, rj, rk */
+/* Data types in instruction templates:  V32QI, CVPOINTER, DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvldx(void *_1, long int _2) {
+  return (__m256i)__builtin_lasx_xvldx((void *)_1, (long int)_2);
+}
+
+/* Assembly instruction format:          xd, rj, rk */
+/* Data types in instruction templates:  VOID, V32QI, CVPOINTER, DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) void
+    __lasx_xvstx(__m256i _1, void *_2, long int _3) {
+  return (void)__builtin_lasx_xvstx((v32i8)_1, (void *)_2, (long int)_3);
+}
+
+/* Assembly instruction format:          xd, xj.  */
+/* Data types in instruction templates:  UV4DI, UV4DI.  */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvextl_qu_du(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvextl_qu_du((v4u64)_1);
+}
+
+/* Assembly instruction format:          xd, rj, ui3 */
+/* Data types in instruction templates:  V8SI, V8SI, SI, UQI */
+#define __lasx_xvinsgr2vr_w(/*__m256i*/ _1, /*int*/ _2, /*ui3*/ _3)            \
+  ((__m256i)__builtin_lasx_xvinsgr2vr_w((v8i32)(_1), (int)(_2), (_3)))
+
+/* Assembly instruction format:          xd, rj, ui2 */
+/* Data types in instruction templates:  V4DI, V4DI, DI, UQI */
+#define __lasx_xvinsgr2vr_d(/*__m256i*/ _1, /*long int*/ _2, /*ui2*/ _3)       \
+  ((__m256i)__builtin_lasx_xvinsgr2vr_d((v4i64)(_1), (long int)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvreplve0_b(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvreplve0_b((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvreplve0_h(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvreplve0_h((v16i16)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvreplve0_w(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvreplve0_w((v8i32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvreplve0_d(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvreplve0_d((v4i64)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvreplve0_q(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvreplve0_q((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V16HI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_vext2xv_h_b(__m256i _1) {
+  return (__m256i)__builtin_lasx_vext2xv_h_b((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_vext2xv_w_h(__m256i _1) {
+  return (__m256i)__builtin_lasx_vext2xv_w_h((v16i16)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_vext2xv_d_w(__m256i _1) {
+  return (__m256i)__builtin_lasx_vext2xv_d_w((v8i32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_vext2xv_w_b(__m256i _1) {
+  return (__m256i)__builtin_lasx_vext2xv_w_b((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_vext2xv_d_h(__m256i _1) {
+  return (__m256i)__builtin_lasx_vext2xv_d_h((v16i16)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_vext2xv_d_b(__m256i _1) {
+  return (__m256i)__builtin_lasx_vext2xv_d_b((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V16HI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_vext2xv_hu_bu(__m256i _1) {
+  return (__m256i)__builtin_lasx_vext2xv_hu_bu((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_vext2xv_wu_hu(__m256i _1) {
+  return (__m256i)__builtin_lasx_vext2xv_wu_hu((v16i16)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_vext2xv_du_wu(__m256i _1) {
+  return (__m256i)__builtin_lasx_vext2xv_du_wu((v8i32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_vext2xv_wu_bu(__m256i _1) {
+  return (__m256i)__builtin_lasx_vext2xv_wu_bu((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_vext2xv_du_hu(__m256i _1) {
+  return (__m256i)__builtin_lasx_vext2xv_du_hu((v16i16)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_vext2xv_du_bu(__m256i _1) {
+  return (__m256i)__builtin_lasx_vext2xv_du_bu((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, USI */
+#define __lasx_xvpermi_q(/*__m256i*/ _1, /*__m256i*/ _2, /*ui8*/ _3)           \
+  ((__m256i)__builtin_lasx_xvpermi_q((v32i8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui8 */
+/* Data types in instruction templates:  V4DI, V4DI, USI */
+#define __lasx_xvpermi_d(/*__m256i*/ _1, /*ui8*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvpermi_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvperm_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvperm_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, rj, si12 */
+/* Data types in instruction templates:  V32QI, CVPOINTER, SI */
+#define __lasx_xvldrepl_b(/*void **/ _1, /*si12*/ _2)                          \
+  ((__m256i)__builtin_lasx_xvldrepl_b((void *)(_1), (_2)))
+
+/* Assembly instruction format:          xd, rj, si11 */
+/* Data types in instruction templates:  V16HI, CVPOINTER, SI */
+#define __lasx_xvldrepl_h(/*void **/ _1, /*si11*/ _2)                          \
+  ((__m256i)__builtin_lasx_xvldrepl_h((void *)(_1), (_2)))
+
+/* Assembly instruction format:          xd, rj, si10 */
+/* Data types in instruction templates:  V8SI, CVPOINTER, SI */
+#define __lasx_xvldrepl_w(/*void **/ _1, /*si10*/ _2)                          \
+  ((__m256i)__builtin_lasx_xvldrepl_w((void *)(_1), (_2)))
+
+/* Assembly instruction format:          xd, rj, si9 */
+/* Data types in instruction templates:  V4DI, CVPOINTER, SI */
+#define __lasx_xvldrepl_d(/*void **/ _1, /*si9*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvldrepl_d((void *)(_1), (_2)))
+
+/* Assembly instruction format:          rd, xj, ui3 */
+/* Data types in instruction templates:  SI, V8SI, UQI */
+#define __lasx_xvpickve2gr_w(/*__m256i*/ _1, /*ui3*/ _2)                       \
+  ((int)__builtin_lasx_xvpickve2gr_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          rd, xj, ui3 */
+/* Data types in instruction templates:  USI, V8SI, UQI */
+#define __lasx_xvpickve2gr_wu(/*__m256i*/ _1, /*ui3*/ _2)                      \
+  ((unsigned int)__builtin_lasx_xvpickve2gr_wu((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          rd, xj, ui2 */
+/* Data types in instruction templates:  DI, V4DI, UQI */
+#define __lasx_xvpickve2gr_d(/*__m256i*/ _1, /*ui2*/ _2)                       \
+  ((long int)__builtin_lasx_xvpickve2gr_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          rd, xj, ui2 */
+/* Data types in instruction templates:  UDI, V4DI, UQI */
+#define __lasx_xvpickve2gr_du(/*__m256i*/ _1, /*ui2*/ _2)                      \
+  ((unsigned long int)__builtin_lasx_xvpickve2gr_du((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwev_q_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwev_q_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwev_d_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwev_d_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwev_w_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwev_w_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwev_h_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwev_h_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwev_q_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwev_q_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwev_d_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwev_d_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwev_w_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwev_w_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwev_h_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwev_h_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwev_q_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwev_q_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwev_d_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwev_d_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwev_w_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwev_w_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwev_h_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwev_h_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwev_q_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwev_q_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwev_d_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwev_d_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwev_w_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwev_w_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwev_h_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwev_h_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwev_q_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwev_q_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwev_d_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwev_d_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwev_w_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwev_w_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwev_h_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwev_h_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwev_q_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwev_q_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwev_d_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwev_d_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwev_w_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwev_w_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwev_h_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwev_h_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwod_q_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwod_q_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwod_d_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwod_d_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwod_w_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwod_w_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwod_h_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwod_h_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwod_q_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwod_q_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwod_d_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwod_d_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwod_w_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwod_w_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwod_h_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwod_h_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwod_q_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwod_q_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwod_d_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwod_d_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwod_w_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwod_w_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwod_h_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwod_h_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwod_q_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwod_q_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwod_d_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwod_d_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwod_w_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwod_w_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsubwod_h_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsubwod_h_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwod_q_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwod_q_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwod_d_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwod_d_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwod_w_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwod_w_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwod_h_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwod_h_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwod_q_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwod_q_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwod_d_wu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwod_d_wu((v8u32)_1, (v8u32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwod_w_hu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwod_w_hu((v16u16)_1, (v16u16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwod_h_bu(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwod_h_bu((v32u8)_1, (v32u8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwev_d_wu_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwev_d_wu_w((v8u32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, UV16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwev_w_hu_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwev_w_hu_h((v16u16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, UV32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwev_h_bu_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwev_h_bu_b((v32u8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwev_d_wu_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwev_d_wu_w((v8u32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, UV16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwev_w_hu_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwev_w_hu_h((v16u16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, UV32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwev_h_bu_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwev_h_bu_b((v32u8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwod_d_wu_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwod_d_wu_w((v8u32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, UV16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwod_w_hu_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwod_w_hu_h((v16u16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, UV32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwod_h_bu_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwod_h_bu_b((v32u8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwod_d_wu_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwod_d_wu_w((v8u32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, UV16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwod_w_hu_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwod_w_hu_h((v16u16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, UV32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwod_h_bu_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwod_h_bu_b((v32u8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhaddw_q_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhaddw_q_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhaddw_qu_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhaddw_qu_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhsubw_q_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhsubw_q_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvhsubw_qu_du(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvhsubw_qu_du((v4u64)_1, (v4u64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwev_q_d(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwev_q_d((v4i64)_1, (v4i64)_2, (v4i64)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwev_d_w(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwev_d_w((v4i64)_1, (v8i32)_2, (v8i32)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwev_w_h(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwev_w_h((v8i32)_1, (v16i16)_2,
+                                               (v16i16)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwev_h_b(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwev_h_b((v16i16)_1, (v32i8)_2,
+                                               (v32i8)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwev_q_du(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwev_q_du((v4u64)_1, (v4u64)_2,
+                                                (v4u64)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwev_d_wu(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwev_d_wu((v4u64)_1, (v8u32)_2,
+                                                (v8u32)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwev_w_hu(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwev_w_hu((v8u32)_1, (v16u16)_2,
+                                                (v16u16)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwev_h_bu(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwev_h_bu((v16u16)_1, (v32u8)_2,
+                                                (v32u8)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwod_q_d(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwod_q_d((v4i64)_1, (v4i64)_2, (v4i64)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwod_d_w(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwod_d_w((v4i64)_1, (v8i32)_2, (v8i32)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwod_w_h(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwod_w_h((v8i32)_1, (v16i16)_2,
+                                               (v16i16)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwod_h_b(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwod_h_b((v16i16)_1, (v32i8)_2,
+                                               (v32i8)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV4DI, UV4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwod_q_du(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwod_q_du((v4u64)_1, (v4u64)_2,
+                                                (v4u64)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV4DI, UV4DI, UV8SI, UV8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwod_d_wu(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwod_d_wu((v4u64)_1, (v8u32)_2,
+                                                (v8u32)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV8SI, UV8SI, UV16HI, UV16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwod_w_hu(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwod_w_hu((v8u32)_1, (v16u16)_2,
+                                                (v16u16)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  UV16HI, UV16HI, UV32QI, UV32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwod_h_bu(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwod_h_bu((v16u16)_1, (v32u8)_2,
+                                                (v32u8)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, UV4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwev_q_du_d(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwev_q_du_d((v4i64)_1, (v4u64)_2,
+                                                  (v4i64)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, UV8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwev_d_wu_w(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwev_d_wu_w((v4i64)_1, (v8u32)_2,
+                                                  (v8i32)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, UV16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwev_w_hu_h(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwev_w_hu_h((v8i32)_1, (v16u16)_2,
+                                                  (v16i16)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, UV32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwev_h_bu_b(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwev_h_bu_b((v16i16)_1, (v32u8)_2,
+                                                  (v32i8)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, UV4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwod_q_du_d(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwod_q_du_d((v4i64)_1, (v4u64)_2,
+                                                  (v4i64)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, UV8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwod_d_wu_w(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwod_d_wu_w((v4i64)_1, (v8u32)_2,
+                                                  (v8i32)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, UV16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwod_w_hu_h(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwod_w_hu_h((v8i32)_1, (v16u16)_2,
+                                                  (v16i16)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, UV32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmaddwod_h_bu_b(__m256i _1, __m256i _2, __m256i _3) {
+  return (__m256i)__builtin_lasx_xvmaddwod_h_bu_b((v16i16)_1, (v32u8)_2,
+                                                  (v32i8)_3);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvrotr_b(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvrotr_b((v32i8)_1, (v32i8)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvrotr_h(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvrotr_h((v16i16)_1, (v16i16)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvrotr_w(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvrotr_w((v8i32)_1, (v8i32)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvrotr_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvrotr_d((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvadd_q(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvadd_q((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvsub_q(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvsub_q((v4i64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwev_q_du_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwev_q_du_d((v4u64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvaddwod_q_du_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvaddwod_q_du_d((v4u64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwev_q_du_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwev_q_du_d((v4u64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj, xk */
+/* Data types in instruction templates:  V4DI, UV4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmulwod_q_du_d(__m256i _1, __m256i _2) {
+  return (__m256i)__builtin_lasx_xvmulwod_q_du_d((v4u64)_1, (v4i64)_2);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmskgez_b(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvmskgez_b((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V32QI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvmsknz_b(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvmsknz_b((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V16HI, V32QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvexth_h_b(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvexth_h_b((v32i8)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V8SI, V16HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvexth_w_h(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvexth_w_h((v16i16)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V8SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvexth_d_w(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvexth_d_w((v8i32)_1);
+}
+
+/* Assembly instruction format:          xd, xj */
+/* Data types in instruction templates:  V4DI, V4DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvexth_q_d(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvexth_q_d((v4i64)_1);
+}
+
+/* Assembly instruction format:          xd, xj.  */
+/* Data types in instruction templates:  UV16HI, UV32QI.  */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvexth_hu_bu(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvexth_hu_bu((v32u8)_1);
+}
+
+/* Assembly instruction format:          xd, xj.  */
+/* Data types in instruction templates:  UV8SI, UV16HI.  */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvexth_wu_hu(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvexth_wu_hu((v16u16)_1);
+}
+
+/* Assembly instruction format:          xd, xj.  */
+/* Data types in instruction templates:  UV4DI, UV8SI.  */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvexth_du_wu(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvexth_du_wu((v8u32)_1);
+}
+
+/* Assembly instruction format:          xd, xj.  */
+/* Data types in instruction templates:  UV4DI, UV4DI.  */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvexth_qu_du(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvexth_qu_du((v4u64)_1);
+}
+
+/* Assembly instruction format:          xd, xj, ui3 */
+/* Data types in instruction templates:  V32QI, V32QI, UQI */
+#define __lasx_xvrotri_b(/*__m256i*/ _1, /*ui3*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvrotri_b((v32i8)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V16HI, V16HI, UQI */
+#define __lasx_xvrotri_h(/*__m256i*/ _1, /*ui4*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvrotri_h((v16i16)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V8SI, V8SI, UQI */
+#define __lasx_xvrotri_w(/*__m256i*/ _1, /*ui5*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvrotri_w((v8i32)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V4DI, V4DI, UQI */
+#define __lasx_xvrotri_d(/*__m256i*/ _1, /*ui6*/ _2)                           \
+  ((__m256i)__builtin_lasx_xvrotri_d((v4i64)(_1), (_2)))
+
+/* Assembly instruction format:          xd, xj.  */
+/* Data types in instruction templates:  V4DI, V4DI.  */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m256i
+    __lasx_xvextl_q_d(__m256i _1) {
+  return (__m256i)__builtin_lasx_xvextl_q_d((v4i64)_1);
+}
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, USI */
+#define __lasx_xvsrlni_b_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui4*/ _3)         \
+  ((__m256i)__builtin_lasx_xvsrlni_b_h((v32i8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, USI */
+#define __lasx_xvsrlni_h_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)         \
+  ((__m256i)__builtin_lasx_xvsrlni_h_w((v16i16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, USI */
+#define __lasx_xvsrlni_w_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui6*/ _3)         \
+  ((__m256i)__builtin_lasx_xvsrlni_w_d((v8i32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui7 */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, USI */
+#define __lasx_xvsrlni_d_q(/*__m256i*/ _1, /*__m256i*/ _2, /*ui7*/ _3)         \
+  ((__m256i)__builtin_lasx_xvsrlni_d_q((v4i64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, USI */
+#define __lasx_xvsrlrni_b_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui4*/ _3)        \
+  ((__m256i)__builtin_lasx_xvsrlrni_b_h((v32i8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, USI */
+#define __lasx_xvsrlrni_h_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)        \
+  ((__m256i)__builtin_lasx_xvsrlrni_h_w((v16i16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, USI */
+#define __lasx_xvsrlrni_w_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui6*/ _3)        \
+  ((__m256i)__builtin_lasx_xvsrlrni_w_d((v8i32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui7 */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, USI */
+#define __lasx_xvsrlrni_d_q(/*__m256i*/ _1, /*__m256i*/ _2, /*ui7*/ _3)        \
+  ((__m256i)__builtin_lasx_xvsrlrni_d_q((v4i64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, USI */
+#define __lasx_xvssrlni_b_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui4*/ _3)        \
+  ((__m256i)__builtin_lasx_xvssrlni_b_h((v32i8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, USI */
+#define __lasx_xvssrlni_h_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)        \
+  ((__m256i)__builtin_lasx_xvssrlni_h_w((v16i16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, USI */
+#define __lasx_xvssrlni_w_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui6*/ _3)        \
+  ((__m256i)__builtin_lasx_xvssrlni_w_d((v8i32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui7 */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, USI */
+#define __lasx_xvssrlni_d_q(/*__m256i*/ _1, /*__m256i*/ _2, /*ui7*/ _3)        \
+  ((__m256i)__builtin_lasx_xvssrlni_d_q((v4i64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  UV32QI, UV32QI, V32QI, USI */
+#define __lasx_xvssrlni_bu_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui4*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrlni_bu_h((v32u8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV16HI, UV16HI, V16HI, USI */
+#define __lasx_xvssrlni_hu_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrlni_hu_w((v16u16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  UV8SI, UV8SI, V8SI, USI */
+#define __lasx_xvssrlni_wu_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui6*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrlni_wu_d((v8u32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui7 */
+/* Data types in instruction templates:  UV4DI, UV4DI, V4DI, USI */
+#define __lasx_xvssrlni_du_q(/*__m256i*/ _1, /*__m256i*/ _2, /*ui7*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrlni_du_q((v4u64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, USI */
+#define __lasx_xvssrlrni_b_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui4*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrlrni_b_h((v32i8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, USI */
+#define __lasx_xvssrlrni_h_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrlrni_h_w((v16i16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, USI */
+#define __lasx_xvssrlrni_w_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui6*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrlrni_w_d((v8i32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui7 */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, USI */
+#define __lasx_xvssrlrni_d_q(/*__m256i*/ _1, /*__m256i*/ _2, /*ui7*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrlrni_d_q((v4i64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  UV32QI, UV32QI, V32QI, USI */
+#define __lasx_xvssrlrni_bu_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui4*/ _3)      \
+  ((__m256i)__builtin_lasx_xvssrlrni_bu_h((v32u8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV16HI, UV16HI, V16HI, USI */
+#define __lasx_xvssrlrni_hu_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)      \
+  ((__m256i)__builtin_lasx_xvssrlrni_hu_w((v16u16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  UV8SI, UV8SI, V8SI, USI */
+#define __lasx_xvssrlrni_wu_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui6*/ _3)      \
+  ((__m256i)__builtin_lasx_xvssrlrni_wu_d((v8u32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui7 */
+/* Data types in instruction templates:  UV4DI, UV4DI, V4DI, USI */
+#define __lasx_xvssrlrni_du_q(/*__m256i*/ _1, /*__m256i*/ _2, /*ui7*/ _3)      \
+  ((__m256i)__builtin_lasx_xvssrlrni_du_q((v4u64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, USI */
+#define __lasx_xvsrani_b_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui4*/ _3)         \
+  ((__m256i)__builtin_lasx_xvsrani_b_h((v32i8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, USI */
+#define __lasx_xvsrani_h_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)         \
+  ((__m256i)__builtin_lasx_xvsrani_h_w((v16i16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, USI */
+#define __lasx_xvsrani_w_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui6*/ _3)         \
+  ((__m256i)__builtin_lasx_xvsrani_w_d((v8i32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui7 */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, USI */
+#define __lasx_xvsrani_d_q(/*__m256i*/ _1, /*__m256i*/ _2, /*ui7*/ _3)         \
+  ((__m256i)__builtin_lasx_xvsrani_d_q((v4i64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, USI */
+#define __lasx_xvsrarni_b_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui4*/ _3)        \
+  ((__m256i)__builtin_lasx_xvsrarni_b_h((v32i8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, USI */
+#define __lasx_xvsrarni_h_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)        \
+  ((__m256i)__builtin_lasx_xvsrarni_h_w((v16i16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, USI */
+#define __lasx_xvsrarni_w_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui6*/ _3)        \
+  ((__m256i)__builtin_lasx_xvsrarni_w_d((v8i32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui7 */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, USI */
+#define __lasx_xvsrarni_d_q(/*__m256i*/ _1, /*__m256i*/ _2, /*ui7*/ _3)        \
+  ((__m256i)__builtin_lasx_xvsrarni_d_q((v4i64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, USI */
+#define __lasx_xvssrani_b_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui4*/ _3)        \
+  ((__m256i)__builtin_lasx_xvssrani_b_h((v32i8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, USI */
+#define __lasx_xvssrani_h_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)        \
+  ((__m256i)__builtin_lasx_xvssrani_h_w((v16i16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, USI */
+#define __lasx_xvssrani_w_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui6*/ _3)        \
+  ((__m256i)__builtin_lasx_xvssrani_w_d((v8i32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui7 */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, USI */
+#define __lasx_xvssrani_d_q(/*__m256i*/ _1, /*__m256i*/ _2, /*ui7*/ _3)        \
+  ((__m256i)__builtin_lasx_xvssrani_d_q((v4i64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  UV32QI, UV32QI, V32QI, USI */
+#define __lasx_xvssrani_bu_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui4*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrani_bu_h((v32u8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV16HI, UV16HI, V16HI, USI */
+#define __lasx_xvssrani_hu_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrani_hu_w((v16u16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  UV8SI, UV8SI, V8SI, USI */
+#define __lasx_xvssrani_wu_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui6*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrani_wu_d((v8u32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui7 */
+/* Data types in instruction templates:  UV4DI, UV4DI, V4DI, USI */
+#define __lasx_xvssrani_du_q(/*__m256i*/ _1, /*__m256i*/ _2, /*ui7*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrani_du_q((v4u64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  V32QI, V32QI, V32QI, USI */
+#define __lasx_xvssrarni_b_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui4*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrarni_b_h((v32i8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  V16HI, V16HI, V16HI, USI */
+#define __lasx_xvssrarni_h_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrarni_h_w((v16i16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  V8SI, V8SI, V8SI, USI */
+#define __lasx_xvssrarni_w_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui6*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrarni_w_d((v8i32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui7 */
+/* Data types in instruction templates:  V4DI, V4DI, V4DI, USI */
+#define __lasx_xvssrarni_d_q(/*__m256i*/ _1, /*__m256i*/ _2, /*ui7*/ _3)       \
+  ((__m256i)__builtin_lasx_xvssrarni_d_q((v4i64)(_1), (v4i64)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui4 */
+/* Data types in instruction templates:  UV32QI, UV32QI, V32QI, USI */
+#define __lasx_xvssrarni_bu_h(/*__m256i*/ _1, /*__m256i*/ _2, /*ui4*/ _3)      \
+  ((__m256i)__builtin_lasx_xvssrarni_bu_h((v32u8)(_1), (v32i8)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui5 */
+/* Data types in instruction templates:  UV16HI, UV16HI, V16HI, USI */
+#define __lasx_xvssrarni_hu_w(/*__m256i*/ _1, /*__m256i*/ _2, /*ui5*/ _3)      \
+  ((__m256i)__builtin_lasx_xvssrarni_hu_w((v16u16)(_1), (v16i16)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui6 */
+/* Data types in instruction templates:  UV8SI, UV8SI, V8SI, USI */
+#define __lasx_xvssrarni_wu_d(/*__m256i*/ _1, /*__m256i*/ _2, /*ui6*/ _3)      \
+  ((__m256i)__builtin_lasx_xvssrarni_wu_d((v8u32)(_1), (v8i32)(_2), (_3)))
+
+/* Assembly instruction format:          xd, xj, ui7 */
+/* Data types in instruction templates:  UV4DI, UV4DI, V4DI, USI */
+#define __lasx_xvssrarni_du_q(/*__m256i*/ _1, /*__m256i*/ _2, /*ui7*/ _3)      \
+  ((__m256i)__builtin_lasx_xvssrarni_du_q((v4u64)(_1), (v4i64)(_2), (_3)))
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lasx_xbnz_v(__m256i _1) {
+  return __builtin_lasx_xbnz_v((v32u8)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lasx_xbz_v(__m256i _1) {
+  return __builtin_lasx_xbz_v((v32u8)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lasx_xbnz_b(__m256i _1) {
+  return __builtin_lasx_xbnz_b((v32u8)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lasx_xbnz_h(__m256i _1) {
+  return __builtin_lasx_xbnz_h((v16u16)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lasx_xbnz_w(__m256i _1) {
+  return __builtin_lasx_xbnz_w((v8u32)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lasx_xbnz_d(__m256i _1) {
+  return __builtin_lasx_xbnz_d((v4u64)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lasx_xbz_b(__m256i _1) {
+  return __builtin_lasx_xbz_b((v32u8)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lasx_xbz_h(__m256i _1) {
+  return __builtin_lasx_xbz_h((v16u16)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lasx_xbz_w(__m256i _1) {
+  return __builtin_lasx_xbz_w((v8u32)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lasx_xbz_d(__m256i _1) {
+  return __builtin_lasx_xbz_d((v4u64)_1);
+}
+
+#if 0
+/* Assembly instruction format:          vd, i10 */
+/* Data types in instruction templates:  V32QI, i10 */
+#define __lasx_xvrepli_b(/*i10*/ _1)	((__m256i)__builtin_lasx_xvrepli_b(_1)
+
+/* Assembly instruction format:          vd, i10 */
+/* Data types in instruction templates:  V16HI, i10 */
+#define __lasx_xvrepli_h(/*i10*/ _1)	((__m256i)__builtin_lasx_xvrepli_h(_1)
+
+/* Assembly instruction format:          vd, i10 */
+/* Data types in instruction templates:  V8SI, i10 */
+#define __lasx_xvrepli_w(/*i10*/ _1)	((__m256i)__builtin_lasx_xvrepli_w(_1)
+
+/* Assembly instruction format:          vd, i10 */
+/* Data types in instruction templates:  V4DI, i10 */
+#define __lasx_xvrepli_d(/*i10*/ _1)	((__m256i)__builtin_lasx_xvrepli_d(_1)
+#endif
+
+#endif /* defined(__loongarch_asx) */
+#endif /* _GCC_LOONGSON_ASXINTRIN_H */
diff --git a/lib/Headers/lsxintrin.h b/lib/Headers/lsxintrin.h
new file mode 100644
index 00000000..bd5f15a0
--- /dev/null
+++ b/lib/Headers/lsxintrin.h
@@ -0,0 +1,5165 @@
+//===----------- lsxintrin.h - LoongArch LSX intrinsics ------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the LoongArch LSX intrinsics.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef _GCC_LOONGSON_SXINTRIN_H
+#define _GCC_LOONGSON_SXINTRIN_H 1
+
+#if defined(__loongarch_sx)
+typedef signed char v16i8 __attribute__((vector_size(16), aligned(16)));
+typedef signed char v16i8_b __attribute__((vector_size(16), aligned(1)));
+typedef unsigned char v16u8 __attribute__((vector_size(16), aligned(16)));
+typedef unsigned char v16u8_b __attribute__((vector_size(16), aligned(1)));
+typedef short v8i16 __attribute__((vector_size(16), aligned(16)));
+typedef short v8i16_h __attribute__((vector_size(16), aligned(2)));
+typedef unsigned short v8u16 __attribute__((vector_size(16), aligned(16)));
+typedef unsigned short v8u16_h __attribute__((vector_size(16), aligned(2)));
+typedef int v4i32 __attribute__((vector_size(16), aligned(16)));
+typedef int v4i32_w __attribute__((vector_size(16), aligned(4)));
+typedef unsigned int v4u32 __attribute__((vector_size(16), aligned(16)));
+typedef unsigned int v4u32_w __attribute__((vector_size(16), aligned(4)));
+typedef long long v2i64 __attribute__((vector_size(16), aligned(16)));
+typedef long long v2i64_d __attribute__((vector_size(16), aligned(8)));
+typedef unsigned long long v2u64 __attribute__((vector_size(16), aligned(16)));
+typedef unsigned long long v2u64_d __attribute__((vector_size(16), aligned(8)));
+typedef float v4f32 __attribute__((vector_size(16), aligned(16)));
+typedef float v4f32_w __attribute__((vector_size(16), aligned(4)));
+typedef double v2f64 __attribute__((vector_size(16), aligned(16)));
+typedef double v2f64_d __attribute__((vector_size(16), aligned(8)));
+
+typedef long long __m128i __attribute__((__vector_size__(16), __may_alias__));
+typedef float __m128 __attribute__((__vector_size__(16), __may_alias__));
+typedef double __m128d __attribute__((__vector_size__(16), __may_alias__));
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsll_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsll_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsll_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsll_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsll_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsll_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsll_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsll_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  V16QI, V16QI, UQI */
+#define __lsx_vslli_b(/*__m128i*/ _1, /*ui3*/ _2)                              \
+  ((__m128i)__builtin_lsx_vslli_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V8HI, V8HI, UQI */
+#define __lsx_vslli_h(/*__m128i*/ _1, /*ui4*/ _2)                              \
+  ((__m128i)__builtin_lsx_vslli_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V4SI, V4SI, UQI */
+#define __lsx_vslli_w(/*__m128i*/ _1, /*ui5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vslli_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V2DI, V2DI, UQI */
+#define __lsx_vslli_d(/*__m128i*/ _1, /*ui6*/ _2)                              \
+  ((__m128i)__builtin_lsx_vslli_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsra_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsra_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsra_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsra_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsra_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsra_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsra_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsra_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  V16QI, V16QI, UQI */
+#define __lsx_vsrai_b(/*__m128i*/ _1, /*ui3*/ _2)                              \
+  ((__m128i)__builtin_lsx_vsrai_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V8HI, V8HI, UQI */
+#define __lsx_vsrai_h(/*__m128i*/ _1, /*ui4*/ _2)                              \
+  ((__m128i)__builtin_lsx_vsrai_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V4SI, V4SI, UQI */
+#define __lsx_vsrai_w(/*__m128i*/ _1, /*ui5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vsrai_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V2DI, V2DI, UQI */
+#define __lsx_vsrai_d(/*__m128i*/ _1, /*ui6*/ _2)                              \
+  ((__m128i)__builtin_lsx_vsrai_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrar_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrar_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrar_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrar_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrar_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrar_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrar_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrar_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  V16QI, V16QI, UQI */
+#define __lsx_vsrari_b(/*__m128i*/ _1, /*ui3*/ _2)                             \
+  ((__m128i)__builtin_lsx_vsrari_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V8HI, V8HI, UQI */
+#define __lsx_vsrari_h(/*__m128i*/ _1, /*ui4*/ _2)                             \
+  ((__m128i)__builtin_lsx_vsrari_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V4SI, V4SI, UQI */
+#define __lsx_vsrari_w(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vsrari_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V2DI, V2DI, UQI */
+#define __lsx_vsrari_d(/*__m128i*/ _1, /*ui6*/ _2)                             \
+  ((__m128i)__builtin_lsx_vsrari_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrl_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrl_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrl_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrl_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrl_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrl_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrl_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrl_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  V16QI, V16QI, UQI */
+#define __lsx_vsrli_b(/*__m128i*/ _1, /*ui3*/ _2)                              \
+  ((__m128i)__builtin_lsx_vsrli_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V8HI, V8HI, UQI */
+#define __lsx_vsrli_h(/*__m128i*/ _1, /*ui4*/ _2)                              \
+  ((__m128i)__builtin_lsx_vsrli_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V4SI, V4SI, UQI */
+#define __lsx_vsrli_w(/*__m128i*/ _1, /*ui5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vsrli_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V2DI, V2DI, UQI */
+#define __lsx_vsrli_d(/*__m128i*/ _1, /*ui6*/ _2)                              \
+  ((__m128i)__builtin_lsx_vsrli_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrlr_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrlr_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrlr_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrlr_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrlr_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrlr_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrlr_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrlr_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  V16QI, V16QI, UQI */
+#define __lsx_vsrlri_b(/*__m128i*/ _1, /*ui3*/ _2)                             \
+  ((__m128i)__builtin_lsx_vsrlri_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V8HI, V8HI, UQI */
+#define __lsx_vsrlri_h(/*__m128i*/ _1, /*ui4*/ _2)                             \
+  ((__m128i)__builtin_lsx_vsrlri_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V4SI, V4SI, UQI */
+#define __lsx_vsrlri_w(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vsrlri_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V2DI, V2DI, UQI */
+#define __lsx_vsrlri_d(/*__m128i*/ _1, /*ui6*/ _2)                             \
+  ((__m128i)__builtin_lsx_vsrlri_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vbitclr_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vbitclr_b((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vbitclr_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vbitclr_h((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vbitclr_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vbitclr_w((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vbitclr_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vbitclr_d((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  UV16QI, UV16QI, UQI */
+#define __lsx_vbitclri_b(/*__m128i*/ _1, /*ui3*/ _2)                           \
+  ((__m128i)__builtin_lsx_vbitclri_b((v16u8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  UV8HI, UV8HI, UQI */
+#define __lsx_vbitclri_h(/*__m128i*/ _1, /*ui4*/ _2)                           \
+  ((__m128i)__builtin_lsx_vbitclri_h((v8u16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV4SI, UV4SI, UQI */
+#define __lsx_vbitclri_w(/*__m128i*/ _1, /*ui5*/ _2)                           \
+  ((__m128i)__builtin_lsx_vbitclri_w((v4u32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  UV2DI, UV2DI, UQI */
+#define __lsx_vbitclri_d(/*__m128i*/ _1, /*ui6*/ _2)                           \
+  ((__m128i)__builtin_lsx_vbitclri_d((v2u64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vbitset_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vbitset_b((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vbitset_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vbitset_h((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vbitset_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vbitset_w((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vbitset_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vbitset_d((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  UV16QI, UV16QI, UQI */
+#define __lsx_vbitseti_b(/*__m128i*/ _1, /*ui3*/ _2)                           \
+  ((__m128i)__builtin_lsx_vbitseti_b((v16u8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  UV8HI, UV8HI, UQI */
+#define __lsx_vbitseti_h(/*__m128i*/ _1, /*ui4*/ _2)                           \
+  ((__m128i)__builtin_lsx_vbitseti_h((v8u16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV4SI, UV4SI, UQI */
+#define __lsx_vbitseti_w(/*__m128i*/ _1, /*ui5*/ _2)                           \
+  ((__m128i)__builtin_lsx_vbitseti_w((v4u32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  UV2DI, UV2DI, UQI */
+#define __lsx_vbitseti_d(/*__m128i*/ _1, /*ui6*/ _2)                           \
+  ((__m128i)__builtin_lsx_vbitseti_d((v2u64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vbitrev_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vbitrev_b((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vbitrev_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vbitrev_h((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vbitrev_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vbitrev_w((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vbitrev_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vbitrev_d((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  UV16QI, UV16QI, UQI */
+#define __lsx_vbitrevi_b(/*__m128i*/ _1, /*ui3*/ _2)                           \
+  ((__m128i)__builtin_lsx_vbitrevi_b((v16u8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  UV8HI, UV8HI, UQI */
+#define __lsx_vbitrevi_h(/*__m128i*/ _1, /*ui4*/ _2)                           \
+  ((__m128i)__builtin_lsx_vbitrevi_h((v8u16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV4SI, UV4SI, UQI */
+#define __lsx_vbitrevi_w(/*__m128i*/ _1, /*ui5*/ _2)                           \
+  ((__m128i)__builtin_lsx_vbitrevi_w((v4u32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  UV2DI, UV2DI, UQI */
+#define __lsx_vbitrevi_d(/*__m128i*/ _1, /*ui6*/ _2)                           \
+  ((__m128i)__builtin_lsx_vbitrevi_d((v2u64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vadd_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vadd_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vadd_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vadd_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vadd_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vadd_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vadd_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vadd_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V16QI, V16QI, UQI */
+#define __lsx_vaddi_bu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vaddi_bu((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V8HI, V8HI, UQI */
+#define __lsx_vaddi_hu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vaddi_hu((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V4SI, V4SI, UQI */
+#define __lsx_vaddi_wu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vaddi_wu((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V2DI, V2DI, UQI */
+#define __lsx_vaddi_du(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vaddi_du((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsub_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsub_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsub_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsub_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsub_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsub_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsub_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsub_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V16QI, V16QI, UQI */
+#define __lsx_vsubi_bu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vsubi_bu((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V8HI, V8HI, UQI */
+#define __lsx_vsubi_hu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vsubi_hu((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V4SI, V4SI, UQI */
+#define __lsx_vsubi_wu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vsubi_wu((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V2DI, V2DI, UQI */
+#define __lsx_vsubi_du(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vsubi_du((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmax_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmax_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmax_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmax_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmax_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmax_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmax_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmax_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V16QI, V16QI, QI */
+#define __lsx_vmaxi_b(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vmaxi_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V8HI, V8HI, QI */
+#define __lsx_vmaxi_h(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vmaxi_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V4SI, V4SI, QI */
+#define __lsx_vmaxi_w(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vmaxi_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V2DI, V2DI, QI */
+#define __lsx_vmaxi_d(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vmaxi_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmax_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmax_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmax_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmax_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmax_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmax_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmax_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmax_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV16QI, UV16QI, UQI */
+#define __lsx_vmaxi_bu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vmaxi_bu((v16u8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV8HI, UV8HI, UQI */
+#define __lsx_vmaxi_hu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vmaxi_hu((v8u16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV4SI, UV4SI, UQI */
+#define __lsx_vmaxi_wu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vmaxi_wu((v4u32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV2DI, UV2DI, UQI */
+#define __lsx_vmaxi_du(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vmaxi_du((v2u64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmin_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmin_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmin_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmin_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmin_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmin_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmin_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmin_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V16QI, V16QI, QI */
+#define __lsx_vmini_b(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vmini_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V8HI, V8HI, QI */
+#define __lsx_vmini_h(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vmini_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V4SI, V4SI, QI */
+#define __lsx_vmini_w(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vmini_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V2DI, V2DI, QI */
+#define __lsx_vmini_d(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vmini_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmin_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmin_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmin_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmin_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmin_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmin_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmin_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmin_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV16QI, UV16QI, UQI */
+#define __lsx_vmini_bu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vmini_bu((v16u8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV8HI, UV8HI, UQI */
+#define __lsx_vmini_hu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vmini_hu((v8u16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV4SI, UV4SI, UQI */
+#define __lsx_vmini_wu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vmini_wu((v4u32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV2DI, UV2DI, UQI */
+#define __lsx_vmini_du(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vmini_du((v2u64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vseq_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vseq_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vseq_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vseq_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vseq_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vseq_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vseq_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vseq_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V16QI, V16QI, QI */
+#define __lsx_vseqi_b(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vseqi_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V8HI, V8HI, QI */
+#define __lsx_vseqi_h(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vseqi_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V4SI, V4SI, QI */
+#define __lsx_vseqi_w(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vseqi_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V2DI, V2DI, QI */
+#define __lsx_vseqi_d(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vseqi_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V16QI, V16QI, QI */
+#define __lsx_vslti_b(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vslti_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vslt_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vslt_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vslt_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vslt_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vslt_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vslt_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vslt_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vslt_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V8HI, V8HI, QI */
+#define __lsx_vslti_h(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vslti_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V4SI, V4SI, QI */
+#define __lsx_vslti_w(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vslti_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V2DI, V2DI, QI */
+#define __lsx_vslti_d(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vslti_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vslt_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vslt_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vslt_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vslt_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vslt_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vslt_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vslt_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vslt_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V16QI, UV16QI, UQI */
+#define __lsx_vslti_bu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vslti_bu((v16u8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V8HI, UV8HI, UQI */
+#define __lsx_vslti_hu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vslti_hu((v8u16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V4SI, UV4SI, UQI */
+#define __lsx_vslti_wu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vslti_wu((v4u32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V2DI, UV2DI, UQI */
+#define __lsx_vslti_du(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vslti_du((v2u64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsle_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsle_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsle_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsle_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsle_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsle_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsle_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsle_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V16QI, V16QI, QI */
+#define __lsx_vslei_b(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vslei_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V8HI, V8HI, QI */
+#define __lsx_vslei_h(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vslei_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V4SI, V4SI, QI */
+#define __lsx_vslei_w(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vslei_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, si5 */
+/* Data types in instruction templates:  V2DI, V2DI, QI */
+#define __lsx_vslei_d(/*__m128i*/ _1, /*si5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vslei_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsle_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsle_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsle_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsle_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsle_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsle_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsle_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsle_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V16QI, UV16QI, UQI */
+#define __lsx_vslei_bu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vslei_bu((v16u8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V8HI, UV8HI, UQI */
+#define __lsx_vslei_hu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vslei_hu((v8u16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V4SI, UV4SI, UQI */
+#define __lsx_vslei_wu(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vslei_wu((v4u32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V2DI, UV2DI, UQI */
+#define __lsx_vslei_du(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vslei_du((v2u64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  V16QI, V16QI, UQI */
+#define __lsx_vsat_b(/*__m128i*/ _1, /*ui3*/ _2)                               \
+  ((__m128i)__builtin_lsx_vsat_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V8HI, V8HI, UQI */
+#define __lsx_vsat_h(/*__m128i*/ _1, /*ui4*/ _2)                               \
+  ((__m128i)__builtin_lsx_vsat_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V4SI, V4SI, UQI */
+#define __lsx_vsat_w(/*__m128i*/ _1, /*ui5*/ _2)                               \
+  ((__m128i)__builtin_lsx_vsat_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V2DI, V2DI, UQI */
+#define __lsx_vsat_d(/*__m128i*/ _1, /*ui6*/ _2)                               \
+  ((__m128i)__builtin_lsx_vsat_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  UV16QI, UV16QI, UQI */
+#define __lsx_vsat_bu(/*__m128i*/ _1, /*ui3*/ _2)                              \
+  ((__m128i)__builtin_lsx_vsat_bu((v16u8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  UV8HI, UV8HI, UQI */
+#define __lsx_vsat_hu(/*__m128i*/ _1, /*ui4*/ _2)                              \
+  ((__m128i)__builtin_lsx_vsat_hu((v8u16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV4SI, UV4SI, UQI */
+#define __lsx_vsat_wu(/*__m128i*/ _1, /*ui5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vsat_wu((v4u32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  UV2DI, UV2DI, UQI */
+#define __lsx_vsat_du(/*__m128i*/ _1, /*ui6*/ _2)                              \
+  ((__m128i)__builtin_lsx_vsat_du((v2u64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vadda_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vadda_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vadda_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vadda_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vadda_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vadda_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vadda_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vadda_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsadd_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsadd_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsadd_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsadd_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsadd_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsadd_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsadd_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsadd_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsadd_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsadd_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsadd_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsadd_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsadd_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsadd_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsadd_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsadd_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavg_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavg_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavg_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavg_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavg_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavg_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavg_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavg_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavg_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavg_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavg_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavg_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavg_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavg_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavg_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavg_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavgr_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavgr_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavgr_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavgr_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavgr_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavgr_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavgr_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavgr_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavgr_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavgr_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavgr_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavgr_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavgr_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavgr_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vavgr_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vavgr_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssub_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssub_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssub_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssub_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssub_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssub_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssub_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssub_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssub_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssub_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssub_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssub_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssub_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssub_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssub_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssub_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vabsd_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vabsd_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vabsd_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vabsd_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vabsd_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vabsd_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vabsd_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vabsd_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vabsd_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vabsd_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vabsd_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vabsd_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vabsd_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vabsd_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vabsd_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vabsd_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmul_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmul_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmul_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmul_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmul_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmul_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmul_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmul_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmadd_b(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmadd_b((v16i8)_1, (v16i8)_2, (v16i8)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmadd_h(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmadd_h((v8i16)_1, (v8i16)_2, (v8i16)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmadd_w(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmadd_w((v4i32)_1, (v4i32)_2, (v4i32)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmadd_d(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmadd_d((v2i64)_1, (v2i64)_2, (v2i64)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmsub_b(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmsub_b((v16i8)_1, (v16i8)_2, (v16i8)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmsub_h(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmsub_h((v8i16)_1, (v8i16)_2, (v8i16)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmsub_w(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmsub_w((v4i32)_1, (v4i32)_2, (v4i32)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmsub_d(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmsub_d((v2i64)_1, (v2i64)_2, (v2i64)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vdiv_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vdiv_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vdiv_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vdiv_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vdiv_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vdiv_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vdiv_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vdiv_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vdiv_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vdiv_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vdiv_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vdiv_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vdiv_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vdiv_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vdiv_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vdiv_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhaddw_h_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhaddw_h_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhaddw_w_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhaddw_w_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhaddw_d_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhaddw_d_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhaddw_hu_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhaddw_hu_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhaddw_wu_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhaddw_wu_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhaddw_du_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhaddw_du_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhsubw_h_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhsubw_h_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhsubw_w_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhsubw_w_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhsubw_d_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhsubw_d_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhsubw_hu_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhsubw_hu_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhsubw_wu_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhsubw_wu_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhsubw_du_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhsubw_du_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmod_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmod_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmod_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmod_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmod_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmod_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmod_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmod_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmod_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmod_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmod_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmod_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmod_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmod_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmod_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmod_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, rk */
+/* Data types in instruction templates:  V16QI, V16QI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vreplve_b(__m128i _1, int _2) {
+  return (__m128i)__builtin_lsx_vreplve_b((v16i8)_1, (int)_2);
+}
+
+/* Assembly instruction format:          vd, vj, rk */
+/* Data types in instruction templates:  V8HI, V8HI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vreplve_h(__m128i _1, int _2) {
+  return (__m128i)__builtin_lsx_vreplve_h((v8i16)_1, (int)_2);
+}
+
+/* Assembly instruction format:          vd, vj, rk */
+/* Data types in instruction templates:  V4SI, V4SI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vreplve_w(__m128i _1, int _2) {
+  return (__m128i)__builtin_lsx_vreplve_w((v4i32)_1, (int)_2);
+}
+
+/* Assembly instruction format:          vd, vj, rk */
+/* Data types in instruction templates:  V2DI, V2DI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vreplve_d(__m128i _1, int _2) {
+  return (__m128i)__builtin_lsx_vreplve_d((v2i64)_1, (int)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V16QI, V16QI, UQI */
+#define __lsx_vreplvei_b(/*__m128i*/ _1, /*ui4*/ _2)                           \
+  ((__m128i)__builtin_lsx_vreplvei_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  V8HI, V8HI, UQI */
+#define __lsx_vreplvei_h(/*__m128i*/ _1, /*ui3*/ _2)                           \
+  ((__m128i)__builtin_lsx_vreplvei_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui2 */
+/* Data types in instruction templates:  V4SI, V4SI, UQI */
+#define __lsx_vreplvei_w(/*__m128i*/ _1, /*ui2*/ _2)                           \
+  ((__m128i)__builtin_lsx_vreplvei_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui1 */
+/* Data types in instruction templates:  V2DI, V2DI, UQI */
+#define __lsx_vreplvei_d(/*__m128i*/ _1, /*ui1*/ _2)                           \
+  ((__m128i)__builtin_lsx_vreplvei_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpickev_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpickev_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpickev_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpickev_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpickev_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpickev_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpickev_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpickev_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpickod_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpickod_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpickod_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpickod_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpickod_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpickod_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpickod_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpickod_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vilvh_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vilvh_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vilvh_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vilvh_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vilvh_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vilvh_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vilvh_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vilvh_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vilvl_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vilvl_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vilvl_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vilvl_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vilvl_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vilvl_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vilvl_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vilvl_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpackev_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpackev_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpackev_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpackev_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpackev_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpackev_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpackev_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpackev_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpackod_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpackod_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpackod_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpackod_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpackod_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpackod_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpackod_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vpackod_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vshuf_h(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vshuf_h((v8i16)_1, (v8i16)_2, (v8i16)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vshuf_w(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vshuf_w((v4i32)_1, (v4i32)_2, (v4i32)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vshuf_d(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vshuf_d((v2i64)_1, (v2i64)_2, (v2i64)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vand_v(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vand_v((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  UV16QI, UV16QI, UQI */
+#define __lsx_vandi_b(/*__m128i*/ _1, /*ui8*/ _2)                              \
+  ((__m128i)__builtin_lsx_vandi_b((v16u8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vor_v(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vor_v((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  UV16QI, UV16QI, UQI */
+#define __lsx_vori_b(/*__m128i*/ _1, /*ui8*/ _2)                               \
+  ((__m128i)__builtin_lsx_vori_b((v16u8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vnor_v(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vnor_v((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  UV16QI, UV16QI, UQI */
+#define __lsx_vnori_b(/*__m128i*/ _1, /*ui8*/ _2)                              \
+  ((__m128i)__builtin_lsx_vnori_b((v16u8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vxor_v(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vxor_v((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  UV16QI, UV16QI, UQI */
+#define __lsx_vxori_b(/*__m128i*/ _1, /*ui8*/ _2)                              \
+  ((__m128i)__builtin_lsx_vxori_b((v16u8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk, va */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vbitsel_v(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vbitsel_v((v16u8)_1, (v16u8)_2, (v16u8)_3);
+}
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI, UQI */
+#define __lsx_vbitseli_b(/*__m128i*/ _1, /*__m128i*/ _2, /*ui8*/ _3)           \
+  ((__m128i)__builtin_lsx_vbitseli_b((v16u8)(_1), (v16u8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  V16QI, V16QI, UQI */
+#define __lsx_vshuf4i_b(/*__m128i*/ _1, /*ui8*/ _2)                            \
+  ((__m128i)__builtin_lsx_vshuf4i_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  V8HI, V8HI, UQI */
+#define __lsx_vshuf4i_h(/*__m128i*/ _1, /*ui8*/ _2)                            \
+  ((__m128i)__builtin_lsx_vshuf4i_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  V4SI, V4SI, UQI */
+#define __lsx_vshuf4i_w(/*__m128i*/ _1, /*ui8*/ _2)                            \
+  ((__m128i)__builtin_lsx_vshuf4i_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, rj */
+/* Data types in instruction templates:  V16QI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vreplgr2vr_b(int _1) {
+  return (__m128i)__builtin_lsx_vreplgr2vr_b((int)_1);
+}
+
+/* Assembly instruction format:          vd, rj */
+/* Data types in instruction templates:  V8HI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vreplgr2vr_h(int _1) {
+  return (__m128i)__builtin_lsx_vreplgr2vr_h((int)_1);
+}
+
+/* Assembly instruction format:          vd, rj */
+/* Data types in instruction templates:  V4SI, SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vreplgr2vr_w(int _1) {
+  return (__m128i)__builtin_lsx_vreplgr2vr_w((int)_1);
+}
+
+/* Assembly instruction format:          vd, rj */
+/* Data types in instruction templates:  V2DI, DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vreplgr2vr_d(long int _1) {
+  return (__m128i)__builtin_lsx_vreplgr2vr_d((long int)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpcnt_b(__m128i _1) {
+  return (__m128i)__builtin_lsx_vpcnt_b((v16i8)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpcnt_h(__m128i _1) {
+  return (__m128i)__builtin_lsx_vpcnt_h((v8i16)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpcnt_w(__m128i _1) {
+  return (__m128i)__builtin_lsx_vpcnt_w((v4i32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vpcnt_d(__m128i _1) {
+  return (__m128i)__builtin_lsx_vpcnt_d((v2i64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vclo_b(__m128i _1) {
+  return (__m128i)__builtin_lsx_vclo_b((v16i8)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vclo_h(__m128i _1) {
+  return (__m128i)__builtin_lsx_vclo_h((v8i16)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vclo_w(__m128i _1) {
+  return (__m128i)__builtin_lsx_vclo_w((v4i32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vclo_d(__m128i _1) {
+  return (__m128i)__builtin_lsx_vclo_d((v2i64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vclz_b(__m128i _1) {
+  return (__m128i)__builtin_lsx_vclz_b((v16i8)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vclz_h(__m128i _1) {
+  return (__m128i)__builtin_lsx_vclz_h((v8i16)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vclz_w(__m128i _1) {
+  return (__m128i)__builtin_lsx_vclz_w((v4i32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vclz_d(__m128i _1) {
+  return (__m128i)__builtin_lsx_vclz_d((v2i64)_1);
+}
+
+/* Assembly instruction format:          rd, vj, ui4 */
+/* Data types in instruction templates:  SI, V16QI, UQI */
+#define __lsx_vpickve2gr_b(/*__m128i*/ _1, /*ui4*/ _2)                         \
+  ((int)__builtin_lsx_vpickve2gr_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          rd, vj, ui3 */
+/* Data types in instruction templates:  SI, V8HI, UQI */
+#define __lsx_vpickve2gr_h(/*__m128i*/ _1, /*ui3*/ _2)                         \
+  ((int)__builtin_lsx_vpickve2gr_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          rd, vj, ui2 */
+/* Data types in instruction templates:  SI, V4SI, UQI */
+#define __lsx_vpickve2gr_w(/*__m128i*/ _1, /*ui2*/ _2)                         \
+  ((int)__builtin_lsx_vpickve2gr_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          rd, vj, ui1 */
+/* Data types in instruction templates:  DI, V2DI, UQI */
+#define __lsx_vpickve2gr_d(/*__m128i*/ _1, /*ui1*/ _2)                         \
+  ((long int)__builtin_lsx_vpickve2gr_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          rd, vj, ui4 */
+/* Data types in instruction templates:  USI, V16QI, UQI */
+#define __lsx_vpickve2gr_bu(/*__m128i*/ _1, /*ui4*/ _2)                        \
+  ((unsigned int)__builtin_lsx_vpickve2gr_bu((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          rd, vj, ui3 */
+/* Data types in instruction templates:  USI, V8HI, UQI */
+#define __lsx_vpickve2gr_hu(/*__m128i*/ _1, /*ui3*/ _2)                        \
+  ((unsigned int)__builtin_lsx_vpickve2gr_hu((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          rd, vj, ui2 */
+/* Data types in instruction templates:  USI, V4SI, UQI */
+#define __lsx_vpickve2gr_wu(/*__m128i*/ _1, /*ui2*/ _2)                        \
+  ((unsigned int)__builtin_lsx_vpickve2gr_wu((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          rd, vj, ui1 */
+/* Data types in instruction templates:  UDI, V2DI, UQI */
+#define __lsx_vpickve2gr_du(/*__m128i*/ _1, /*ui1*/ _2)                        \
+  ((unsigned long int)__builtin_lsx_vpickve2gr_du((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, rj, ui4 */
+/* Data types in instruction templates:  V16QI, V16QI, SI, UQI */
+#define __lsx_vinsgr2vr_b(/*__m128i*/ _1, /*int*/ _2, /*ui4*/ _3)              \
+  ((__m128i)__builtin_lsx_vinsgr2vr_b((v16i8)(_1), (int)(_2), (_3)))
+
+/* Assembly instruction format:          vd, rj, ui3 */
+/* Data types in instruction templates:  V8HI, V8HI, SI, UQI */
+#define __lsx_vinsgr2vr_h(/*__m128i*/ _1, /*int*/ _2, /*ui3*/ _3)              \
+  ((__m128i)__builtin_lsx_vinsgr2vr_h((v8i16)(_1), (int)(_2), (_3)))
+
+/* Assembly instruction format:          vd, rj, ui2 */
+/* Data types in instruction templates:  V4SI, V4SI, SI, UQI */
+#define __lsx_vinsgr2vr_w(/*__m128i*/ _1, /*int*/ _2, /*ui2*/ _3)              \
+  ((__m128i)__builtin_lsx_vinsgr2vr_w((v4i32)(_1), (int)(_2), (_3)))
+
+/* Assembly instruction format:          vd, rj, ui1 */
+/* Data types in instruction templates:  V2DI, V2DI, SI, UQI */
+#define __lsx_vinsgr2vr_d(/*__m128i*/ _1, /*long int*/ _2, /*ui1*/ _3)         \
+  ((__m128i)__builtin_lsx_vinsgr2vr_d((v2i64)(_1), (long int)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_caf_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_caf_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_caf_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_caf_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cor_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cor_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cor_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cor_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cun_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cun_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cun_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cun_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cune_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cune_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cune_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cune_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cueq_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cueq_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cueq_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cueq_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_ceq_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_ceq_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_ceq_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_ceq_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cne_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cne_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cne_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cne_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_clt_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_clt_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_clt_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_clt_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cult_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cult_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cult_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cult_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cle_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cle_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cle_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cle_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cule_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cule_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_cule_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_cule_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_saf_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_saf_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_saf_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_saf_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sor_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sor_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sor_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sor_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sun_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sun_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sun_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sun_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sune_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sune_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sune_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sune_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sueq_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sueq_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sueq_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sueq_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_seq_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_seq_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_seq_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_seq_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sne_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sne_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sne_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sne_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_slt_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_slt_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_slt_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_slt_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sult_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sult_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sult_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sult_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sle_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sle_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sle_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sle_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sule_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sule_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcmp_sule_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vfcmp_sule_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SF, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfadd_s(__m128 _1, __m128 _2) {
+  return (__m128)__builtin_lsx_vfadd_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DF, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfadd_d(__m128d _1, __m128d _2) {
+  return (__m128d)__builtin_lsx_vfadd_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SF, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfsub_s(__m128 _1, __m128 _2) {
+  return (__m128)__builtin_lsx_vfsub_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DF, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfsub_d(__m128d _1, __m128d _2) {
+  return (__m128d)__builtin_lsx_vfsub_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SF, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfmul_s(__m128 _1, __m128 _2) {
+  return (__m128)__builtin_lsx_vfmul_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DF, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfmul_d(__m128d _1, __m128d _2) {
+  return (__m128d)__builtin_lsx_vfmul_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SF, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfdiv_s(__m128 _1, __m128 _2) {
+  return (__m128)__builtin_lsx_vfdiv_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DF, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfdiv_d(__m128d _1, __m128d _2) {
+  return (__m128d)__builtin_lsx_vfdiv_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfcvt_h_s(__m128 _1, __m128 _2) {
+  return (__m128i)__builtin_lsx_vfcvt_h_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SF, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfcvt_s_d(__m128d _1, __m128d _2) {
+  return (__m128)__builtin_lsx_vfcvt_s_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SF, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfmin_s(__m128 _1, __m128 _2) {
+  return (__m128)__builtin_lsx_vfmin_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DF, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfmin_d(__m128d _1, __m128d _2) {
+  return (__m128d)__builtin_lsx_vfmin_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SF, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfmina_s(__m128 _1, __m128 _2) {
+  return (__m128)__builtin_lsx_vfmina_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DF, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfmina_d(__m128d _1, __m128d _2) {
+  return (__m128d)__builtin_lsx_vfmina_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SF, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfmax_s(__m128 _1, __m128 _2) {
+  return (__m128)__builtin_lsx_vfmax_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DF, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfmax_d(__m128d _1, __m128d _2) {
+  return (__m128d)__builtin_lsx_vfmax_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SF, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfmaxa_s(__m128 _1, __m128 _2) {
+  return (__m128)__builtin_lsx_vfmaxa_s((v4f32)_1, (v4f32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DF, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfmaxa_d(__m128d _1, __m128d _2) {
+  return (__m128d)__builtin_lsx_vfmaxa_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfclass_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vfclass_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfclass_d(__m128d _1) {
+  return (__m128i)__builtin_lsx_vfclass_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfsqrt_s(__m128 _1) {
+  return (__m128)__builtin_lsx_vfsqrt_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfsqrt_d(__m128d _1) {
+  return (__m128d)__builtin_lsx_vfsqrt_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfrecip_s(__m128 _1) {
+  return (__m128)__builtin_lsx_vfrecip_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfrecip_d(__m128d _1) {
+  return (__m128d)__builtin_lsx_vfrecip_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfrint_s(__m128 _1) {
+  return (__m128)__builtin_lsx_vfrint_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfrint_d(__m128d _1) {
+  return (__m128d)__builtin_lsx_vfrint_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfrsqrt_s(__m128 _1) {
+  return (__m128)__builtin_lsx_vfrsqrt_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfrsqrt_d(__m128d _1) {
+  return (__m128d)__builtin_lsx_vfrsqrt_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vflogb_s(__m128 _1) {
+  return (__m128)__builtin_lsx_vflogb_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vflogb_d(__m128d _1) {
+  return (__m128d)__builtin_lsx_vflogb_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SF, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfcvth_s_h(__m128i _1) {
+  return (__m128)__builtin_lsx_vfcvth_s_h((v8i16)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfcvth_d_s(__m128 _1) {
+  return (__m128d)__builtin_lsx_vfcvth_d_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SF, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfcvtl_s_h(__m128i _1) {
+  return (__m128)__builtin_lsx_vfcvtl_s_h((v8i16)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfcvtl_d_s(__m128 _1) {
+  return (__m128d)__builtin_lsx_vfcvtl_d_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftint_w_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftint_w_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftint_l_d(__m128d _1) {
+  return (__m128i)__builtin_lsx_vftint_l_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  UV4SI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftint_wu_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftint_wu_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  UV2DI, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftint_lu_d(__m128d _1) {
+  return (__m128i)__builtin_lsx_vftint_lu_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrz_w_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintrz_w_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrz_l_d(__m128d _1) {
+  return (__m128i)__builtin_lsx_vftintrz_l_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  UV4SI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrz_wu_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintrz_wu_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  UV2DI, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrz_lu_d(__m128d _1) {
+  return (__m128i)__builtin_lsx_vftintrz_lu_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SF, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vffint_s_w(__m128i _1) {
+  return (__m128)__builtin_lsx_vffint_s_w((v4i32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DF, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vffint_d_l(__m128i _1) {
+  return (__m128d)__builtin_lsx_vffint_d_l((v2i64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SF, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vffint_s_wu(__m128i _1) {
+  return (__m128)__builtin_lsx_vffint_s_wu((v4u32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DF, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vffint_d_lu(__m128i _1) {
+  return (__m128d)__builtin_lsx_vffint_d_lu((v2u64)_1);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vandn_v(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vandn_v((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vneg_b(__m128i _1) {
+  return (__m128i)__builtin_lsx_vneg_b((v16i8)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vneg_h(__m128i _1) {
+  return (__m128i)__builtin_lsx_vneg_h((v8i16)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vneg_w(__m128i _1) {
+  return (__m128i)__builtin_lsx_vneg_w((v4i32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vneg_d(__m128i _1) {
+  return (__m128i)__builtin_lsx_vneg_d((v2i64)_1);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmuh_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmuh_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmuh_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmuh_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmuh_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmuh_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmuh_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmuh_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmuh_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmuh_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmuh_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmuh_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmuh_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmuh_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmuh_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmuh_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  V8HI, V16QI, UQI */
+#define __lsx_vsllwil_h_b(/*__m128i*/ _1, /*ui3*/ _2)                          \
+  ((__m128i)__builtin_lsx_vsllwil_h_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V4SI, V8HI, UQI */
+#define __lsx_vsllwil_w_h(/*__m128i*/ _1, /*ui4*/ _2)                          \
+  ((__m128i)__builtin_lsx_vsllwil_w_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V2DI, V4SI, UQI */
+#define __lsx_vsllwil_d_w(/*__m128i*/ _1, /*ui5*/ _2)                          \
+  ((__m128i)__builtin_lsx_vsllwil_d_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  UV8HI, UV16QI, UQI */
+#define __lsx_vsllwil_hu_bu(/*__m128i*/ _1, /*ui3*/ _2)                        \
+  ((__m128i)__builtin_lsx_vsllwil_hu_bu((v16u8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  UV4SI, UV8HI, UQI */
+#define __lsx_vsllwil_wu_hu(/*__m128i*/ _1, /*ui4*/ _2)                        \
+  ((__m128i)__builtin_lsx_vsllwil_wu_hu((v8u16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV2DI, UV4SI, UQI */
+#define __lsx_vsllwil_du_wu(/*__m128i*/ _1, /*ui5*/ _2)                        \
+  ((__m128i)__builtin_lsx_vsllwil_du_wu((v4u32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsran_b_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsran_b_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsran_h_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsran_h_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsran_w_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsran_w_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssran_b_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssran_b_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssran_h_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssran_h_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssran_w_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssran_w_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssran_bu_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssran_bu_h((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssran_hu_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssran_hu_w((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssran_wu_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssran_wu_d((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrarn_b_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrarn_b_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrarn_h_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrarn_h_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrarn_w_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrarn_w_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrarn_b_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrarn_b_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrarn_h_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrarn_h_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrarn_w_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrarn_w_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrarn_bu_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrarn_bu_h((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrarn_hu_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrarn_hu_w((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrarn_wu_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrarn_wu_d((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrln_b_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrln_b_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrln_h_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrln_h_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrln_w_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrln_w_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrln_bu_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrln_bu_h((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrln_hu_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrln_hu_w((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrln_wu_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrln_wu_d((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrlrn_b_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrlrn_b_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrlrn_h_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrlrn_h_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsrlrn_w_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsrlrn_w_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV16QI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrlrn_bu_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrlrn_bu_h((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrlrn_hu_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrlrn_hu_w((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrlrn_wu_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrlrn_wu_d((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI, UQI */
+#define __lsx_vfrstpi_b(/*__m128i*/ _1, /*__m128i*/ _2, /*ui5*/ _3)            \
+  ((__m128i)__builtin_lsx_vfrstpi_b((v16i8)(_1), (v16i8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI, UQI */
+#define __lsx_vfrstpi_h(/*__m128i*/ _1, /*__m128i*/ _2, /*ui5*/ _3)            \
+  ((__m128i)__builtin_lsx_vfrstpi_h((v8i16)(_1), (v8i16)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfrstp_b(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vfrstp_b((v16i8)_1, (v16i8)_2, (v16i8)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfrstp_h(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vfrstp_h((v8i16)_1, (v8i16)_2, (v8i16)_3);
+}
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, USI */
+#define __lsx_vshuf4i_d(/*__m128i*/ _1, /*__m128i*/ _2, /*ui8*/ _3)            \
+  ((__m128i)__builtin_lsx_vshuf4i_d((v2i64)(_1), (v2i64)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V16QI, V16QI, UQI */
+#define __lsx_vbsrl_v(/*__m128i*/ _1, /*ui5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vbsrl_v((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V16QI, V16QI, UQI */
+#define __lsx_vbsll_v(/*__m128i*/ _1, /*ui5*/ _2)                              \
+  ((__m128i)__builtin_lsx_vbsll_v((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI, UQI */
+#define __lsx_vextrins_b(/*__m128i*/ _1, /*__m128i*/ _2, /*ui8*/ _3)           \
+  ((__m128i)__builtin_lsx_vextrins_b((v16i8)(_1), (v16i8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI, UQI */
+#define __lsx_vextrins_h(/*__m128i*/ _1, /*__m128i*/ _2, /*ui8*/ _3)           \
+  ((__m128i)__builtin_lsx_vextrins_h((v8i16)(_1), (v8i16)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI, UQI */
+#define __lsx_vextrins_w(/*__m128i*/ _1, /*__m128i*/ _2, /*ui8*/ _3)           \
+  ((__m128i)__builtin_lsx_vextrins_w((v4i32)(_1), (v4i32)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, UQI */
+#define __lsx_vextrins_d(/*__m128i*/ _1, /*__m128i*/ _2, /*ui8*/ _3)           \
+  ((__m128i)__builtin_lsx_vextrins_d((v2i64)(_1), (v2i64)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmskltz_b(__m128i _1) {
+  return (__m128i)__builtin_lsx_vmskltz_b((v16i8)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmskltz_h(__m128i _1) {
+  return (__m128i)__builtin_lsx_vmskltz_h((v8i16)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmskltz_w(__m128i _1) {
+  return (__m128i)__builtin_lsx_vmskltz_w((v4i32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmskltz_d(__m128i _1) {
+  return (__m128i)__builtin_lsx_vmskltz_d((v2i64)_1);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsigncov_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsigncov_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsigncov_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsigncov_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsigncov_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsigncov_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsigncov_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsigncov_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk, va */
+/* Data types in instruction templates:  V4SF, V4SF, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfmadd_s(__m128 _1, __m128 _2, __m128 _3) {
+  return (__m128)__builtin_lsx_vfmadd_s((v4f32)_1, (v4f32)_2, (v4f32)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk, va */
+/* Data types in instruction templates:  V2DF, V2DF, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfmadd_d(__m128d _1, __m128d _2, __m128d _3) {
+  return (__m128d)__builtin_lsx_vfmadd_d((v2f64)_1, (v2f64)_2, (v2f64)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk, va */
+/* Data types in instruction templates:  V4SF, V4SF, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfmsub_s(__m128 _1, __m128 _2, __m128 _3) {
+  return (__m128)__builtin_lsx_vfmsub_s((v4f32)_1, (v4f32)_2, (v4f32)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk, va */
+/* Data types in instruction templates:  V2DF, V2DF, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfmsub_d(__m128d _1, __m128d _2, __m128d _3) {
+  return (__m128d)__builtin_lsx_vfmsub_d((v2f64)_1, (v2f64)_2, (v2f64)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk, va */
+/* Data types in instruction templates:  V4SF, V4SF, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfnmadd_s(__m128 _1, __m128 _2, __m128 _3) {
+  return (__m128)__builtin_lsx_vfnmadd_s((v4f32)_1, (v4f32)_2, (v4f32)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk, va */
+/* Data types in instruction templates:  V2DF, V2DF, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfnmadd_d(__m128d _1, __m128d _2, __m128d _3) {
+  return (__m128d)__builtin_lsx_vfnmadd_d((v2f64)_1, (v2f64)_2, (v2f64)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk, va */
+/* Data types in instruction templates:  V4SF, V4SF, V4SF, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vfnmsub_s(__m128 _1, __m128 _2, __m128 _3) {
+  return (__m128)__builtin_lsx_vfnmsub_s((v4f32)_1, (v4f32)_2, (v4f32)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk, va */
+/* Data types in instruction templates:  V2DF, V2DF, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vfnmsub_d(__m128d _1, __m128d _2, __m128d _3) {
+  return (__m128d)__builtin_lsx_vfnmsub_d((v2f64)_1, (v2f64)_2, (v2f64)_3);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrne_w_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintrne_w_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrne_l_d(__m128d _1) {
+  return (__m128i)__builtin_lsx_vftintrne_l_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrp_w_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintrp_w_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrp_l_d(__m128d _1) {
+  return (__m128i)__builtin_lsx_vftintrp_l_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrm_w_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintrm_w_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrm_l_d(__m128d _1) {
+  return (__m128i)__builtin_lsx_vftintrm_l_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftint_w_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vftint_w_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SF, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128
+    __lsx_vffint_s_l(__m128i _1, __m128i _2) {
+  return (__m128)__builtin_lsx_vffint_s_l((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrz_w_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vftintrz_w_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrp_w_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vftintrp_w_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrm_w_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vftintrm_w_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V2DF, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrne_w_d(__m128d _1, __m128d _2) {
+  return (__m128i)__builtin_lsx_vftintrne_w_d((v2f64)_1, (v2f64)_2);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintl_l_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintl_l_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftinth_l_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftinth_l_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DF, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vffinth_d_w(__m128i _1) {
+  return (__m128d)__builtin_lsx_vffinth_d_w((v4i32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DF, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128d
+    __lsx_vffintl_d_w(__m128i _1) {
+  return (__m128d)__builtin_lsx_vffintl_d_w((v4i32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrzl_l_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintrzl_l_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrzh_l_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintrzh_l_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrpl_l_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintrpl_l_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrph_l_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintrph_l_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrml_l_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintrml_l_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrmh_l_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintrmh_l_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrnel_l_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintrnel_l_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vftintrneh_l_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vftintrneh_l_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfrintrne_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vfrintrne_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfrintrne_d(__m128d _1) {
+  return (__m128i)__builtin_lsx_vfrintrne_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfrintrz_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vfrintrz_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfrintrz_d(__m128d _1) {
+  return (__m128i)__builtin_lsx_vfrintrz_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfrintrp_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vfrintrp_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfrintrp_d(__m128d _1) {
+  return (__m128i)__builtin_lsx_vfrintrp_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V4SF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfrintrm_s(__m128 _1) {
+  return (__m128i)__builtin_lsx_vfrintrm_s((v4f32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DF */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vfrintrm_d(__m128d _1) {
+  return (__m128i)__builtin_lsx_vfrintrm_d((v2f64)_1);
+}
+
+/* Assembly instruction format:          vd, rj, si8, idx */
+/* Data types in instruction templates:  VOID, V16QI, CVPOINTER, SI, UQI */
+#define __lsx_vstelm_b(/*__m128i*/ _1, /*void **/ _2, /*si8*/ _3, /*idx*/ _4)  \
+  ((void)__builtin_lsx_vstelm_b((v16i8)(_1), (void *)(_2), (_3), (_4)))
+
+/* Assembly instruction format:          vd, rj, si8, idx */
+/* Data types in instruction templates:  VOID, V8HI, CVPOINTER, SI, UQI */
+#define __lsx_vstelm_h(/*__m128i*/ _1, /*void **/ _2, /*si8*/ _3, /*idx*/ _4)  \
+  ((void)__builtin_lsx_vstelm_h((v8i16)(_1), (void *)(_2), (_3), (_4)))
+
+/* Assembly instruction format:          vd, rj, si8, idx */
+/* Data types in instruction templates:  VOID, V4SI, CVPOINTER, SI, UQI */
+#define __lsx_vstelm_w(/*__m128i*/ _1, /*void **/ _2, /*si8*/ _3, /*idx*/ _4)  \
+  ((void)__builtin_lsx_vstelm_w((v4i32)(_1), (void *)(_2), (_3), (_4)))
+
+/* Assembly instruction format:          vd, rj, si8, idx */
+/* Data types in instruction templates:  VOID, V2DI, CVPOINTER, SI, UQI */
+#define __lsx_vstelm_d(/*__m128i*/ _1, /*void **/ _2, /*si8*/ _3, /*idx*/ _4)  \
+  ((void)__builtin_lsx_vstelm_d((v2i64)(_1), (void *)(_2), (_3), (_4)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwev_d_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwev_d_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwev_w_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwev_w_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwev_h_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwev_h_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwod_d_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwod_d_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwod_w_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwod_w_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwod_h_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwod_h_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwev_d_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwev_d_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwev_w_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwev_w_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwev_h_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwev_h_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwod_d_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwod_d_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwod_w_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwod_w_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwod_h_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwod_h_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwev_d_wu_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwev_d_wu_w((v4u32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, UV8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwev_w_hu_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwev_w_hu_h((v8u16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, UV16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwev_h_bu_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwev_h_bu_b((v16u8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwod_d_wu_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwod_d_wu_w((v4u32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, UV8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwod_w_hu_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwod_w_hu_h((v8u16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, UV16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwod_h_bu_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwod_h_bu_b((v16u8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwev_d_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwev_d_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwev_w_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwev_w_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwev_h_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwev_h_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwod_d_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwod_d_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwod_w_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwod_w_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwod_h_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwod_h_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwev_d_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwev_d_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwev_w_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwev_w_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwev_h_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwev_h_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwod_d_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwod_d_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwod_w_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwod_w_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwod_h_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwod_h_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwev_q_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwev_q_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwod_q_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwod_q_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwev_q_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwev_q_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwod_q_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwod_q_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwev_q_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwev_q_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwod_q_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwod_q_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwev_q_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwev_q_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsubwod_q_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsubwod_q_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwev_q_du_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwev_q_du_d((v2u64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vaddwod_q_du_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vaddwod_q_du_d((v2u64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwev_d_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwev_d_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwev_w_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwev_w_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwev_h_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwev_h_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwod_d_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwod_d_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwod_w_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwod_w_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwod_h_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwod_h_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwev_d_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwev_d_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwev_w_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwev_w_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwev_h_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwev_h_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwod_d_wu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwod_d_wu((v4u32)_1, (v4u32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwod_w_hu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwod_w_hu((v8u16)_1, (v8u16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwod_h_bu(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwod_h_bu((v16u8)_1, (v16u8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwev_d_wu_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwev_d_wu_w((v4u32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, UV8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwev_w_hu_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwev_w_hu_h((v8u16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, UV16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwev_h_bu_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwev_h_bu_b((v16u8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwod_d_wu_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwod_d_wu_w((v4u32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, UV8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwod_w_hu_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwod_w_hu_h((v8u16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, UV16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwod_h_bu_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwod_h_bu_b((v16u8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwev_q_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwev_q_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwod_q_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwod_q_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwev_q_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwev_q_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwod_q_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwod_q_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwev_q_du_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwev_q_du_d((v2u64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, UV2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmulwod_q_du_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vmulwod_q_du_d((v2u64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhaddw_q_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhaddw_q_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhaddw_qu_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhaddw_qu_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhsubw_q_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhsubw_q_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vhsubw_qu_du(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vhsubw_qu_du((v2u64)_1, (v2u64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwev_d_w(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwev_d_w((v2i64)_1, (v4i32)_2, (v4i32)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwev_w_h(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwev_w_h((v4i32)_1, (v8i16)_2, (v8i16)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwev_h_b(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwev_h_b((v8i16)_1, (v16i8)_2, (v16i8)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwev_d_wu(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwev_d_wu((v2u64)_1, (v4u32)_2, (v4u32)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwev_w_hu(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwev_w_hu((v4u32)_1, (v8u16)_2, (v8u16)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwev_h_bu(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwev_h_bu((v8u16)_1, (v16u8)_2, (v16u8)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwod_d_w(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwod_d_w((v2i64)_1, (v4i32)_2, (v4i32)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwod_w_h(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwod_w_h((v4i32)_1, (v8i16)_2, (v8i16)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwod_h_b(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwod_h_b((v8i16)_1, (v16i8)_2, (v16i8)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV4SI, UV4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwod_d_wu(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwod_d_wu((v2u64)_1, (v4u32)_2, (v4u32)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV4SI, UV4SI, UV8HI, UV8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwod_w_hu(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwod_w_hu((v4u32)_1, (v8u16)_2, (v8u16)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV8HI, UV8HI, UV16QI, UV16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwod_h_bu(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwod_h_bu((v8u16)_1, (v16u8)_2, (v16u8)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, UV4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwev_d_wu_w(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwev_d_wu_w((v2i64)_1, (v4u32)_2,
+                                                (v4i32)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, UV8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwev_w_hu_h(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwev_w_hu_h((v4i32)_1, (v8u16)_2,
+                                                (v8i16)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, UV16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwev_h_bu_b(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwev_h_bu_b((v8i16)_1, (v16u8)_2,
+                                                (v16i8)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, UV4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwod_d_wu_w(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwod_d_wu_w((v2i64)_1, (v4u32)_2,
+                                                (v4i32)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, UV8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwod_w_hu_h(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwod_w_hu_h((v4i32)_1, (v8u16)_2,
+                                                (v8i16)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, UV16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwod_h_bu_b(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwod_h_bu_b((v8i16)_1, (v16u8)_2,
+                                                (v16i8)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwev_q_d(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwev_q_d((v2i64)_1, (v2i64)_2, (v2i64)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwod_q_d(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwod_q_d((v2i64)_1, (v2i64)_2, (v2i64)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwev_q_du(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwev_q_du((v2u64)_1, (v2u64)_2, (v2u64)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  UV2DI, UV2DI, UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwod_q_du(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwod_q_du((v2u64)_1, (v2u64)_2, (v2u64)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, UV2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwev_q_du_d(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwev_q_du_d((v2i64)_1, (v2u64)_2,
+                                                (v2i64)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, UV2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmaddwod_q_du_d(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vmaddwod_q_du_d((v2i64)_1, (v2u64)_2,
+                                                (v2i64)_3);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vrotr_b(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vrotr_b((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vrotr_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vrotr_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vrotr_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vrotr_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vrotr_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vrotr_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vadd_q(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vadd_q((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vsub_q(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vsub_q((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, rj, si12 */
+/* Data types in instruction templates:  V16QI, CVPOINTER, SI */
+#define __lsx_vldrepl_b(/*void **/ _1, /*si12*/ _2)                            \
+  ((__m128i)__builtin_lsx_vldrepl_b((void *)(_1), (_2)))
+
+/* Assembly instruction format:          vd, rj, si11 */
+/* Data types in instruction templates:  V8HI, CVPOINTER, SI */
+#define __lsx_vldrepl_h(/*void **/ _1, /*si11*/ _2)                            \
+  ((__m128i)__builtin_lsx_vldrepl_h((void *)(_1), (_2)))
+
+/* Assembly instruction format:          vd, rj, si10 */
+/* Data types in instruction templates:  V4SI, CVPOINTER, SI */
+#define __lsx_vldrepl_w(/*void **/ _1, /*si10*/ _2)                            \
+  ((__m128i)__builtin_lsx_vldrepl_w((void *)(_1), (_2)))
+
+/* Assembly instruction format:          vd, rj, si9 */
+/* Data types in instruction templates:  V2DI, CVPOINTER, SI */
+#define __lsx_vldrepl_d(/*void **/ _1, /*si9*/ _2)                             \
+  ((__m128i)__builtin_lsx_vldrepl_d((void *)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmskgez_b(__m128i _1) {
+  return (__m128i)__builtin_lsx_vmskgez_b((v16i8)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vmsknz_b(__m128i _1) {
+  return (__m128i)__builtin_lsx_vmsknz_b((v16i8)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V8HI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vexth_h_b(__m128i _1) {
+  return (__m128i)__builtin_lsx_vexth_h_b((v16i8)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V4SI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vexth_w_h(__m128i _1) {
+  return (__m128i)__builtin_lsx_vexth_w_h((v8i16)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vexth_d_w(__m128i _1) {
+  return (__m128i)__builtin_lsx_vexth_d_w((v4i32)_1);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vexth_q_d(__m128i _1) {
+  return (__m128i)__builtin_lsx_vexth_q_d((v2i64)_1);
+}
+
+/* Assembly instruction format:          vd, vj.  */
+/* Data types in instruction templates:  UV8HI, UV16QI.  */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vexth_hu_bu(__m128i _1) {
+  return (__m128i)__builtin_lsx_vexth_hu_bu((v16u8)_1);
+}
+
+/* Assembly instruction format:          vd, vj.  */
+/* Data types in instruction templates:  UV4SI, UV8HI.  */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vexth_wu_hu(__m128i _1) {
+  return (__m128i)__builtin_lsx_vexth_wu_hu((v8u16)_1);
+}
+
+/* Assembly instruction format:          vd, vj.  */
+/* Data types in instruction templates:  UV2DI, UV4SI.  */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vexth_du_wu(__m128i _1) {
+  return (__m128i)__builtin_lsx_vexth_du_wu((v4u32)_1);
+}
+
+/* Assembly instruction format:          vd, vj.  */
+/* Data types in instruction templates:  UV2DI, UV2DI.  */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vexth_qu_du(__m128i _1) {
+  return (__m128i)__builtin_lsx_vexth_qu_du((v2u64)_1);
+}
+
+/* Assembly instruction format:          vd, vj, ui3 */
+/* Data types in instruction templates:  V16QI, V16QI, UQI */
+#define __lsx_vrotri_b(/*__m128i*/ _1, /*ui3*/ _2)                             \
+  ((__m128i)__builtin_lsx_vrotri_b((v16i8)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V8HI, V8HI, UQI */
+#define __lsx_vrotri_h(/*__m128i*/ _1, /*ui4*/ _2)                             \
+  ((__m128i)__builtin_lsx_vrotri_h((v8i16)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V4SI, V4SI, UQI */
+#define __lsx_vrotri_w(/*__m128i*/ _1, /*ui5*/ _2)                             \
+  ((__m128i)__builtin_lsx_vrotri_w((v4i32)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V2DI, V2DI, UQI */
+#define __lsx_vrotri_d(/*__m128i*/ _1, /*ui6*/ _2)                             \
+  ((__m128i)__builtin_lsx_vrotri_d((v2i64)(_1), (_2)))
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vextl_q_d(__m128i _1) {
+  return (__m128i)__builtin_lsx_vextl_q_d((v2i64)_1);
+}
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI, USI */
+#define __lsx_vsrlni_b_h(/*__m128i*/ _1, /*__m128i*/ _2, /*ui4*/ _3)           \
+  ((__m128i)__builtin_lsx_vsrlni_b_h((v16i8)(_1), (v16i8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI, USI */
+#define __lsx_vsrlni_h_w(/*__m128i*/ _1, /*__m128i*/ _2, /*ui5*/ _3)           \
+  ((__m128i)__builtin_lsx_vsrlni_h_w((v8i16)(_1), (v8i16)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI, USI */
+#define __lsx_vsrlni_w_d(/*__m128i*/ _1, /*__m128i*/ _2, /*ui6*/ _3)           \
+  ((__m128i)__builtin_lsx_vsrlni_w_d((v4i32)(_1), (v4i32)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui7 */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, USI */
+#define __lsx_vsrlni_d_q(/*__m128i*/ _1, /*__m128i*/ _2, /*ui7*/ _3)           \
+  ((__m128i)__builtin_lsx_vsrlni_d_q((v2i64)(_1), (v2i64)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI, USI */
+#define __lsx_vssrlni_b_h(/*__m128i*/ _1, /*__m128i*/ _2, /*ui4*/ _3)          \
+  ((__m128i)__builtin_lsx_vssrlni_b_h((v16i8)(_1), (v16i8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI, USI */
+#define __lsx_vssrlni_h_w(/*__m128i*/ _1, /*__m128i*/ _2, /*ui5*/ _3)          \
+  ((__m128i)__builtin_lsx_vssrlni_h_w((v8i16)(_1), (v8i16)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI, USI */
+#define __lsx_vssrlni_w_d(/*__m128i*/ _1, /*__m128i*/ _2, /*ui6*/ _3)          \
+  ((__m128i)__builtin_lsx_vssrlni_w_d((v4i32)(_1), (v4i32)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui7 */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, USI */
+#define __lsx_vssrlni_d_q(/*__m128i*/ _1, /*__m128i*/ _2, /*ui7*/ _3)          \
+  ((__m128i)__builtin_lsx_vssrlni_d_q((v2i64)(_1), (v2i64)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  UV16QI, UV16QI, V16QI, USI */
+#define __lsx_vssrlni_bu_h(/*__m128i*/ _1, /*__m128i*/ _2, /*ui4*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrlni_bu_h((v16u8)(_1), (v16i8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV8HI, UV8HI, V8HI, USI */
+#define __lsx_vssrlni_hu_w(/*__m128i*/ _1, /*__m128i*/ _2, /*ui5*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrlni_hu_w((v8u16)(_1), (v8i16)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  UV4SI, UV4SI, V4SI, USI */
+#define __lsx_vssrlni_wu_d(/*__m128i*/ _1, /*__m128i*/ _2, /*ui6*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrlni_wu_d((v4u32)(_1), (v4i32)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui7 */
+/* Data types in instruction templates:  UV2DI, UV2DI, V2DI, USI */
+#define __lsx_vssrlni_du_q(/*__m128i*/ _1, /*__m128i*/ _2, /*ui7*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrlni_du_q((v2u64)(_1), (v2i64)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI, USI */
+#define __lsx_vssrlrni_b_h(/*__m128i*/ _1, /*__m128i*/ _2, /*ui4*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrlrni_b_h((v16i8)(_1), (v16i8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI, USI */
+#define __lsx_vssrlrni_h_w(/*__m128i*/ _1, /*__m128i*/ _2, /*ui5*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrlrni_h_w((v8i16)(_1), (v8i16)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI, USI */
+#define __lsx_vssrlrni_w_d(/*__m128i*/ _1, /*__m128i*/ _2, /*ui6*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrlrni_w_d((v4i32)(_1), (v4i32)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui7 */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, USI */
+#define __lsx_vssrlrni_d_q(/*__m128i*/ _1, /*__m128i*/ _2, /*ui7*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrlrni_d_q((v2i64)(_1), (v2i64)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  UV16QI, UV16QI, V16QI, USI */
+#define __lsx_vssrlrni_bu_h(/*__m128i*/ _1, /*__m128i*/ _2, /*ui4*/ _3)        \
+  ((__m128i)__builtin_lsx_vssrlrni_bu_h((v16u8)(_1), (v16i8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV8HI, UV8HI, V8HI, USI */
+#define __lsx_vssrlrni_hu_w(/*__m128i*/ _1, /*__m128i*/ _2, /*ui5*/ _3)        \
+  ((__m128i)__builtin_lsx_vssrlrni_hu_w((v8u16)(_1), (v8i16)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  UV4SI, UV4SI, V4SI, USI */
+#define __lsx_vssrlrni_wu_d(/*__m128i*/ _1, /*__m128i*/ _2, /*ui6*/ _3)        \
+  ((__m128i)__builtin_lsx_vssrlrni_wu_d((v4u32)(_1), (v4i32)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui7 */
+/* Data types in instruction templates:  UV2DI, UV2DI, V2DI, USI */
+#define __lsx_vssrlrni_du_q(/*__m128i*/ _1, /*__m128i*/ _2, /*ui7*/ _3)        \
+  ((__m128i)__builtin_lsx_vssrlrni_du_q((v2u64)(_1), (v2i64)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI, USI */
+#define __lsx_vsrani_b_h(/*__m128i*/ _1, /*__m128i*/ _2, /*ui4*/ _3)           \
+  ((__m128i)__builtin_lsx_vsrani_b_h((v16i8)(_1), (v16i8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI, USI */
+#define __lsx_vsrani_h_w(/*__m128i*/ _1, /*__m128i*/ _2, /*ui5*/ _3)           \
+  ((__m128i)__builtin_lsx_vsrani_h_w((v8i16)(_1), (v8i16)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI, USI */
+#define __lsx_vsrani_w_d(/*__m128i*/ _1, /*__m128i*/ _2, /*ui6*/ _3)           \
+  ((__m128i)__builtin_lsx_vsrani_w_d((v4i32)(_1), (v4i32)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui7 */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, USI */
+#define __lsx_vsrani_d_q(/*__m128i*/ _1, /*__m128i*/ _2, /*ui7*/ _3)           \
+  ((__m128i)__builtin_lsx_vsrani_d_q((v2i64)(_1), (v2i64)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI, USI */
+#define __lsx_vsrarni_b_h(/*__m128i*/ _1, /*__m128i*/ _2, /*ui4*/ _3)          \
+  ((__m128i)__builtin_lsx_vsrarni_b_h((v16i8)(_1), (v16i8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI, USI */
+#define __lsx_vsrarni_h_w(/*__m128i*/ _1, /*__m128i*/ _2, /*ui5*/ _3)          \
+  ((__m128i)__builtin_lsx_vsrarni_h_w((v8i16)(_1), (v8i16)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI, USI */
+#define __lsx_vsrarni_w_d(/*__m128i*/ _1, /*__m128i*/ _2, /*ui6*/ _3)          \
+  ((__m128i)__builtin_lsx_vsrarni_w_d((v4i32)(_1), (v4i32)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui7 */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, USI */
+#define __lsx_vsrarni_d_q(/*__m128i*/ _1, /*__m128i*/ _2, /*ui7*/ _3)          \
+  ((__m128i)__builtin_lsx_vsrarni_d_q((v2i64)(_1), (v2i64)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI, USI */
+#define __lsx_vssrani_b_h(/*__m128i*/ _1, /*__m128i*/ _2, /*ui4*/ _3)          \
+  ((__m128i)__builtin_lsx_vssrani_b_h((v16i8)(_1), (v16i8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI, USI */
+#define __lsx_vssrani_h_w(/*__m128i*/ _1, /*__m128i*/ _2, /*ui5*/ _3)          \
+  ((__m128i)__builtin_lsx_vssrani_h_w((v8i16)(_1), (v8i16)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI, USI */
+#define __lsx_vssrani_w_d(/*__m128i*/ _1, /*__m128i*/ _2, /*ui6*/ _3)          \
+  ((__m128i)__builtin_lsx_vssrani_w_d((v4i32)(_1), (v4i32)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui7 */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, USI */
+#define __lsx_vssrani_d_q(/*__m128i*/ _1, /*__m128i*/ _2, /*ui7*/ _3)          \
+  ((__m128i)__builtin_lsx_vssrani_d_q((v2i64)(_1), (v2i64)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  UV16QI, UV16QI, V16QI, USI */
+#define __lsx_vssrani_bu_h(/*__m128i*/ _1, /*__m128i*/ _2, /*ui4*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrani_bu_h((v16u8)(_1), (v16i8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV8HI, UV8HI, V8HI, USI */
+#define __lsx_vssrani_hu_w(/*__m128i*/ _1, /*__m128i*/ _2, /*ui5*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrani_hu_w((v8u16)(_1), (v8i16)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  UV4SI, UV4SI, V4SI, USI */
+#define __lsx_vssrani_wu_d(/*__m128i*/ _1, /*__m128i*/ _2, /*ui6*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrani_wu_d((v4u32)(_1), (v4i32)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui7 */
+/* Data types in instruction templates:  UV2DI, UV2DI, V2DI, USI */
+#define __lsx_vssrani_du_q(/*__m128i*/ _1, /*__m128i*/ _2, /*ui7*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrani_du_q((v2u64)(_1), (v2i64)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI, USI */
+#define __lsx_vssrarni_b_h(/*__m128i*/ _1, /*__m128i*/ _2, /*ui4*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrarni_b_h((v16i8)(_1), (v16i8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  V8HI, V8HI, V8HI, USI */
+#define __lsx_vssrarni_h_w(/*__m128i*/ _1, /*__m128i*/ _2, /*ui5*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrarni_h_w((v8i16)(_1), (v8i16)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI, USI */
+#define __lsx_vssrarni_w_d(/*__m128i*/ _1, /*__m128i*/ _2, /*ui6*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrarni_w_d((v4i32)(_1), (v4i32)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui7 */
+/* Data types in instruction templates:  V2DI, V2DI, V2DI, USI */
+#define __lsx_vssrarni_d_q(/*__m128i*/ _1, /*__m128i*/ _2, /*ui7*/ _3)         \
+  ((__m128i)__builtin_lsx_vssrarni_d_q((v2i64)(_1), (v2i64)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui4 */
+/* Data types in instruction templates:  UV16QI, UV16QI, V16QI, USI */
+#define __lsx_vssrarni_bu_h(/*__m128i*/ _1, /*__m128i*/ _2, /*ui4*/ _3)        \
+  ((__m128i)__builtin_lsx_vssrarni_bu_h((v16u8)(_1), (v16i8)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui5 */
+/* Data types in instruction templates:  UV8HI, UV8HI, V8HI, USI */
+#define __lsx_vssrarni_hu_w(/*__m128i*/ _1, /*__m128i*/ _2, /*ui5*/ _3)        \
+  ((__m128i)__builtin_lsx_vssrarni_hu_w((v8u16)(_1), (v8i16)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui6 */
+/* Data types in instruction templates:  UV4SI, UV4SI, V4SI, USI */
+#define __lsx_vssrarni_wu_d(/*__m128i*/ _1, /*__m128i*/ _2, /*ui6*/ _3)        \
+  ((__m128i)__builtin_lsx_vssrarni_wu_d((v4u32)(_1), (v4i32)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui7 */
+/* Data types in instruction templates:  UV2DI, UV2DI, V2DI, USI */
+#define __lsx_vssrarni_du_q(/*__m128i*/ _1, /*__m128i*/ _2, /*ui7*/ _3)        \
+  ((__m128i)__builtin_lsx_vssrarni_du_q((v2u64)(_1), (v2i64)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, ui8 */
+/* Data types in instruction templates:  V4SI, V4SI, V4SI, USI */
+#define __lsx_vpermi_w(/*__m128i*/ _1, /*__m128i*/ _2, /*ui8*/ _3)             \
+  ((__m128i)__builtin_lsx_vpermi_w((v4i32)(_1), (v4i32)(_2), (_3)))
+
+/* Assembly instruction format:          vd, rj, si12 */
+/* Data types in instruction templates:  V16QI, CVPOINTER, SI */
+#define __lsx_vld(/*void **/ _1, /*si12*/ _2)                                  \
+  ((__m128i)__builtin_lsx_vld((void *)(_1), (_2)))
+
+/* Assembly instruction format:          vd, rj, si12 */
+/* Data types in instruction templates:  VOID, V16QI, CVPOINTER, SI */
+#define __lsx_vst(/*__m128i*/ _1, /*void **/ _2, /*si12*/ _3)                  \
+  ((void)__builtin_lsx_vst((v16i8)(_1), (void *)(_2), (_3)))
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrlrn_b_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrlrn_b_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrlrn_h_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrlrn_h_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrlrn_w_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrlrn_w_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V8HI, V8HI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrln_b_h(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrln_b_h((v8i16)_1, (v8i16)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V8HI, V4SI, V4SI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrln_h_w(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrln_h_w((v4i32)_1, (v4i32)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V4SI, V2DI, V2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vssrln_w_d(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vssrln_w_d((v2i64)_1, (v2i64)_2);
+}
+
+/* Assembly instruction format:          vd, vj, vk */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vorn_v(__m128i _1, __m128i _2) {
+  return (__m128i)__builtin_lsx_vorn_v((v16i8)_1, (v16i8)_2);
+}
+
+/* Assembly instruction format:          vd, i13 */
+/* Data types in instruction templates:  V2DI, HI */
+#define __lsx_vldi(/*i13*/ _1) ((__m128i)__builtin_lsx_vldi((_1)))
+
+/* Assembly instruction format:          vd, vj, vk, va */
+/* Data types in instruction templates:  V16QI, V16QI, V16QI, V16QI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vshuf_b(__m128i _1, __m128i _2, __m128i _3) {
+  return (__m128i)__builtin_lsx_vshuf_b((v16i8)_1, (v16i8)_2, (v16i8)_3);
+}
+
+/* Assembly instruction format:          vd, rj, rk */
+/* Data types in instruction templates:  V16QI, CVPOINTER, DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vldx(void *_1, long int _2) {
+  return (__m128i)__builtin_lsx_vldx((void *)_1, (long int)_2);
+}
+
+/* Assembly instruction format:          vd, rj, rk */
+/* Data types in instruction templates:  VOID, V16QI, CVPOINTER, DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) void
+    __lsx_vstx(__m128i _1, void *_2, long int _3) {
+  return (void)__builtin_lsx_vstx((v16i8)_1, (void *)_2, (long int)_3);
+}
+
+/* Assembly instruction format:          vd, vj */
+/* Data types in instruction templates:  UV2DI, UV2DI */
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) __m128i
+    __lsx_vextl_qu_du(__m128i _1) {
+  return (__m128i)__builtin_lsx_vextl_qu_du((v2u64)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lsx_bnz_v(__m128i _1) {
+  return __builtin_lsx_bnz_v((v16u8)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lsx_bz_v(__m128i _1) {
+  return __builtin_lsx_bz_v((v16u8)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lsx_bnz_b(__m128i _1) {
+  return __builtin_lsx_bnz_b((v16u8)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lsx_bnz_h(__m128i _1) {
+  return __builtin_lsx_bnz_h((v8u16)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lsx_bnz_w(__m128i _1) {
+  return __builtin_lsx_bnz_w((v4u32)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lsx_bnz_d(__m128i _1) {
+  return __builtin_lsx_bnz_d((v2u64)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lsx_bz_b(__m128i _1) {
+  return __builtin_lsx_bz_b((v16u8)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lsx_bz_h(__m128i _1) {
+  return __builtin_lsx_bz_h((v8u16)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lsx_bz_w(__m128i _1) {
+  return __builtin_lsx_bz_w((v4u32)_1);
+}
+
+extern __inline
+    __attribute__((__gnu_inline__, __always_inline__, __artificial__)) int
+    __lsx_bz_d(__m128i _1) {
+  return __builtin_lsx_bz_d((v2u64)_1);
+}
+
+#if 0
+/* Assembly instruction format:          vd, i10 */
+/* Data types in instruction templates:  V16QI, i10 */
+#define __lsx_vrepli_b(/*i10*/ _1)	((__m128i)__builtin_lsx_vrepli_b(_1)
+
+/* Assembly instruction format:          vd, i10 */
+/* Data types in instruction templates:  V8HI, i10 */
+#define __lsx_vrepli_h(/*i10*/ _1)	((__m128i)__builtin_lsx_vrepli_h(_1)
+
+/* Assembly instruction format:          vd, i10 */
+/* Data types in instruction templates:  V4SI, i10 */
+#define __lsx_vrepli_w(/*i10*/ _1)	((__m128i)__builtin_lsx_vrepli_w(_1)
+
+/* Assembly instruction format:          vd, i10 */
+/* Data types in instruction templates:  V2DI, i10 */
+#define __lsx_vrepli_d(/*i10*/ _1)	((__m128i)__builtin_lsx_vrepli_d(_1)
+
+#endif
+
+#endif /* defined(__loongarch_sx) */
+#endif /* _GCC_LOONGSON_SXINTRIN_H */
diff --git a/lib/Sema/SemaChecking.cpp b/lib/Sema/SemaChecking.cpp
index 69dcc3aa..d5721b52 100644
--- a/lib/Sema/SemaChecking.cpp
+++ b/lib/Sema/SemaChecking.cpp
@@ -1658,6 +1658,9 @@ bool Sema::CheckTSBuiltinFunctionCall(const TargetInfo &TI, unsigned BuiltinID,
   case llvm::Triple::riscv32:
   case llvm::Triple::riscv64:
     return CheckRISCVBuiltinFunctionCall(TI, BuiltinID, TheCall);
+  case llvm::Triple::loongarch32:
+  case llvm::Triple::loongarch64:
+    return CheckLoongArchBuiltinFunctionCall(TI, BuiltinID, TheCall);
   }
 }
 
@@ -4032,6 +4035,547 @@ bool Sema::CheckRISCVBuiltinFunctionCall(const TargetInfo &TI,
   return false;
 }
 
+// CheckLoongArchBuiltinFunctionCall - Checks the constant value passed to the
+// intrinsic is correct.
+//
+// FIXME: The size tests here should instead be tablegen'd along with the
+//        definitions from include/clang/Basic/BuiltinsLoongArch.def.
+// FIXME: GCC is strict on signedness for some of these intrinsics, we should
+//        be too.
+bool Sema::CheckLoongArchBuiltinFunctionCall(const TargetInfo &TI,
+                                             unsigned BuiltinID,
+                                             CallExpr *TheCall) {
+  unsigned i = 0, l = 0, u = 0, m = 0;
+  switch (BuiltinID) {
+  default: return false;
+  // LSX/LASX intrinsics.
+  // These intrinsics take an unsigned 3 bit immediate.
+  case LoongArch::BI__builtin_lsx_vbitclri_b:
+  case LoongArch::BI__builtin_lasx_xvbitclri_b:
+  case LoongArch::BI__builtin_lsx_vbitrevi_b:
+  case LoongArch::BI__builtin_lasx_xvbitrevi_b:
+  case LoongArch::BI__builtin_lsx_vbitseti_b:
+  case LoongArch::BI__builtin_lasx_xvbitseti_b:
+  case LoongArch::BI__builtin_lsx_vsat_b:
+  case LoongArch::BI__builtin_lsx_vsat_bu:
+  case LoongArch::BI__builtin_lasx_xvsat_b:
+  case LoongArch::BI__builtin_lasx_xvsat_bu:
+  case LoongArch::BI__builtin_lsx_vslli_b:
+  case LoongArch::BI__builtin_lasx_xvslli_b:
+  case LoongArch::BI__builtin_lsx_vsrai_b:
+  case LoongArch::BI__builtin_lasx_xvsrai_b:
+  case LoongArch::BI__builtin_lsx_vsrari_b:
+  case LoongArch::BI__builtin_lasx_xvsrari_b:
+  case LoongArch::BI__builtin_lsx_vsrli_b:
+  case LoongArch::BI__builtin_lasx_xvsrli_b:
+  case LoongArch::BI__builtin_lsx_vsllwil_h_b:
+  case LoongArch::BI__builtin_lsx_vsllwil_hu_bu:
+  case LoongArch::BI__builtin_lasx_xvsllwil_h_b:
+  case LoongArch::BI__builtin_lasx_xvsllwil_hu_bu:
+  case LoongArch::BI__builtin_lsx_vrotri_b:
+  case LoongArch::BI__builtin_lasx_xvrotri_b:
+  case LoongArch::BI__builtin_lasx_xvsrlri_b:
+  case LoongArch::BI__builtin_lsx_vsrlri_b:
+    i = 1;
+    l = 0;
+    u = 7;
+    break;
+  // These intrinsics take an unsigned 4 bit immediate.
+  case LoongArch::BI__builtin_lsx_vbitclri_h:
+  case LoongArch::BI__builtin_lasx_xvbitclri_h:
+  case LoongArch::BI__builtin_lsx_vbitrevi_h:
+  case LoongArch::BI__builtin_lasx_xvbitrevi_h:
+  case LoongArch::BI__builtin_lsx_vbitseti_h:
+  case LoongArch::BI__builtin_lasx_xvbitseti_h:
+  case LoongArch::BI__builtin_lsx_vsat_h:
+  case LoongArch::BI__builtin_lsx_vsat_hu:
+  case LoongArch::BI__builtin_lasx_xvsat_h:
+  case LoongArch::BI__builtin_lasx_xvsat_hu:
+  case LoongArch::BI__builtin_lsx_vslli_h:
+  case LoongArch::BI__builtin_lasx_xvslli_h:
+  case LoongArch::BI__builtin_lsx_vsrai_h:
+  case LoongArch::BI__builtin_lasx_xvsrai_h:
+  case LoongArch::BI__builtin_lsx_vsrari_h:
+  case LoongArch::BI__builtin_lasx_xvsrari_h:
+  case LoongArch::BI__builtin_lsx_vsrli_h:
+  case LoongArch::BI__builtin_lasx_xvsrli_h:
+  case LoongArch::BI__builtin_lsx_vsllwil_w_h:
+  case LoongArch::BI__builtin_lsx_vsllwil_wu_hu:
+  case LoongArch::BI__builtin_lasx_xvsllwil_w_h:
+  case LoongArch::BI__builtin_lasx_xvsllwil_wu_hu:
+  case LoongArch::BI__builtin_lsx_vrotri_h:
+  case LoongArch::BI__builtin_lasx_xvrotri_h:
+  case LoongArch::BI__builtin_lasx_xvsrlri_h:
+  case LoongArch::BI__builtin_lsx_vsrlri_h:
+    i = 1;
+    l = 0;
+    u = 15;
+    break;
+  case LoongArch::BI__builtin_lsx_vssrarni_b_h:
+  case LoongArch::BI__builtin_lsx_vssrarni_bu_h:
+  case LoongArch::BI__builtin_lasx_xvssrarni_b_h:
+  case LoongArch::BI__builtin_lasx_xvssrarni_bu_h:
+  case LoongArch::BI__builtin_lsx_vssrani_b_h:
+  case LoongArch::BI__builtin_lsx_vssrani_bu_h:
+  case LoongArch::BI__builtin_lasx_xvssrani_b_h:
+  case LoongArch::BI__builtin_lasx_xvssrani_bu_h:
+  case LoongArch::BI__builtin_lsx_vsrarni_b_h:
+  case LoongArch::BI__builtin_lasx_xvsrarni_b_h:
+  case LoongArch::BI__builtin_lsx_vsrlni_b_h:
+  case LoongArch::BI__builtin_lasx_xvsrlni_b_h:
+  case LoongArch::BI__builtin_lasx_xvsrlrni_b_h:
+  case LoongArch::BI__builtin_lsx_vssrlni_b_h:
+  case LoongArch::BI__builtin_lsx_vssrlni_bu_h:
+  case LoongArch::BI__builtin_lasx_xvssrlni_b_h:
+  case LoongArch::BI__builtin_lasx_xvssrlni_bu_h:
+  case LoongArch::BI__builtin_lsx_vssrlrni_b_h:
+  case LoongArch::BI__builtin_lsx_vssrlrni_bu_h:
+  case LoongArch::BI__builtin_lasx_xvssrlrni_b_h:
+  case LoongArch::BI__builtin_lasx_xvssrlrni_bu_h:
+  case LoongArch::BI__builtin_lsx_vsrani_b_h:
+  case LoongArch::BI__builtin_lasx_xvsrani_b_h:
+    i = 2;
+    l = 0;
+    u = 15;
+    break;
+  // These intrinsics take an unsigned 5 bit immediate.
+  // The first block of intrinsics actually have an unsigned 5 bit field,
+  // not a df/n field.
+  case LoongArch::BI__builtin_lsx_vslei_bu:
+  case LoongArch::BI__builtin_lsx_vslei_hu:
+  case LoongArch::BI__builtin_lsx_vslei_wu:
+  case LoongArch::BI__builtin_lsx_vslei_du:
+  case LoongArch::BI__builtin_lasx_xvslei_bu:
+  case LoongArch::BI__builtin_lasx_xvslei_hu:
+  case LoongArch::BI__builtin_lasx_xvslei_wu:
+  case LoongArch::BI__builtin_lasx_xvslei_du:
+  case LoongArch::BI__builtin_lsx_vslti_bu:
+  case LoongArch::BI__builtin_lsx_vslti_hu:
+  case LoongArch::BI__builtin_lsx_vslti_wu:
+  case LoongArch::BI__builtin_lsx_vslti_du:
+  case LoongArch::BI__builtin_lasx_xvslti_bu:
+  case LoongArch::BI__builtin_lasx_xvslti_hu:
+  case LoongArch::BI__builtin_lasx_xvslti_wu:
+  case LoongArch::BI__builtin_lasx_xvslti_du:
+  case LoongArch::BI__builtin_lsx_vmaxi_bu:
+  case LoongArch::BI__builtin_lsx_vmaxi_hu:
+  case LoongArch::BI__builtin_lsx_vmaxi_wu:
+  case LoongArch::BI__builtin_lsx_vmaxi_du:
+  case LoongArch::BI__builtin_lasx_xvmaxi_bu:
+  case LoongArch::BI__builtin_lasx_xvmaxi_hu:
+  case LoongArch::BI__builtin_lasx_xvmaxi_wu:
+  case LoongArch::BI__builtin_lasx_xvmaxi_du:
+  case LoongArch::BI__builtin_lsx_vmini_bu:
+  case LoongArch::BI__builtin_lsx_vmini_hu:
+  case LoongArch::BI__builtin_lsx_vmini_wu:
+  case LoongArch::BI__builtin_lsx_vmini_du:
+  case LoongArch::BI__builtin_lasx_xvmini_bu:
+  case LoongArch::BI__builtin_lasx_xvmini_hu:
+  case LoongArch::BI__builtin_lasx_xvmini_wu:
+  case LoongArch::BI__builtin_lasx_xvmini_du:
+  case LoongArch::BI__builtin_lsx_vaddi_bu:
+  case LoongArch::BI__builtin_lsx_vaddi_hu:
+  case LoongArch::BI__builtin_lsx_vaddi_wu:
+  case LoongArch::BI__builtin_lsx_vaddi_du:
+  case LoongArch::BI__builtin_lasx_xvaddi_bu:
+  case LoongArch::BI__builtin_lasx_xvaddi_hu:
+  case LoongArch::BI__builtin_lasx_xvaddi_wu:
+  case LoongArch::BI__builtin_lasx_xvaddi_du:
+  case LoongArch::BI__builtin_lsx_vbitclri_w:
+  case LoongArch::BI__builtin_lasx_xvbitclri_w:
+  case LoongArch::BI__builtin_lsx_vbitrevi_w:
+  case LoongArch::BI__builtin_lasx_xvbitrevi_w:
+  case LoongArch::BI__builtin_lsx_vbitseti_w:
+  case LoongArch::BI__builtin_lasx_xvbitseti_w:
+  case LoongArch::BI__builtin_lsx_vsat_w:
+  case LoongArch::BI__builtin_lsx_vsat_wu:
+  case LoongArch::BI__builtin_lasx_xvsat_w:
+  case LoongArch::BI__builtin_lasx_xvsat_wu:
+  case LoongArch::BI__builtin_lsx_vslli_w:
+  case LoongArch::BI__builtin_lasx_xvslli_w:
+  case LoongArch::BI__builtin_lsx_vsrai_w:
+  case LoongArch::BI__builtin_lasx_xvsrai_w:
+  case LoongArch::BI__builtin_lsx_vsrari_w:
+  case LoongArch::BI__builtin_lasx_xvsrari_w:
+  case LoongArch::BI__builtin_lsx_vsrli_w:
+  case LoongArch::BI__builtin_lasx_xvsrli_w:
+  case LoongArch::BI__builtin_lsx_vsllwil_d_w:
+  case LoongArch::BI__builtin_lsx_vsllwil_du_wu:
+  case LoongArch::BI__builtin_lasx_xvsllwil_d_w:
+  case LoongArch::BI__builtin_lasx_xvsllwil_du_wu:
+  case LoongArch::BI__builtin_lsx_vsrlri_w:
+  case LoongArch::BI__builtin_lasx_xvsrlri_w:
+  case LoongArch::BI__builtin_lsx_vrotri_w:
+  case LoongArch::BI__builtin_lasx_xvrotri_w:
+  case LoongArch::BI__builtin_lsx_vsubi_bu:
+  case LoongArch::BI__builtin_lsx_vsubi_hu:
+  case LoongArch::BI__builtin_lasx_xvsubi_bu:
+  case LoongArch::BI__builtin_lasx_xvsubi_hu:
+  case LoongArch::BI__builtin_lasx_xvsubi_wu:
+  case LoongArch::BI__builtin_lasx_xvsubi_du:
+  case LoongArch::BI__builtin_lsx_vbsrl_v:
+  case LoongArch::BI__builtin_lsx_vbsll_v:
+  case LoongArch::BI__builtin_lasx_xvbsrl_v:
+  case LoongArch::BI__builtin_lasx_xvbsll_v:
+  case LoongArch::BI__builtin_lsx_vsubi_wu:
+  case LoongArch::BI__builtin_lsx_vsubi_du:
+    i = 1;
+    l = 0;
+    u = 31;
+    break;
+  case LoongArch::BI__builtin_lsx_vssrarni_h_w:
+  case LoongArch::BI__builtin_lsx_vssrarni_hu_w:
+  case LoongArch::BI__builtin_lasx_xvssrarni_h_w:
+  case LoongArch::BI__builtin_lasx_xvssrarni_hu_w:
+  case LoongArch::BI__builtin_lsx_vssrani_h_w:
+  case LoongArch::BI__builtin_lsx_vssrani_hu_w:
+  case LoongArch::BI__builtin_lasx_xvssrani_h_w:
+  case LoongArch::BI__builtin_lasx_xvssrani_hu_w:
+  case LoongArch::BI__builtin_lsx_vsrarni_h_w:
+  case LoongArch::BI__builtin_lasx_xvsrarni_h_w:
+  case LoongArch::BI__builtin_lsx_vsrani_h_w:
+  case LoongArch::BI__builtin_lasx_xvsrani_h_w:
+  case LoongArch::BI__builtin_lsx_vfrstpi_b:
+  case LoongArch::BI__builtin_lsx_vfrstpi_h:
+  case LoongArch::BI__builtin_lasx_xvfrstpi_b:
+  case LoongArch::BI__builtin_lasx_xvfrstpi_h:
+  case LoongArch::BI__builtin_lsx_vsrlni_h_w:
+  case LoongArch::BI__builtin_lasx_xvsrlni_h_w:
+  case LoongArch::BI__builtin_lasx_xvsrlrni_h_w:
+  case LoongArch::BI__builtin_lsx_vssrlni_h_w:
+  case LoongArch::BI__builtin_lsx_vssrlni_hu_w:
+  case LoongArch::BI__builtin_lasx_xvssrlni_h_w:
+  case LoongArch::BI__builtin_lasx_xvssrlni_hu_w:
+  case LoongArch::BI__builtin_lsx_vssrlrni_h_w:
+  case LoongArch::BI__builtin_lsx_vssrlrni_hu_w:
+  case LoongArch::BI__builtin_lasx_xvssrlrni_h_w:
+  case LoongArch::BI__builtin_lasx_xvssrlrni_hu_w:
+    i = 2;
+    l = 0;
+    u = 31;
+    break;
+  case LoongArch::BI__builtin_lasx_xvstelm_b:
+    return SemaBuiltinConstantArgRange(TheCall, 2, -128, 127) ||
+           SemaBuiltinConstantArgRange(TheCall, 3, 0, 31);
+  // These intrinsics take an unsigned 6 bit immediate.
+  case LoongArch::BI__builtin_lsx_vbitclri_d:
+  case LoongArch::BI__builtin_lasx_xvbitclri_d:
+  case LoongArch::BI__builtin_lsx_vbitrevi_d:
+  case LoongArch::BI__builtin_lasx_xvbitrevi_d:
+  case LoongArch::BI__builtin_lsx_vbitseti_d:
+  case LoongArch::BI__builtin_lasx_xvbitseti_d:
+  case LoongArch::BI__builtin_lsx_vsat_d:
+  case LoongArch::BI__builtin_lsx_vsat_du:
+  case LoongArch::BI__builtin_lasx_xvsat_d:
+  case LoongArch::BI__builtin_lasx_xvsat_du:
+  case LoongArch::BI__builtin_lsx_vslli_d:
+  case LoongArch::BI__builtin_lasx_xvslli_d:
+  case LoongArch::BI__builtin_lsx_vsrai_d:
+  case LoongArch::BI__builtin_lasx_xvsrai_d:
+  case LoongArch::BI__builtin_lsx_vsrli_d:
+  case LoongArch::BI__builtin_lasx_xvsrli_d:
+  case LoongArch::BI__builtin_lsx_vsrari_d:
+  case LoongArch::BI__builtin_lasx_xvsrari_d:
+  case LoongArch::BI__builtin_lsx_vrotri_d:
+  case LoongArch::BI__builtin_lasx_xvrotri_d:
+  case LoongArch::BI__builtin_lasx_xvsrlri_d:
+  case LoongArch::BI__builtin_lsx_vsrlri_d:
+    i = 1;
+    l = 0;
+    u = 63;
+    break;
+  case LoongArch::BI__builtin_lsx_vssrarni_w_d:
+  case LoongArch::BI__builtin_lsx_vssrarni_wu_d:
+  case LoongArch::BI__builtin_lasx_xvssrarni_w_d:
+  case LoongArch::BI__builtin_lasx_xvssrarni_wu_d:
+  case LoongArch::BI__builtin_lsx_vssrani_w_d:
+  case LoongArch::BI__builtin_lsx_vssrani_wu_d:
+  case LoongArch::BI__builtin_lasx_xvssrani_w_d:
+  case LoongArch::BI__builtin_lasx_xvssrani_wu_d:
+  case LoongArch::BI__builtin_lsx_vsrarni_w_d:
+  case LoongArch::BI__builtin_lasx_xvsrarni_w_d:
+  case LoongArch::BI__builtin_lsx_vsrlni_w_d:
+  case LoongArch::BI__builtin_lasx_xvsrlni_w_d:
+  case LoongArch::BI__builtin_lasx_xvsrlrni_w_d:
+  case LoongArch::BI__builtin_lsx_vssrlni_w_d:
+  case LoongArch::BI__builtin_lsx_vssrlni_wu_d:
+  case LoongArch::BI__builtin_lasx_xvssrlni_w_d:
+  case LoongArch::BI__builtin_lasx_xvssrlni_wu_d:
+  case LoongArch::BI__builtin_lsx_vssrlrni_w_d:
+  case LoongArch::BI__builtin_lsx_vssrlrni_wu_d:
+  case LoongArch::BI__builtin_lasx_xvssrlrni_w_d:
+  case LoongArch::BI__builtin_lasx_xvssrlrni_wu_d:
+  case LoongArch::BI__builtin_lsx_vsrani_w_d:
+  case LoongArch::BI__builtin_lasx_xvsrani_w_d:
+    i = 2;
+    l = 0;
+    u = 63;
+    break;
+  // These intrinsics take an unsigned 7 bit immediate.
+  case LoongArch::BI__builtin_lsx_vssrarni_d_q:
+  case LoongArch::BI__builtin_lsx_vssrarni_du_q:
+  case LoongArch::BI__builtin_lasx_xvssrarni_d_q:
+  case LoongArch::BI__builtin_lasx_xvssrarni_du_q:
+  case LoongArch::BI__builtin_lsx_vssrani_d_q:
+  case LoongArch::BI__builtin_lsx_vssrani_du_q:
+  case LoongArch::BI__builtin_lasx_xvssrani_d_q:
+  case LoongArch::BI__builtin_lasx_xvssrani_du_q:
+  case LoongArch::BI__builtin_lsx_vsrarni_d_q:
+  case LoongArch::BI__builtin_lasx_xvsrarni_d_q:
+  case LoongArch::BI__builtin_lsx_vssrlni_d_q:
+  case LoongArch::BI__builtin_lsx_vssrlni_du_q:
+  case LoongArch::BI__builtin_lasx_xvssrlni_d_q:
+  case LoongArch::BI__builtin_lasx_xvssrlni_du_q:
+  case LoongArch::BI__builtin_lsx_vssrlrni_d_q:
+  case LoongArch::BI__builtin_lsx_vssrlrni_du_q:
+  case LoongArch::BI__builtin_lasx_xvssrlrni_d_q:
+  case LoongArch::BI__builtin_lasx_xvssrlrni_du_q:
+  case LoongArch::BI__builtin_lsx_vsrani_d_q:
+  case LoongArch::BI__builtin_lasx_xvsrani_d_q:
+  case LoongArch::BI__builtin_lasx_xvsrlni_d_q:
+  case LoongArch::BI__builtin_lasx_xvsrlrni_d_q:
+  case LoongArch::BI__builtin_lsx_vsrlni_d_q:
+    i = 2;
+    l = 0;
+    u = 127;
+    break;
+  // These intrinsics take a signed 5 bit immediate.
+  case LoongArch::BI__builtin_lsx_vseqi_b:
+  case LoongArch::BI__builtin_lsx_vseqi_h:
+  case LoongArch::BI__builtin_lsx_vseqi_w:
+  case LoongArch::BI__builtin_lsx_vseqi_d:
+  case LoongArch::BI__builtin_lasx_xvseqi_b:
+  case LoongArch::BI__builtin_lasx_xvseqi_h:
+  case LoongArch::BI__builtin_lasx_xvseqi_w:
+  case LoongArch::BI__builtin_lasx_xvseqi_d:
+  case LoongArch::BI__builtin_lsx_vslti_b:
+  case LoongArch::BI__builtin_lsx_vslti_h:
+  case LoongArch::BI__builtin_lsx_vslti_w:
+  case LoongArch::BI__builtin_lsx_vslti_d:
+  case LoongArch::BI__builtin_lasx_xvslti_b:
+  case LoongArch::BI__builtin_lasx_xvslti_h:
+  case LoongArch::BI__builtin_lasx_xvslti_w:
+  case LoongArch::BI__builtin_lasx_xvslti_d:
+  case LoongArch::BI__builtin_lsx_vslei_b:
+  case LoongArch::BI__builtin_lsx_vslei_h:
+  case LoongArch::BI__builtin_lsx_vslei_w:
+  case LoongArch::BI__builtin_lsx_vslei_d:
+  case LoongArch::BI__builtin_lasx_xvslei_b:
+  case LoongArch::BI__builtin_lasx_xvslei_h:
+  case LoongArch::BI__builtin_lasx_xvslei_w:
+  case LoongArch::BI__builtin_lasx_xvslei_d:
+  case LoongArch::BI__builtin_lsx_vmaxi_b:
+  case LoongArch::BI__builtin_lsx_vmaxi_h:
+  case LoongArch::BI__builtin_lsx_vmaxi_w:
+  case LoongArch::BI__builtin_lsx_vmaxi_d:
+  case LoongArch::BI__builtin_lasx_xvmaxi_b:
+  case LoongArch::BI__builtin_lasx_xvmaxi_h:
+  case LoongArch::BI__builtin_lasx_xvmaxi_w:
+  case LoongArch::BI__builtin_lasx_xvmaxi_d:
+  case LoongArch::BI__builtin_lsx_vmini_b:
+  case LoongArch::BI__builtin_lsx_vmini_h:
+  case LoongArch::BI__builtin_lsx_vmini_w:
+  case LoongArch::BI__builtin_lasx_xvmini_b:
+  case LoongArch::BI__builtin_lasx_xvmini_h:
+  case LoongArch::BI__builtin_lasx_xvmini_w:
+  case LoongArch::BI__builtin_lasx_xvmini_d:
+  case LoongArch::BI__builtin_lsx_vmini_d:
+    i = 1;
+    l = -16;
+    u = 15;
+    break;
+  // These intrinsics take a signed 9 bit immediate.
+  case LoongArch::BI__builtin_lasx_xvldrepl_d:
+  case LoongArch::BI__builtin_lsx_vldrepl_d:
+    i = 1;
+    l = -256;
+    u = 255;
+    break;
+  // These intrinsics take an unsigned 8 bit immediate.
+  case LoongArch::BI__builtin_lsx_vandi_b:
+  case LoongArch::BI__builtin_lasx_xvandi_b:
+  case LoongArch::BI__builtin_lsx_vnori_b:
+  case LoongArch::BI__builtin_lasx_xvnori_b:
+  case LoongArch::BI__builtin_lsx_vori_b:
+  case LoongArch::BI__builtin_lasx_xvori_b:
+  case LoongArch::BI__builtin_lsx_vshuf4i_b:
+  case LoongArch::BI__builtin_lsx_vshuf4i_h:
+  case LoongArch::BI__builtin_lsx_vshuf4i_w:
+  case LoongArch::BI__builtin_lasx_xvshuf4i_b:
+  case LoongArch::BI__builtin_lasx_xvshuf4i_h:
+  case LoongArch::BI__builtin_lasx_xvshuf4i_w:
+  case LoongArch::BI__builtin_lasx_xvxori_b:
+  case LoongArch::BI__builtin_lasx_xvpermi_d:
+  case LoongArch::BI__builtin_lsx_vxori_b:
+    i = 1;
+    l = 0;
+    u = 255;
+    break;
+  case LoongArch::BI__builtin_lsx_vbitseli_b:
+  case LoongArch::BI__builtin_lasx_xvbitseli_b:
+  case LoongArch::BI__builtin_lsx_vshuf4i_d:
+  case LoongArch::BI__builtin_lasx_xvshuf4i_d:
+  case LoongArch::BI__builtin_lsx_vextrins_b:
+  case LoongArch::BI__builtin_lsx_vextrins_h:
+  case LoongArch::BI__builtin_lsx_vextrins_w:
+  case LoongArch::BI__builtin_lsx_vextrins_d:
+  case LoongArch::BI__builtin_lasx_xvextrins_b:
+  case LoongArch::BI__builtin_lasx_xvextrins_h:
+  case LoongArch::BI__builtin_lasx_xvextrins_w:
+  case LoongArch::BI__builtin_lasx_xvextrins_d:
+  case LoongArch::BI__builtin_lasx_xvpermi_q:
+  case LoongArch::BI__builtin_lsx_vpermi_w:
+  case LoongArch::BI__builtin_lasx_xvpermi_w:
+    i = 2;
+    l = 0;
+    u = 255;
+    break;
+  // df/n format
+  // These intrinsics take an unsigned 4 bit immediate.
+  case LoongArch::BI__builtin_lsx_vpickve2gr_b:
+  case LoongArch::BI__builtin_lsx_vpickve2gr_bu:
+  case LoongArch::BI__builtin_lasx_xvrepl128vei_b:
+  case LoongArch::BI__builtin_lsx_vreplvei_b:
+    i = 1;
+    l = 0;
+    u = 15;
+    break;
+  case LoongArch::BI__builtin_lsx_vinsgr2vr_b:
+    i = 2;
+    l = 0;
+    u = 15;
+    break;
+  case LoongArch::BI__builtin_lasx_xvstelm_h:
+  case LoongArch::BI__builtin_lsx_vstelm_b:
+    return SemaBuiltinConstantArgRange(TheCall, 2, -128, 127) ||
+           SemaBuiltinConstantArgRange(TheCall, 3, 0, 15);
+  // These intrinsics take an unsigned 3 bit immediate.
+  case LoongArch::BI__builtin_lsx_vpickve2gr_h:
+  case LoongArch::BI__builtin_lsx_vpickve2gr_hu:
+  case LoongArch::BI__builtin_lasx_xvrepl128vei_h:
+  case LoongArch::BI__builtin_lasx_xvpickve2gr_w:
+  case LoongArch::BI__builtin_lasx_xvpickve2gr_wu:
+  case LoongArch::BI__builtin_lasx_xvpickve_w:
+  case LoongArch::BI__builtin_lsx_vreplvei_h:
+    i = 1;
+    l = 0;
+    u = 7;
+    break;
+  case LoongArch::BI__builtin_lsx_vinsgr2vr_h:
+  case LoongArch::BI__builtin_lasx_xvinsgr2vr_w:
+  case LoongArch::BI__builtin_lasx_xvinsve0_w:
+    i = 2;
+    l = 0;
+    u = 7;
+    break;
+  case LoongArch::BI__builtin_lasx_xvstelm_w:
+  case LoongArch::BI__builtin_lsx_vstelm_h:
+    return SemaBuiltinConstantArgRange(TheCall, 2, -128, 127) ||
+           SemaBuiltinConstantArgRange(TheCall, 3, 0, 7);
+  // These intrinsics take an unsigned 2 bit immediate.
+  case LoongArch::BI__builtin_lsx_vpickve2gr_w:
+  case LoongArch::BI__builtin_lsx_vpickve2gr_wu:
+  case LoongArch::BI__builtin_lasx_xvrepl128vei_w:
+  case LoongArch::BI__builtin_lasx_xvpickve2gr_d:
+  case LoongArch::BI__builtin_lasx_xvpickve2gr_du:
+  case LoongArch::BI__builtin_lasx_xvpickve_d:
+  case LoongArch::BI__builtin_lsx_vreplvei_w:
+    i = 1;
+    l = 0;
+    u = 3;
+    break;
+  case LoongArch::BI__builtin_lsx_vinsgr2vr_w:
+  case LoongArch::BI__builtin_lasx_xvinsve0_d:
+  case LoongArch::BI__builtin_lasx_xvinsgr2vr_d:
+    i = 2;
+    l = 0;
+    u = 3;
+    break;
+  case LoongArch::BI__builtin_lasx_xvstelm_d:
+  case LoongArch::BI__builtin_lsx_vstelm_w:
+    return SemaBuiltinConstantArgRange(TheCall, 2, -128, 127) ||
+           SemaBuiltinConstantArgRange(TheCall, 3, 0, 3);
+  // These intrinsics take an unsigned 1 bit immediate.
+  case LoongArch::BI__builtin_lsx_vpickve2gr_d:
+  case LoongArch::BI__builtin_lsx_vpickve2gr_du:
+  case LoongArch::BI__builtin_lasx_xvrepl128vei_d:
+  case LoongArch::BI__builtin_lsx_vreplvei_d:
+    i = 1;
+    l = 0;
+    u = 1;
+    break;
+  case LoongArch::BI__builtin_lsx_vinsgr2vr_d:
+    i = 2;
+    l = 0;
+    u = 1;
+    break;
+  case LoongArch::BI__builtin_lsx_vstelm_d:
+    return SemaBuiltinConstantArgRange(TheCall, 2, -128, 127) ||
+           SemaBuiltinConstantArgRange(TheCall, 3, 0, 1);
+  // Memory offsets and immediate loads.
+  // These intrinsics take a signed 10 bit immediate.
+  case LoongArch::BI__builtin_lasx_xvldrepl_w:
+  case LoongArch::BI__builtin_lsx_vldrepl_w:
+    i = 1;
+    l = -512;
+    u = 511;
+    break;
+  case LoongArch::BI__builtin_lasx_xvldrepl_h:
+  case LoongArch::BI__builtin_lsx_vldrepl_h:
+    i = 1;
+    l = -1024;
+    u = 1023;
+    break;
+  case LoongArch::BI__builtin_lasx_xvldrepl_b:
+  case LoongArch::BI__builtin_lsx_vldrepl_b:
+    i = 1;
+    l = -2048;
+    u = 2047;
+    break;
+  case LoongArch::BI__builtin_lasx_xvld:
+  case LoongArch::BI__builtin_lsx_vld:
+    i = 1;
+    l = -2048;
+    u = 2047;
+    break;
+  case LoongArch::BI__builtin_lsx_vst:
+  case LoongArch::BI__builtin_lasx_xvst:
+    i = 2;
+    l = -2048;
+    u = 2047;
+    break;
+  case LoongArch::BI__builtin_lasx_xvldi:
+  case LoongArch::BI__builtin_lsx_vldi:
+    i = 0;
+    l = -4096;
+    u = 4095;
+    break;
+  // These intrinsics take an unsigned 5 bit immediate and a signed 12 bit immediate.
+  case LoongArch::BI__builtin_loongarch_cacop:
+  case LoongArch::BI__builtin_loongarch_dcacop:
+    return SemaBuiltinConstantArgRange(TheCall, 0, 0, 31) ||
+           SemaBuiltinConstantArgRange(TheCall, 2, -2048, 2047);
+  // These intrinsics take an unsigned 14 bit immediate.
+  case LoongArch::BI__builtin_loongarch_csrrd:
+  case LoongArch::BI__builtin_loongarch_dcsrrd: i = 0; l = 0; u = 16383; break;
+  case LoongArch::BI__builtin_loongarch_csrwr:
+  case LoongArch::BI__builtin_loongarch_dcsrwr: i = 1; l = 0; u = 16383; break;
+  case LoongArch::BI__builtin_loongarch_csrxchg:
+  case LoongArch::BI__builtin_loongarch_dcsrxchg: i = 2; l = 0; u = 16383; break;
+  // These intrinsics take an unsigned 15 bit immediate.
+  case LoongArch::BI__builtin_loongarch_dbar:
+  case LoongArch::BI__builtin_loongarch_ibar:
+  case LoongArch::BI__builtin_loongarch_syscall:
+  case LoongArch::BI__builtin_loongarch_break: i = 0; l = 0; u = 32767; break;
+  }
+
+  if (!m)
+    return SemaBuiltinConstantArgRange(TheCall, i, l, u);
+
+  return SemaBuiltinConstantArgRange(TheCall, i, l, u) ||
+         SemaBuiltinConstantArgMultiple(TheCall, i, m);
+}
+
 bool Sema::CheckSystemZBuiltinFunctionCall(unsigned BuiltinID,
                                            CallExpr *TheCall) {
   if (BuiltinID == SystemZ::BI__builtin_tabort) {
diff --git a/lib/Sema/SemaTemplateInstantiateDecl.cpp b/lib/Sema/SemaTemplateInstantiateDecl.cpp
index 29378282..467372c7 100644
--- a/lib/Sema/SemaTemplateInstantiateDecl.cpp
+++ b/lib/Sema/SemaTemplateInstantiateDecl.cpp
@@ -4826,8 +4826,7 @@ void Sema::InstantiateFunctionDefinition(SourceLocation PointOfInstantiation,
                                      /*Complain*/DefinitionRequired)) {
     if (DefinitionRequired)
       Function->setInvalidDecl();
-    else if (TSK == TSK_ExplicitInstantiationDefinition ||
-             (Function->isConstexpr() && !Recursive)) {
+    else if (TSK == TSK_ExplicitInstantiationDefinition) {
       // Try again at the end of the translation unit (at which point a
       // definition will be required).
       assert(!Recursive);
@@ -4842,7 +4841,7 @@ void Sema::InstantiateFunctionDefinition(SourceLocation PointOfInstantiation,
         Diag(PatternDecl->getLocation(), diag::note_forward_template_decl);
         if (getLangOpts().CPlusPlus11)
           Diag(PointOfInstantiation, diag::note_inst_declaration_hint)
-              << Function;
+            << Function;
       }
     }
 
diff --git a/test/CodeGen/sanitize-coverage-old-pm.c b/test/CodeGen/sanitize-coverage-old-pm.c
index 9b4f8991..18123a53 100644
--- a/test/CodeGen/sanitize-coverage-old-pm.c
+++ b/test/CodeGen/sanitize-coverage-old-pm.c
@@ -7,8 +7,8 @@
 //
 // Host armv7 is currently unsupported: https://bugs.llvm.org/show_bug.cgi?id=46117
 // UNSUPPORTED: armv7, armv7l, thumbv7, armv8l
-// The same issue also occurs on a riscv32 host.
-// XFAIL: riscv32
+// The same issue also occurs on riscv32 and loongarch64 hosts.
+// XFAIL: riscv32, loongarch64
 
 int x[10];
 
diff --git a/test/CodeGen/ubsan-function.cpp b/test/CodeGen/ubsan-function.cpp
index 8a16dfdf..2466d8a2 100644
--- a/test/CodeGen/ubsan-function.cpp
+++ b/test/CodeGen/ubsan-function.cpp
@@ -1,7 +1,6 @@
 // RUN: %clang_cc1 -triple x86_64-linux-gnu -emit-llvm -o - %s -fsanitize=function -fno-sanitize-recover=all | FileCheck %s
 
-// CHECK: @[[PROXY:.*]] = private unnamed_addr constant i8* bitcast ({ i8*, i8* }* @_ZTIFvvE to i8*)
-// CHECK: define{{.*}} void @_Z3funv() #0 !func_sanitize ![[FUNCSAN:.*]] {
+// CHECK-LABEL: define{{.*}} void @_Z3funv() #0 prologue <{ i32, i32 }> <{ i32 846595819, i32 trunc (i64 sub (i64 ptrtoint (i8** @0 to i64), i64 ptrtoint (void ()* @_Z3funv to i64)) to i32) }> {
 void fun() {}
 
 // CHECK-LABEL: define{{.*}} void @_Z6callerPFvvE(void ()* noundef %f)
@@ -21,5 +20,3 @@ void fun() {}
 // CHECK: [[LABEL3]]:
 // CHECK: br label %[[LABEL4]], !nosanitize
 void caller(void (*f)()) { f(); }
-
-// CHECK: ![[FUNCSAN]] = !{i32 846595819, i8** @[[PROXY]]}
diff --git a/test/CodeGenCXX/catch-undef-behavior.cpp b/test/CodeGenCXX/catch-undef-behavior.cpp
index ade29797..d6b094cb 100644
--- a/test/CodeGenCXX/catch-undef-behavior.cpp
+++ b/test/CodeGenCXX/catch-undef-behavior.cpp
@@ -1,8 +1,8 @@
-// RUN: %clang_cc1 -disable-noundef-analysis -std=c++11 -fsanitize=signed-integer-overflow,integer-divide-by-zero,float-divide-by-zero,shift-base,shift-exponent,unreachable,return,vla-bound,alignment,null,vptr,object-size,float-cast-overflow,bool,enum,array-bounds,function -fsanitize-recover=signed-integer-overflow,integer-divide-by-zero,float-divide-by-zero,shift-base,shift-exponent,vla-bound,alignment,null,vptr,object-size,float-cast-overflow,bool,enum,array-bounds,function -emit-llvm %s -o - -triple x86_64-linux-gnu | opt -instnamer -S | FileCheck %s --check-prefixes=CHECK,CHECK-FUNCSAN
+// RUN: %clang_cc1 -disable-noundef-analysis -std=c++11 -fsanitize=signed-integer-overflow,integer-divide-by-zero,float-divide-by-zero,shift-base,shift-exponent,unreachable,return,vla-bound,alignment,null,vptr,object-size,float-cast-overflow,bool,enum,array-bounds,function -fsanitize-recover=signed-integer-overflow,integer-divide-by-zero,float-divide-by-zero,shift-base,shift-exponent,vla-bound,alignment,null,vptr,object-size,float-cast-overflow,bool,enum,array-bounds,function -emit-llvm %s -o - -triple x86_64-linux-gnu | opt -instnamer -S | FileCheck %s
 // RUN: %clang_cc1 -disable-noundef-analysis -std=c++11 -fsanitize=vptr,address -fsanitize-recover=vptr,address -emit-llvm %s -o - -triple x86_64-linux-gnu | FileCheck %s --check-prefix=CHECK-ASAN
 // RUN: %clang_cc1 -disable-noundef-analysis -std=c++11 -fsanitize=vptr -fsanitize-recover=vptr -emit-llvm %s -o - -triple x86_64-linux-gnu | FileCheck %s --check-prefix=DOWNCAST-NULL
-// RUN: %clang_cc1 -disable-noundef-analysis -std=c++11 -fsanitize=function -emit-llvm %s -o - -triple x86_64-linux-gnux32 | FileCheck %s --check-prefix=CHECK-FUNCSAN
-// RUN: %clang_cc1 -disable-noundef-analysis -std=c++11 -fsanitize=function -emit-llvm %s -o - -triple i386-linux-gnu | FileCheck %s --check-prefix=CHECK-FUNCSAN
+// RUN: %clang_cc1 -disable-noundef-analysis -std=c++11 -fsanitize=function -emit-llvm %s -o - -triple x86_64-linux-gnux32 | FileCheck %s --check-prefix=CHECK-X32
+// RUN: %clang_cc1 -disable-noundef-analysis -std=c++11 -fsanitize=function -emit-llvm %s -o - -triple i386-linux-gnu | FileCheck %s --check-prefix=CHECK-X86
 
 struct S {
   double d;
@@ -16,7 +16,9 @@ struct S {
 // Check that type mismatch handler is not modified by ASan.
 // CHECK-ASAN: private unnamed_addr global { { [{{.*}} x i8]*, i32, i32 }, { i16, i16, [4 x i8] }*, i8*, i8 } { {{.*}}, { i16, i16, [4 x i8] }* [[TYPE_DESCR]], {{.*}} }
 
-// CHECK-FUNCSAN: [[PROXY:@.+]] = private unnamed_addr constant i8* bitcast ({ i8*, i8* }* @_ZTIFvPFviEE to i8*)
+// CHECK: [[IndirectRTTI_ZTIFvPFviEE:@.+]] = private constant i8* bitcast ({ i8*, i8* }* @_ZTIFvPFviEE to i8*)
+// CHECK-X86: [[IndirectRTTI_ZTIFvPFviEE:@.+]] = private constant i8* bitcast ({ i8*, i8* }* @_ZTIFvPFviEE to i8*)
+// CHECK-X32: [[IndirectRTTI_ZTIFvPFviEE:@.+]] = private constant i8* bitcast ({ i8*, i8* }* @_ZTIFvPFviEE to i8*)
 
 struct T : S {};
 
@@ -397,7 +399,10 @@ void downcast_reference(B &b) {
   // CHECK-NEXT: br i1 [[AND]]
 }
 
-// CHECK-FUNCSAN: @_Z22indirect_function_callPFviE({{.*}} !func_sanitize ![[FUNCSAN:.*]] {
+//
+// CHECK-LABEL: @_Z22indirect_function_callPFviE({{.*}} prologue <{ i32, i32 }> <{ i32 846595819, i32 trunc (i64 sub (i64 ptrtoint (i8** {{.*}} to i64), i64 ptrtoint (void (void (i32)*)* @_Z22indirect_function_callPFviE to i64)) to i32) }>
+// CHECK-X32: @_Z22indirect_function_callPFviE({{.*}} prologue <{ i32, i32 }> <{ i32 846595819, i32 sub (i32 ptrtoint (i8** [[IndirectRTTI_ZTIFvPFviEE]] to i32), i32 ptrtoint (void (void (i32)*)* @_Z22indirect_function_callPFviE to i32)) }>
+// CHECK-X86: @_Z22indirect_function_callPFviE({{.*}} prologue <{ i32, i32 }> <{ i32 846595819, i32 sub (i32 ptrtoint (i8** [[IndirectRTTI_ZTIFvPFviEE]] to i32), i32 ptrtoint (void (void (i32)*)* @_Z22indirect_function_callPFviE to i32)) }>
 void indirect_function_call(void (*p)(int)) {
   // CHECK: [[PTR:%.+]] = bitcast void (i32)* {{.*}} to <{ i32, i32 }>*
 
@@ -478,34 +483,34 @@ void force_irgen() {
 }
 
 // CHECK-LABEL: define{{.*}} void @_ZN29FunctionSanitizerVirtualCalls1B1fEv
-// CHECK-NOT: !func_sanitize
+// CHECK-NOT: prologue
 //
 // CHECK-LABEL: define{{.*}} void @_ZTv0_n24_N29FunctionSanitizerVirtualCalls1B1fEv
-// CHECK-NOT: !func_sanitize
+// CHECK-NOT: prologue
 //
 // CHECK-LABEL: define{{.*}} void @_ZN29FunctionSanitizerVirtualCalls11force_irgenEv()
-// CHECK: !func_sanitize
+// CHECK: prologue
 //
 // CHECK-LABEL: define linkonce_odr void @_ZN29FunctionSanitizerVirtualCalls1AC1Ev
-// CHECK-NOT: !func_sanitize
+// CHECK-NOT: prologue
 //
 // CHECK-LABEL: define linkonce_odr void @_ZN29FunctionSanitizerVirtualCalls1A1gEv
-// CHECK-NOT: !func_sanitize
+// CHECK-NOT: prologue
 //
 // CHECK-LABEL: define linkonce_odr void @_ZN29FunctionSanitizerVirtualCalls1A1hEv
-// CHECK-NOT: !func_sanitize
+// CHECK-NOT: prologue
 //
 // CHECK-LABEL: define linkonce_odr void @_ZN29FunctionSanitizerVirtualCalls1BC1Ev
-// CHECK-NOT: !func_sanitize
+// CHECK-NOT: prologue
 //
 // CHECK-LABEL: define linkonce_odr void @_ZN29FunctionSanitizerVirtualCalls1B1bEv
-// CHECK-NOT: !func_sanitize
+// CHECK-NOT: prologue
 //
 // CHECK-LABEL: define linkonce_odr void @_ZN29FunctionSanitizerVirtualCalls1B1gEv
-// CHECK-NOT: !func_sanitize
+// CHECK-NOT: prologue
 //
 // CHECK-LABEL: define linkonce_odr void @_ZN29FunctionSanitizerVirtualCalls1B1qEv
-// CHECK: !func_sanitize
+// CHECK: prologue
 
 }
 
@@ -749,5 +754,3 @@ void ThisAlign::this_align_lambda_2() {
 }
 
 // CHECK: attributes [[NR_NUW]] = { noreturn nounwind }
-
-// CHECK-FUNCSAN: ![[FUNCSAN]] = !{i32 846595819, i8** [[PROXY]]}
diff --git a/test/CodeGenCXX/ubsan-function-noexcept.cpp b/test/CodeGenCXX/ubsan-function-noexcept.cpp
index 9d5eb1ed..3c0c0e8b 100644
--- a/test/CodeGenCXX/ubsan-function-noexcept.cpp
+++ b/test/CodeGenCXX/ubsan-function-noexcept.cpp
@@ -2,8 +2,8 @@
 
 // Check that typeinfo recorded in function prolog doesn't have "Do" noexcept
 // qualifier in its mangled name.
-// CHECK: [[PROXY:@.*]] = private unnamed_addr constant i8* bitcast ({ i8*, i8* }* @_ZTIFvvE to i8*)
-// CHECK: define{{.*}} void @_Z1fv() #{{.*}} !func_sanitize ![[FUNCSAN:.*]] {
+// CHECK: @[[RTTI:[0-9]+]] = private constant i8* bitcast ({ i8*, i8* }* @_ZTIFvvE to i8*)
+// CHECK: define{{.*}} void @_Z1fv() #{{.*}} prologue <{ i32, i32 }> <{ i32 {{.*}}, i32 trunc (i64 sub (i64 ptrtoint (i8** @[[RTTI]] to i64), i64 ptrtoint (void ()* @_Z1fv to i64)) to i32) }>
 void f() noexcept {}
 
 // CHECK: define{{.*}} void @_Z1gPDoFvvE
@@ -13,5 +13,3 @@ void g(void (*p)() noexcept) {
   // CHECK: icmp eq i8* %{{.*}}, bitcast ({ i8*, i8* }* @_ZTIFvvE to i8*), !nosanitize
   p();
 }
-
-// CHECK: ![[FUNCSAN]] = !{i32 846595819, i8** [[PROXY]]}
diff --git a/test/Driver/baremetal-sysroot.cpp b/test/Driver/baremetal-sysroot.cpp
index ae174e01..fc660207 100644
--- a/test/Driver/baremetal-sysroot.cpp
+++ b/test/Driver/baremetal-sysroot.cpp
@@ -10,7 +10,7 @@
 // RUN: ln -s %clang %T/baremetal_default_sysroot/bin/clang
 
 // RUN: %T/baremetal_default_sysroot/bin/clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     -target armv6m-none-eabi \
+// RUN:     -target armv6m-none-eabi --sysroot= \
 // RUN:   | FileCheck --check-prefix=CHECK-V6M-C %s
 // CHECK-V6M-C: "{{.*}}clang{{.*}}" "-cc1" "-triple" "thumbv6m-none-unknown-eabi"
 // CHECK-V6M-C-SAME: "-internal-isystem" "{{.*}}/baremetal_default_sysroot{{[/\\]+}}bin{{[/\\]+}}..{{[/\\]+}}lib{{[/\\]+}}clang-runtimes{{[/\\]+}}armv6m-none-eabi{{[/\\]+}}include{{[/\\]+}}c++{{[/\\]+}}v1"
diff --git a/test/Driver/baremetal.cpp b/test/Driver/baremetal.cpp
index 7c11fe67..56eb5b70 100644
--- a/test/Driver/baremetal.cpp
+++ b/test/Driver/baremetal.cpp
@@ -105,7 +105,7 @@
 // CHECK-SYSROOT-INC-NOT: "-internal-isystem" "include"
 
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:      -target aarch64-none-elf \
+// RUN:      -target aarch64-none-elf --sysroot= \
 // RUN:   | FileCheck --check-prefix=CHECK-AARCH64-NO-HOST-INC %s
 // Verify that the bare metal driver does not include any host system paths:
 // CHECK-AARCH64-NO-HOST-INC: InstalledDir: [[INSTALLEDDIR:.+]]
diff --git a/test/Driver/fsanitize.c b/test/Driver/fsanitize.c
index a98fc2ee..17fce198 100644
--- a/test/Driver/fsanitize.c
+++ b/test/Driver/fsanitize.c
@@ -666,12 +666,12 @@
 // RUN: %clang -fno-sanitize=safe-stack -### %s 2>&1 | FileCheck %s -check-prefix=NOSP
 // NOSP-NOT: "-fsanitize=safe-stack"
 
-// RUN: %clang -target x86_64-linux-gnu -fsanitize=safe-stack -### %s 2>&1 | FileCheck %s -check-prefix=SP
+// RUN: %clang -target x86_64-linux-gnu -fsanitize=safe-stack -### %s 2>&1 | FileCheck %s -check-prefix=NO-SP
 // RUN: %clang -target x86_64-linux-gnu -fsanitize=address,safe-stack -### %s 2>&1 | FileCheck %s -check-prefix=SP-ASAN
 // RUN: %clang -target x86_64-linux-gnu -fstack-protector -fsanitize=safe-stack -### %s 2>&1 | FileCheck %s -check-prefix=SP
 // RUN: %clang -target x86_64-linux-gnu -fsanitize=safe-stack -fstack-protector-all -### %s 2>&1 | FileCheck %s -check-prefix=SP
-// RUN: %clang -target arm-linux-androideabi -fsanitize=safe-stack -### %s 2>&1 | FileCheck %s -check-prefix=SP
-// RUN: %clang -target aarch64-linux-android -fsanitize=safe-stack -### %s 2>&1 | FileCheck %s -check-prefix=SP
+// RUN: %clang -target arm-linux-androideabi -fsanitize=safe-stack -### %s 2>&1 | FileCheck %s -check-prefix=NO-SP
+// RUN: %clang -target aarch64-linux-android -fsanitize=safe-stack -### %s 2>&1 | FileCheck %s -check-prefix=NO-SP
 // RUN: %clang -target i386-contiki-unknown -fsanitize=safe-stack -### %s 2>&1 | FileCheck %s -check-prefix=NO-SP
 // NO-SP-NOT: stack-protector
 // NO-SP: "-fsanitize=safe-stack"
@@ -915,6 +915,3 @@
 
 // RUN: %clang -fsanitize=undefined,float-divide-by-zero %s -### 2>&1 | FileCheck %s --check-prefix=CHECK-DIVBYZERO-UBSAN
 // CHECK-DIVBYZERO-UBSAN: "-fsanitize={{.*}},float-divide-by-zero,{{.*}}"
-
-// RUN: %clang -target x86_64-linux-gnu -fsanitize=undefined,function -mcmodel=large %s -### 2>&1 | FileCheck %s --check-prefix=CHECK-UBSAN-FUNCTION-CODE-MODEL
-// CHECK-UBSAN-FUNCTION-CODE-MODEL: error: invalid argument '-fsanitize=function' only allowed with '-mcmodel=small'
diff --git a/test/Driver/hexagon-toolchain-linux.c b/test/Driver/hexagon-toolchain-linux.c
index da595903..1ef0561f 100644
--- a/test/Driver/hexagon-toolchain-linux.c
+++ b/test/Driver/hexagon-toolchain-linux.c
@@ -100,7 +100,7 @@
 // -----------------------------------------------------------------------------
 // internal-isystem for linux with and without musl
 // -----------------------------------------------------------------------------
-// RUN: %clang -### -target hexagon-unknown-linux-musl \
+// RUN: %clang -### -target hexagon-unknown-linux-musl --sysroot= \
 // RUN:   -ccc-install-dir %S/Inputs/hexagon_tree/Tools/bin \
 // RUN:   -resource-dir=%S/Inputs/resource_dir \
 // RUN:   %s 2>&1 \
@@ -110,7 +110,7 @@
 // CHECK008-SAME: {{^}} "-internal-isystem" "[[RESOURCE]]/include"
 // CHECK008-SAME: {{^}} "-internal-externc-isystem" "[[INSTALLED_DIR]]/../target/hexagon/include"
 
-// RUN: %clang -### -target hexagon-unknown-linux \
+// RUN: %clang -### -target hexagon-unknown-linux --sysroot= \
 // RUN:   -ccc-install-dir %S/Inputs/hexagon_tree/Tools/bin \
 // RUN:   -resource-dir=%S/Inputs/resource_dir \
 // RUN:   %s 2>&1 \
diff --git a/test/Driver/mips-cs.cpp b/test/Driver/mips-cs.cpp
index 39f87d8f..6ef4c5d4 100644
--- a/test/Driver/mips-cs.cpp
+++ b/test/Driver/mips-cs.cpp
@@ -4,7 +4,7 @@
 //
 // = Big-endian, hard float
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips-linux-gnu -no-pie \
+// RUN:     --target=mips-linux-gnu \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-BE-HF-32 %s
 // CHECK-BE-HF-32: "-internal-isystem"
@@ -32,7 +32,7 @@
 //
 // = Big-endian, hard float, uclibc
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips-linux-gnu -muclibc -no-pie \
+// RUN:     --target=mips-linux-gnu -muclibc \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-BE-UC-HF-32 %s
 // CHECK-BE-UC-HF-32: "-internal-isystem"
@@ -61,7 +61,7 @@
 //
 // = Big-endian, hard float, mips16
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips-linux-gnu -mips16 -no-pie \
+// RUN:     --target=mips-linux-gnu -mips16 \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-BE-HF-16 %s
 // CHECK-BE-HF-16: "-internal-isystem"
@@ -90,7 +90,7 @@
 //
 // = Big-endian, hard float, mmicromips
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips-linux-gnu -mmicromips -no-pie \
+// RUN:     --target=mips-linux-gnu -mmicromips \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-BE-HF-MICRO %s
 // CHECK-BE-HF-MICRO: "-internal-isystem"
@@ -119,7 +119,7 @@
 //
 // = Big-endian, hard float, nan2008
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips-linux-gnu -mnan=2008 -no-pie \
+// RUN:     --target=mips-linux-gnu -mnan=2008 \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-BE-HF-NAN %s
 // CHECK-BE-HF-NAN: "-internal-isystem"
@@ -148,7 +148,7 @@
 //
 // = Big-endian, hard float, uclibc, nan2008
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips-linux-gnu -muclibc -mnan=2008 -no-pie \
+// RUN:     --target=mips-linux-gnu -muclibc -mnan=2008 \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-BE-UC-HF-NAN %s
 // CHECK-BE-UC-HF-NAN: "-internal-isystem"
@@ -177,7 +177,7 @@
 //
 // = Big-endian, soft float
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips-linux-gnu -msoft-float -no-pie \
+// RUN:     --target=mips-linux-gnu -msoft-float \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-BE-SF-32 %s
 // CHECK-BE-SF-32: "-internal-isystem"
@@ -206,7 +206,7 @@
 //
 // = Big-endian, soft float, uclibc
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips-linux-gnu -muclibc -msoft-float -no-pie \
+// RUN:     --target=mips-linux-gnu -muclibc -msoft-float \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-BE-UC-SF-32 %s
 // CHECK-BE-UC-SF-32: "-internal-isystem"
@@ -235,7 +235,7 @@
 //
 // = Big-endian, soft float, mips16
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips-linux-gnu -msoft-float -mips16 -no-pie \
+// RUN:     --target=mips-linux-gnu -msoft-float -mips16 \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-BE-SF-16 %s
 // CHECK-BE-SF-16: "-internal-isystem"
@@ -264,7 +264,7 @@
 //
 // = Big-endian, soft float, micromips
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips-linux-gnu -msoft-float -mmicromips -no-pie \
+// RUN:     --target=mips-linux-gnu -msoft-float -mmicromips \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-BE-SF-MICRO %s
 // CHECK-BE-SF-MICRO: "-internal-isystem"
@@ -293,7 +293,7 @@
 //
 // = Big-endian, hard float, 64-bit
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips64-linux-gnu -no-pie \
+// RUN:     --target=mips64-linux-gnu \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-BE-HF-64 %s
 // CHECK-BE-HF-64: "-internal-isystem"
@@ -322,7 +322,7 @@
 //
 // = Big-endian, soft float, 64-bit
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips64-linux-gnu -msoft-float -no-pie \
+// RUN:     --target=mips64-linux-gnu -msoft-float \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-BE-SF-64 %s
 // CHECK-BE-SF-64: "-internal-isystem"
@@ -351,7 +351,7 @@
 //
 // = Little-endian, hard float
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mipsel-linux-gnu -mhard-float -no-pie \
+// RUN:     --target=mipsel-linux-gnu -mhard-float \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-EL-HF-32 %s
 // CHECK-EL-HF-32: "-internal-isystem"
@@ -380,7 +380,7 @@
 //
 // = Little-endian, hard float, uclibc
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mipsel-linux-gnu -mhard-float -muclibc -no-pie \
+// RUN:     --target=mipsel-linux-gnu -mhard-float -muclibc \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-EL-UC-HF-32 %s
 // CHECK-EL-UC-HF-32: "-internal-isystem"
@@ -409,7 +409,7 @@
 //
 // = Little-endian, hard float, mips16
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mipsel-linux-gnu -mips16 -no-pie \
+// RUN:     --target=mipsel-linux-gnu -mips16 \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-EL-HF-16 %s
 // CHECK-EL-HF-16: "-internal-isystem"
@@ -438,7 +438,7 @@
 //
 // = Little-endian, hard float, micromips
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mipsel-linux-gnu -mmicromips -no-pie \
+// RUN:     --target=mipsel-linux-gnu -mmicromips \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-EL-HF-MICRO %s
 // CHECK-EL-HF-MICRO: "-internal-isystem"
@@ -467,7 +467,7 @@
 //
 // = Little-endian, hard float, nan2008
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mipsel-linux-gnu -mnan=2008 -no-pie \
+// RUN:     --target=mipsel-linux-gnu -mnan=2008 \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-EL-HF-NAN %s
 // CHECK-EL-HF-NAN: "-internal-isystem"
@@ -496,7 +496,7 @@
 //
 // = Little-endian, hard float, uclibc, nan2008
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mipsel-linux-gnu -muclibc -mnan=2008 -no-pie \
+// RUN:     --target=mipsel-linux-gnu -muclibc -mnan=2008 \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-EL-UC-HF-NAN %s
 // CHECK-EL-UC-HF-NAN: "-internal-isystem"
@@ -525,7 +525,7 @@
 //
 // = Little-endian, soft float
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mipsel-linux-gnu -mfloat-abi=soft -no-pie \
+// RUN:     --target=mipsel-linux-gnu -mfloat-abi=soft \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-EL-SF-32 %s
 // CHECK-EL-SF-32: "-internal-isystem"
@@ -554,7 +554,7 @@
 //
 // = Little-endian, soft float, uclibc
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mipsel-linux-gnu -mfloat-abi=soft -muclibc -no-pie \
+// RUN:     --target=mipsel-linux-gnu -mfloat-abi=soft -muclibc \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-EL-UC-SF-32 %s
 // CHECK-EL-UC-SF-32: "-internal-isystem"
@@ -583,7 +583,7 @@
 //
 // = Little-endian, soft float, mips16
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mipsel-linux-gnu -mips16 -msoft-float -no-pie \
+// RUN:     --target=mipsel-linux-gnu -mips16 -msoft-float \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-EL-SF-16 %s
 // CHECK-EL-SF-16: "-internal-isystem"
@@ -612,7 +612,7 @@
 //
 // = Little-endian, soft float, micromips
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mipsel-linux-gnu -mmicromips -msoft-float -no-pie \
+// RUN:     --target=mipsel-linux-gnu -mmicromips -msoft-float \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-EL-SF-MICRO %s
 // CHECK-EL-SF-MICRO: "-internal-isystem"
@@ -641,7 +641,7 @@
 //
 // = Little-endian, hard float, 64-bit
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips64el-linux-gnu -no-pie \
+// RUN:     --target=mips64el-linux-gnu \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-EL-HF-64 %s
 // CHECK-EL-HF-64: "-internal-isystem"
@@ -670,7 +670,7 @@
 //
 // = Little-endian, soft float, 64-bit
 // RUN: %clang -no-canonical-prefixes %s -### -o %t.o 2>&1 \
-// RUN:     --target=mips64el-linux-gnu -msoft-float -no-pie \
+// RUN:     --target=mips64el-linux-gnu -msoft-float \
 // RUN:     -stdlib=libstdc++ --gcc-toolchain=%S/Inputs/mips_cs_tree \
 // RUN:   | FileCheck --check-prefix=CHECK-EL-SF-64 %s
 // CHECK-EL-SF-64: "-internal-isystem"
diff --git a/test/Driver/stack-protector.c b/test/Driver/stack-protector.c
index dfffe0d6..a3e40b50 100644
--- a/test/Driver/stack-protector.c
+++ b/test/Driver/stack-protector.c
@@ -3,11 +3,11 @@
 // NOSSP-NOT: "-stack-protector-buffer-size" 
 
 // RUN: %clang -target i386-unknown-linux -fstack-protector -### %s 2>&1 | FileCheck %s -check-prefix=SSP
-// SSP: "-stack-protector" "2"
+// SSP: "-stack-protector" "1"
 // SSP-NOT: "-stack-protector-buffer-size" 
 
 // RUN: %clang -target i386-unknown-linux -fstack-protector --param ssp-buffer-size=16 -### %s 2>&1 | FileCheck %s -check-prefix=SSP-BUF
-// SSP-BUF: "-stack-protector" "2"
+// SSP-BUF: "-stack-protector" "1"
 // SSP-BUF: "-stack-protector-buffer-size" "16" 
 
 // RUN: %clang -target i386-pc-openbsd -### %s 2>&1 | FileCheck %s -check-prefix=OPENBSD
diff --git a/test/Preprocessor/init.c b/test/Preprocessor/init.c
index 46cfcd6d..c83c82d7 100644
--- a/test/Preprocessor/init.c
+++ b/test/Preprocessor/init.c
@@ -2603,3 +2603,33 @@
 // RISCV64-LINUX: #define __unix__ 1
 // RISCV64-LINUX: #define linux 1
 // RISCV64-LINUX: #define unix 1
+
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 -target-feature +d /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefixes=LOONGARCH64,LOONGARCH64-HASBASICD %s
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 -target-feature +f /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefixes=LOONGARCH64,LOONGARCH64-HASBASICF %s
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 -target-feature -d  -target-feature -f /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefixes=LOONGARCH64,LOONGARCH64-SOFT %s
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 -target-abi lp64s /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefixes=LOONGARCH64,LOONGARCH64-LP64S %s
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 -target-abi lp64f /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefixes=LOONGARCH64,LOONGARCH64-LP64F,LOONGARCH64-HARD %s
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 -target-abi lp64d /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefixes=LOONGARCH64,LOONGARCH64-LP64D,LOONGARCH64-HARD %s
+// RUN: %clang_cc1 -x c -E -dM -ffreestanding -fgnuc-version=4.2.1 -triple=loongarch64 /dev/null \
+// RUN:   | FileCheck -match-full-lines -check-prefix=LOONGARCH64 %s
+// LOONGARCH64: #define _LOONGARCH_ARCH "loongarch64"
+// LOONGARCH64: #define _LOONGARCH_SZINT 32
+// LOONGARCH64: #define _LOONGARCH_SZLONG 64
+// LOONGARCH64: #define _LOONGARCH_SZPTR 64
+// LOONGARCH64: #define _LOONGARCH_TUNE "la464"
+// LOONGARCH64: #define __loongarch__ 1
+// LOONGARCH64-LP64D: #define __loongarch_double_float 1
+// LOONGARCH64-HASBASICD: #define __loongarch_frlen 64
+// LOONGARCH64-HASBASICF: #define __loongarch_frlen 32
+// LOONGARCH64-SOFT: #define __loongarch_frlen 0
+// LOONGARCH64: #define __loongarch_grlen 64
+// LOONGARCH64-HARD: #define __loongarch_hard_float 1
+// LOONGARCH64: #define __loongarch_lp64 1
+// LOONGARCH64-LP64F: #define __loongarch_single_float 1
+// LOONGARCH64-LP64S: #define __loongarch_soft_float 1
diff --git a/unittests/Interpreter/ExceptionTests/InterpreterExceptionTest.cpp b/unittests/Interpreter/ExceptionTests/InterpreterExceptionTest.cpp
index 75928d91..3350ee3f 100644
--- a/unittests/Interpreter/ExceptionTests/InterpreterExceptionTest.cpp
+++ b/unittests/Interpreter/ExceptionTests/InterpreterExceptionTest.cpp
@@ -104,6 +104,11 @@ extern "C" int throw_exception() {
   if (Triple.isPPC())
     return;
 
+  // FIXME: LoongArch64 fails due to `Symbols not found:
+  // [DW.ref.__gxx_personality_v0]`
+  if (Triple.isLoongArch64())
+    return;
+
   // FIXME: ARM fails due to `Not implemented relocation type!`
   if (Triple.isARM())
     return;
-- 
2.38.1

